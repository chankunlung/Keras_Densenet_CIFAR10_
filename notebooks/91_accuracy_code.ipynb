{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Vs3EE0hk464w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8458
        },
        "outputId": "4f59c69b-303b-42b6-9c6e-8cfacfc4f240"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "l = 10\n",
        "num_filter = 40\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "\n",
        "\n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# img_height = 32 \n",
        "# img_width = 32\n",
        "# channel = 3\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate=0)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate=0)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "# Fourth_Block = add_denseblock(Third_Transition, num_filter, dropout_rate)\n",
        "# Fourth_Transition = add_transition(Fourth_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "# top_5_accuracy = keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "\n",
        "\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "filepath = f'/gdrive/My Drive/CIFAR_10/__{epochs}_.h5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='auto', \n",
        "                                period=5)\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_center=True,\n",
        "#     featurewise_std_normalization=True,\n",
        "#     brightness_range = [0.2 , 0.6],\n",
        "#     rotation_range=15,\n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "     fill_mode='constant',\n",
        "#     zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=epochs , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True)\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#                     batch_size=batch_size,\n",
        "#                     epochs=epochs,\n",
        "#                     verbose=1,\n",
        "#                     validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"version_1_Test_acuracy_0.8616_DNST_model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 32, 32, 40)   1080        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 32, 32, 40)   160         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 32, 32, 40)   0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 32, 32, 20)   7200        activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_81 (Concatenate)    (None, 32, 32, 60)   0           conv2d_89[0][0]                  \n",
            "                                                                 conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 32, 32, 60)   240         concatenate_81[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 32, 32, 60)   0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 32, 32, 20)   10800       activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_82 (Concatenate)    (None, 32, 32, 80)   0           concatenate_81[0][0]             \n",
            "                                                                 conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 32, 32, 80)   320         concatenate_82[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 32, 32, 80)   0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 32, 32, 20)   14400       activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_83 (Concatenate)    (None, 32, 32, 100)  0           concatenate_82[0][0]             \n",
            "                                                                 conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 32, 32, 100)  400         concatenate_83[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 32, 32, 100)  0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 32, 32, 20)   18000       activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_84 (Concatenate)    (None, 32, 32, 120)  0           concatenate_83[0][0]             \n",
            "                                                                 conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 32, 32, 120)  480         concatenate_84[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 32, 32, 120)  0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 32, 32, 20)   21600       activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_85 (Concatenate)    (None, 32, 32, 140)  0           concatenate_84[0][0]             \n",
            "                                                                 conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 32, 32, 140)  560         concatenate_85[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 32, 32, 140)  0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 32, 32, 20)   25200       activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_86 (Concatenate)    (None, 32, 32, 160)  0           concatenate_85[0][0]             \n",
            "                                                                 conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 32, 32, 160)  640         concatenate_86[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 32, 32, 160)  0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 32, 32, 20)   28800       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_87 (Concatenate)    (None, 32, 32, 180)  0           concatenate_86[0][0]             \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 32, 32, 180)  720         concatenate_87[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 32, 32, 180)  0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 32, 32, 20)   32400       activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_88 (Concatenate)    (None, 32, 32, 200)  0           concatenate_87[0][0]             \n",
            "                                                                 conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 32, 32, 200)  800         concatenate_88[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 32, 32, 200)  0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 32, 32, 20)   36000       activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_89 (Concatenate)    (None, 32, 32, 220)  0           concatenate_88[0][0]             \n",
            "                                                                 conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 32, 32, 220)  880         concatenate_89[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 32, 32, 220)  0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 32, 32, 20)   39600       activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_90 (Concatenate)    (None, 32, 32, 240)  0           concatenate_89[0][0]             \n",
            "                                                                 conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 32, 32, 240)  960         concatenate_90[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 32, 32, 240)  0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 32, 32, 20)   4800        activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 16, 16, 20)   0           conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 16, 16, 20)   80          average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 16, 16, 20)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 16, 16, 20)   3600        activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_65 (Dropout)            (None, 16, 16, 20)   0           conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_91 (Concatenate)    (None, 16, 16, 40)   0           average_pooling2d_9[0][0]        \n",
            "                                                                 dropout_65[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 16, 16, 40)   160         concatenate_91[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 16, 16, 40)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 16, 16, 20)   7200        activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_66 (Dropout)            (None, 16, 16, 20)   0           conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_92 (Concatenate)    (None, 16, 16, 60)   0           concatenate_91[0][0]             \n",
            "                                                                 dropout_66[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 16, 16, 60)   240         concatenate_92[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 16, 16, 60)   0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 16, 16, 20)   10800       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_67 (Dropout)            (None, 16, 16, 20)   0           conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_93 (Concatenate)    (None, 16, 16, 80)   0           concatenate_92[0][0]             \n",
            "                                                                 dropout_67[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 16, 16, 80)   320         concatenate_93[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 16, 16, 80)   0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 16, 16, 20)   14400       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_68 (Dropout)            (None, 16, 16, 20)   0           conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_94 (Concatenate)    (None, 16, 16, 100)  0           concatenate_93[0][0]             \n",
            "                                                                 dropout_68[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 16, 16, 100)  400         concatenate_94[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 16, 16, 100)  0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 16, 16, 20)   18000       activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_69 (Dropout)            (None, 16, 16, 20)   0           conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_95 (Concatenate)    (None, 16, 16, 120)  0           concatenate_94[0][0]             \n",
            "                                                                 dropout_69[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 16, 16, 120)  480         concatenate_95[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 16, 16, 120)  0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 16, 16, 20)   21600       activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_70 (Dropout)            (None, 16, 16, 20)   0           conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_96 (Concatenate)    (None, 16, 16, 140)  0           concatenate_95[0][0]             \n",
            "                                                                 dropout_70[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 16, 16, 140)  560         concatenate_96[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 16, 16, 140)  0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 16, 16, 20)   25200       activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_71 (Dropout)            (None, 16, 16, 20)   0           conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_97 (Concatenate)    (None, 16, 16, 160)  0           concatenate_96[0][0]             \n",
            "                                                                 dropout_71[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 16, 16, 160)  640         concatenate_97[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 16, 16, 160)  0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 16, 16, 20)   28800       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_72 (Dropout)            (None, 16, 16, 20)   0           conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_98 (Concatenate)    (None, 16, 16, 180)  0           concatenate_97[0][0]             \n",
            "                                                                 dropout_72[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 16, 16, 180)  720         concatenate_98[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 16, 16, 180)  0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 16, 16, 20)   32400       activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_73 (Dropout)            (None, 16, 16, 20)   0           conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_99 (Concatenate)    (None, 16, 16, 200)  0           concatenate_98[0][0]             \n",
            "                                                                 dropout_73[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 16, 16, 200)  800         concatenate_99[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 16, 16, 200)  0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 16, 16, 20)   36000       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_74 (Dropout)            (None, 16, 16, 20)   0           conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_100 (Concatenate)   (None, 16, 16, 220)  0           concatenate_99[0][0]             \n",
            "                                                                 dropout_74[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 16, 16, 220)  880         concatenate_100[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 16, 16, 220)  0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 16, 16, 20)   4400        activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_75 (Dropout)            (None, 16, 16, 20)   0           conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_10 (AveragePo (None, 8, 8, 20)     0           dropout_75[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 8, 8, 20)     80          average_pooling2d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 8, 8, 20)     0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 8, 8, 20)     3600        activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_76 (Dropout)            (None, 8, 8, 20)     0           conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_101 (Concatenate)   (None, 8, 8, 40)     0           average_pooling2d_10[0][0]       \n",
            "                                                                 dropout_76[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 8, 8, 40)     160         concatenate_101[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 8, 8, 40)     0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 8, 8, 20)     7200        activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_77 (Dropout)            (None, 8, 8, 20)     0           conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_102 (Concatenate)   (None, 8, 8, 60)     0           concatenate_101[0][0]            \n",
            "                                                                 dropout_77[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 8, 8, 60)     240         concatenate_102[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 8, 8, 60)     0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 8, 8, 20)     10800       activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_78 (Dropout)            (None, 8, 8, 20)     0           conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_103 (Concatenate)   (None, 8, 8, 80)     0           concatenate_102[0][0]            \n",
            "                                                                 dropout_78[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 8, 8, 80)     320         concatenate_103[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 8, 8, 80)     0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 8, 8, 20)     14400       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_79 (Dropout)            (None, 8, 8, 20)     0           conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_104 (Concatenate)   (None, 8, 8, 100)    0           concatenate_103[0][0]            \n",
            "                                                                 dropout_79[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 8, 8, 100)    400         concatenate_104[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 8, 8, 100)    0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 8, 8, 20)     18000       activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_80 (Dropout)            (None, 8, 8, 20)     0           conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_105 (Concatenate)   (None, 8, 8, 120)    0           concatenate_104[0][0]            \n",
            "                                                                 dropout_80[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 8, 8, 120)    480         concatenate_105[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 8, 8, 120)    0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 8, 8, 20)     21600       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_81 (Dropout)            (None, 8, 8, 20)     0           conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_106 (Concatenate)   (None, 8, 8, 140)    0           concatenate_105[0][0]            \n",
            "                                                                 dropout_81[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 8, 8, 140)    560         concatenate_106[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 8, 8, 140)    0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 8, 8, 20)     25200       activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_82 (Dropout)            (None, 8, 8, 20)     0           conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_107 (Concatenate)   (None, 8, 8, 160)    0           concatenate_106[0][0]            \n",
            "                                                                 dropout_82[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 8, 8, 160)    640         concatenate_107[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 8, 8, 160)    0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 8, 8, 20)     28800       activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_83 (Dropout)            (None, 8, 8, 20)     0           conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_108 (Concatenate)   (None, 8, 8, 180)    0           concatenate_107[0][0]            \n",
            "                                                                 dropout_83[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 8, 8, 180)    720         concatenate_108[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 8, 8, 180)    0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 8, 8, 20)     32400       activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_84 (Dropout)            (None, 8, 8, 20)     0           conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_109 (Concatenate)   (None, 8, 8, 200)    0           concatenate_108[0][0]            \n",
            "                                                                 dropout_84[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 8, 8, 200)    800         concatenate_109[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 8, 8, 200)    0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 8, 8, 20)     36000       activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_85 (Dropout)            (None, 8, 8, 20)     0           conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_110 (Concatenate)   (None, 8, 8, 220)    0           concatenate_109[0][0]            \n",
            "                                                                 dropout_85[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 8, 8, 220)    880         concatenate_110[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 8, 8, 220)    0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 8, 8, 20)     4400        activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_86 (Dropout)            (None, 8, 8, 20)     0           conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_11 (AveragePo (None, 4, 4, 20)     0           dropout_86[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 4, 4, 20)     80          average_pooling2d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 4, 4, 20)     0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 4, 4, 20)     3600        activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_87 (Dropout)            (None, 4, 4, 20)     0           conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_111 (Concatenate)   (None, 4, 4, 40)     0           average_pooling2d_11[0][0]       \n",
            "                                                                 dropout_87[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 4, 4, 40)     160         concatenate_111[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 4, 4, 40)     0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 4, 4, 20)     7200        activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_88 (Dropout)            (None, 4, 4, 20)     0           conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_112 (Concatenate)   (None, 4, 4, 60)     0           concatenate_111[0][0]            \n",
            "                                                                 dropout_88[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 4, 4, 60)     240         concatenate_112[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 4, 4, 60)     0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 4, 4, 20)     10800       activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_89 (Dropout)            (None, 4, 4, 20)     0           conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_113 (Concatenate)   (None, 4, 4, 80)     0           concatenate_112[0][0]            \n",
            "                                                                 dropout_89[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 4, 4, 80)     320         concatenate_113[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 4, 4, 80)     0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 4, 4, 20)     14400       activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_90 (Dropout)            (None, 4, 4, 20)     0           conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_114 (Concatenate)   (None, 4, 4, 100)    0           concatenate_113[0][0]            \n",
            "                                                                 dropout_90[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 4, 4, 100)    400         concatenate_114[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 4, 4, 100)    0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 4, 4, 20)     18000       activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_91 (Dropout)            (None, 4, 4, 20)     0           conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_115 (Concatenate)   (None, 4, 4, 120)    0           concatenate_114[0][0]            \n",
            "                                                                 dropout_91[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 4, 4, 120)    480         concatenate_115[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 4, 4, 120)    0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 4, 4, 20)     21600       activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_92 (Dropout)            (None, 4, 4, 20)     0           conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_116 (Concatenate)   (None, 4, 4, 140)    0           concatenate_115[0][0]            \n",
            "                                                                 dropout_92[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 4, 4, 140)    560         concatenate_116[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 4, 4, 140)    0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 4, 4, 20)     25200       activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_93 (Dropout)            (None, 4, 4, 20)     0           conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_117 (Concatenate)   (None, 4, 4, 160)    0           concatenate_116[0][0]            \n",
            "                                                                 dropout_93[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 4, 4, 160)    640         concatenate_117[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 4, 4, 160)    0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 4, 4, 20)     28800       activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_94 (Dropout)            (None, 4, 4, 20)     0           conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_118 (Concatenate)   (None, 4, 4, 180)    0           concatenate_117[0][0]            \n",
            "                                                                 dropout_94[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 4, 4, 180)    720         concatenate_118[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 4, 4, 180)    0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 4, 4, 20)     32400       activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_95 (Dropout)            (None, 4, 4, 20)     0           conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_119 (Concatenate)   (None, 4, 4, 200)    0           concatenate_118[0][0]            \n",
            "                                                                 dropout_95[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 4, 4, 200)    800         concatenate_119[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 4, 4, 200)    0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 4, 4, 20)     36000       activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_96 (Dropout)            (None, 4, 4, 20)     0           conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_120 (Concatenate)   (None, 4, 4, 220)    0           concatenate_119[0][0]            \n",
            "                                                                 dropout_96[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 4, 4, 220)    880         concatenate_120[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 4, 4, 220)    0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, 2, 2, 220)    0           activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 880)          0           average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           8810        flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 873,490\n",
            "Trainable params: 862,490\n",
            "Non-trainable params: 11,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J_Cxf4vSACTw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1400
        },
        "outputId": "02756e6b-e32d-4b8f-ed25-462e010ce4f4"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__100_.h5\")\n",
        "\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    callbacks=[csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=30 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "782/782 [==============================] - 241s 308ms/step - loss: 0.3818 - acc: 0.8668 - val_loss: 0.7583 - val_acc: 0.8081\n",
            "Epoch 2/30\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.3715 - acc: 0.8700 - val_loss: 0.6519 - val_acc: 0.8222\n",
            "Epoch 3/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.3637 - acc: 0.8735 - val_loss: 0.6595 - val_acc: 0.8304\n",
            "Epoch 4/30\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.3547 - acc: 0.8778 - val_loss: 0.6298 - val_acc: 0.8171\n",
            "Epoch 5/30\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.3397 - acc: 0.8827 - val_loss: 0.5769 - val_acc: 0.8460\n",
            "\n",
            "Epoch 00005: acc did not improve from 0.89532\n",
            "Epoch 6/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.3377 - acc: 0.8840 - val_loss: 0.6445 - val_acc: 0.8235\n",
            "Epoch 7/30\n",
            "782/782 [==============================] - 239s 306ms/step - loss: 0.3313 - acc: 0.8840 - val_loss: 0.8150 - val_acc: 0.7903\n",
            "Epoch 8/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.3227 - acc: 0.8879 - val_loss: 0.6252 - val_acc: 0.8286\n",
            "Epoch 9/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.3158 - acc: 0.8918 - val_loss: 0.5758 - val_acc: 0.8420\n",
            "Epoch 10/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.3099 - acc: 0.8933 - val_loss: 0.5252 - val_acc: 0.8564\n",
            "\n",
            "Epoch 00010: acc did not improve from 0.89532\n",
            "Epoch 11/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.3044 - acc: 0.8934 - val_loss: 0.6116 - val_acc: 0.8390\n",
            "Epoch 12/30\n",
            "782/782 [==============================] - 239s 306ms/step - loss: 0.2981 - acc: 0.8968 - val_loss: 0.6420 - val_acc: 0.8318\n",
            "Epoch 13/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2915 - acc: 0.8992 - val_loss: 0.6612 - val_acc: 0.8234\n",
            "Epoch 14/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2828 - acc: 0.9009 - val_loss: 0.5908 - val_acc: 0.8453\n",
            "Epoch 15/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.2793 - acc: 0.9031 - val_loss: 0.6474 - val_acc: 0.8332\n",
            "\n",
            "Epoch 00015: acc improved from 0.89532 to 0.90304, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 16/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.2776 - acc: 0.9039 - val_loss: 0.4799 - val_acc: 0.8666\n",
            "Epoch 17/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.2702 - acc: 0.9074 - val_loss: 0.5622 - val_acc: 0.8509\n",
            "Epoch 18/30\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.2719 - acc: 0.9054 - val_loss: 0.6810 - val_acc: 0.8297\n",
            "Epoch 19/30\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.2723 - acc: 0.9055 - val_loss: 0.5474 - val_acc: 0.8573\n",
            "Epoch 20/30\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.2602 - acc: 0.9096 - val_loss: 0.5303 - val_acc: 0.8560\n",
            "\n",
            "Epoch 00020: acc improved from 0.90304 to 0.90978, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 21/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.2581 - acc: 0.9093 - val_loss: 0.5575 - val_acc: 0.8580\n",
            "Epoch 22/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2524 - acc: 0.9128 - val_loss: 0.5067 - val_acc: 0.8669\n",
            "Epoch 23/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2475 - acc: 0.9146 - val_loss: 0.5383 - val_acc: 0.8618\n",
            "Epoch 24/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2446 - acc: 0.9145 - val_loss: 0.6258 - val_acc: 0.8462\n",
            "Epoch 25/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2402 - acc: 0.9168 - val_loss: 0.5809 - val_acc: 0.8508\n",
            "\n",
            "Epoch 00025: acc improved from 0.90978 to 0.91678, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 26/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2402 - acc: 0.9160 - val_loss: 0.5746 - val_acc: 0.8595\n",
            "Epoch 27/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2392 - acc: 0.9167 - val_loss: 0.7056 - val_acc: 0.8259\n",
            "Epoch 28/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2333 - acc: 0.9186 - val_loss: 0.5773 - val_acc: 0.8523\n",
            "Epoch 29/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2285 - acc: 0.9193 - val_loss: 0.4250 - val_acc: 0.8870\n",
            "Epoch 30/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2268 - acc: 0.9208 - val_loss: 0.5870 - val_acc: 0.8558\n",
            "\n",
            "Epoch 00030: acc improved from 0.91678 to 0.92080, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f39352ef588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "JdpSIzdcdy11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "2af74b76-6856-4036-d623-6a70292e8d8b"
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 18s 2ms/step\n",
            "Test loss: 0.5870169169366359\n",
            "Test accuracy: 0.8558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3wBePIk5Adku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1026
        },
        "outputId": "bd4ba4d2-3e26-438c-c97c-7400bd5f4917"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__100_.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=5, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    callbacks=[reduce_lr , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=20 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.2232 - acc: 0.9221 - val_loss: 0.7536 - val_acc: 0.8318\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2292 - acc: 0.9192 - val_loss: 0.6578 - val_acc: 0.8404\n",
            "Epoch 3/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.2169 - acc: 0.9241 - val_loss: 0.6643 - val_acc: 0.8382\n",
            "Epoch 4/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.2145 - acc: 0.9239 - val_loss: 0.7835 - val_acc: 0.8382\n",
            "Epoch 5/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.2128 - acc: 0.9247 - val_loss: 0.4297 - val_acc: 0.8819\n",
            "\n",
            "Epoch 00005: acc improved from 0.92080 to 0.92458, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 6/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2126 - acc: 0.9248 - val_loss: 0.4650 - val_acc: 0.8811\n",
            "Epoch 7/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2048 - acc: 0.9267 - val_loss: 0.5597 - val_acc: 0.8593\n",
            "Epoch 8/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2059 - acc: 0.9279 - val_loss: 0.4287 - val_acc: 0.8863\n",
            "Epoch 9/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2031 - acc: 0.9289 - val_loss: 0.5942 - val_acc: 0.8485\n",
            "Epoch 10/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2017 - acc: 0.9286 - val_loss: 0.6702 - val_acc: 0.8461\n",
            "\n",
            "Epoch 00010: acc improved from 0.92458 to 0.92856, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 11/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1975 - acc: 0.9306 - val_loss: 0.5712 - val_acc: 0.8602\n",
            "Epoch 12/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1990 - acc: 0.9297 - val_loss: 0.4948 - val_acc: 0.8773\n",
            "Epoch 13/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1902 - acc: 0.9328 - val_loss: 0.5292 - val_acc: 0.8699\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 14/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1578 - acc: 0.9446 - val_loss: 0.3693 - val_acc: 0.9031\n",
            "Epoch 15/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1387 - acc: 0.9521 - val_loss: 0.3751 - val_acc: 0.9039\n",
            "\n",
            "Epoch 00015: acc improved from 0.92856 to 0.95216, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 16/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1374 - acc: 0.9513 - val_loss: 0.3712 - val_acc: 0.9072\n",
            "Epoch 17/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1317 - acc: 0.9542 - val_loss: 0.3720 - val_acc: 0.9058\n",
            "Epoch 18/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1294 - acc: 0.9550 - val_loss: 0.3807 - val_acc: 0.9045\n",
            "Epoch 19/20\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.1278 - acc: 0.9554 - val_loss: 0.3831 - val_acc: 0.9077\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 20/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1260 - acc: 0.9561 - val_loss: 0.3873 - val_acc: 0.9037\n",
            "\n",
            "Epoch 00020: acc improved from 0.95216 to 0.95610, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3951053198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "_8G1hOdGAdgM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "135a918a-b8ed-4cdd-9012-dabcb81d3829"
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 18s 2ms/step\n",
            "Test loss: 0.3872690258145332\n",
            "Test accuracy: 0.9037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f1W4g0OKAddM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1045
        },
        "outputId": "5c1646e2-9b4a-4760-cf8a-de1d65bf5d5a"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__100_.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=5, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    callbacks=[reduce_lr , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=20 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "782/782 [==============================] - 240s 306ms/step - loss: 0.1268 - acc: 0.9557 - val_loss: 0.3831 - val_acc: 0.9070\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1238 - acc: 0.9566 - val_loss: 0.3676 - val_acc: 0.9091\n",
            "Epoch 3/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1211 - acc: 0.9570 - val_loss: 0.3865 - val_acc: 0.9062\n",
            "Epoch 4/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1197 - acc: 0.9585 - val_loss: 0.3870 - val_acc: 0.9057\n",
            "Epoch 5/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1200 - acc: 0.9568 - val_loss: 0.3863 - val_acc: 0.9045\n",
            "\n",
            "Epoch 00005: acc improved from 0.95610 to 0.95680, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 6/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1202 - acc: 0.9574 - val_loss: 0.3924 - val_acc: 0.9060\n",
            "Epoch 7/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1204 - acc: 0.9572 - val_loss: 0.3795 - val_acc: 0.9072\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 8/20\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9591Epoch 9/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1159 - acc: 0.9582 - val_loss: 0.3787 - val_acc: 0.9096\n",
            "Epoch 10/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1150 - acc: 0.9592 - val_loss: 0.4011 - val_acc: 0.9055\n",
            "\n",
            "Epoch 00010: acc improved from 0.95680 to 0.95924, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 11/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1132 - acc: 0.9601 - val_loss: 0.3944 - val_acc: 0.9055\n",
            "Epoch 12/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1150 - acc: 0.9600 - val_loss: 0.3968 - val_acc: 0.9037\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 13/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1120 - acc: 0.9612 - val_loss: 0.3897 - val_acc: 0.9065\n",
            "Epoch 14/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1117 - acc: 0.9601 - val_loss: 0.3946 - val_acc: 0.9074\n",
            "Epoch 15/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1132 - acc: 0.9594 - val_loss: 0.3994 - val_acc: 0.9073\n",
            "\n",
            "Epoch 00015: acc improved from 0.95924 to 0.95940, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 16/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1107 - acc: 0.9615 - val_loss: 0.3856 - val_acc: 0.9090\n",
            "Epoch 17/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1100 - acc: 0.9613 - val_loss: 0.4063 - val_acc: 0.9049\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 18/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1086 - acc: 0.9619 - val_loss: 0.3925 - val_acc: 0.9086\n",
            "Epoch 19/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1120 - acc: 0.9609 - val_loss: 0.3913 - val_acc: 0.9089\n",
            "Epoch 20/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1095 - acc: 0.9624 - val_loss: 0.3916 - val_acc: 0.9098\n",
            "\n",
            "Epoch 00020: acc improved from 0.95940 to 0.96244, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3951062198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "1gm3SIbHAdaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "c803c53f-2dfa-468a-c194-38c8dad302f0"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__100_.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=1.0, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=2, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.01)\n",
        "\n",
        "\n",
        "#model_checkpoint\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    callbacks=[reduce_lr , csv_logger ] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=5 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.1124 - acc: 0.9594 - val_loss: 0.4045 - val_acc: 0.9059\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1070 - acc: 0.9617 - val_loss: 0.4026 - val_acc: 0.9069\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1074 - acc: 0.9620 - val_loss: 0.4031 - val_acc: 0.9077\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1093 - acc: 0.9615 - val_loss: 0.3968 - val_acc: 0.9082\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1088 - acc: 0.9604 - val_loss: 0.3989 - val_acc: 0.9080\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3951045f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "mGOPQ8cpAdXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(\"/gdrive/My Drive/CIFAR_10/__epochs5_valaccuracy9080__4PM.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e9ulR1shAdUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "a7cfac0e-9acb-4ea1-f6e7-6ca61efc6fc4"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__epochs5_valaccuracy9080__4PM.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=1.0, decay=1e-9, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=20, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.001)\n",
        "\n",
        "\n",
        "#model_checkpoint\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    callbacks=[reduce_lr , csv_logger ] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=30 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 285s 183ms/step - loss: 0.1321 - acc: 0.9533 - val_loss: 0.4043 - val_acc: 0.9068\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 282s 180ms/step - loss: 0.1284 - acc: 0.9545 - val_loss: 0.3935 - val_acc: 0.9089\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 281s 180ms/step - loss: 0.1317 - acc: 0.9530 - val_loss: 0.4387 - val_acc: 0.8971\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1276 - acc: 0.9538 - val_loss: 0.4143 - val_acc: 0.9042\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1255 - acc: 0.9563 - val_loss: 0.4020 - val_acc: 0.9054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3950fcf1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "AUshjZ3rAdRU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(\"/gdrive/My Drive/CIFAR_10/epoch5_reduceLR_factor02_patience20_minlr0001_decay1e9.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e4rJ75Gr9kaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 91.06 % Accuracy at 34 Epoch"
      ]
    },
    {
      "metadata": {
        "id": "i0Cf9j55AdOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1605
        },
        "outputId": "7c190bff-af35-4b7d-aca7-6b06160f2e57"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__epochs5_valaccuracy9080__4PM.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "filepath = \"/gdrive/My Drive/CIFAR_10/__best_val_accuracy__.h5\"\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='auto', \n",
        "                                period=10)\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=17, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.001)\n",
        "\n",
        "\n",
        "#model_checkpoint\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    callbacks=[reduce_lr , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=50 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1301 - acc: 0.9535 - val_loss: 0.4220 - val_acc: 0.9030\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 281s 180ms/step - loss: 0.1310 - acc: 0.9538 - val_loss: 0.4199 - val_acc: 0.9043\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 282s 180ms/step - loss: 0.1282 - acc: 0.9542 - val_loss: 0.4010 - val_acc: 0.9055\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1298 - acc: 0.9538 - val_loss: 0.3870 - val_acc: 0.9092\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 284s 181ms/step - loss: 0.1304 - acc: 0.9538 - val_loss: 0.4026 - val_acc: 0.9038\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1273 - acc: 0.9557 - val_loss: 0.4113 - val_acc: 0.9054\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1252 - acc: 0.9553 - val_loss: 0.4229 - val_acc: 0.9023\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1273 - acc: 0.9546 - val_loss: 0.4063 - val_acc: 0.9059\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1215 - acc: 0.9575 - val_loss: 0.4208 - val_acc: 0.9056\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1241 - acc: 0.9564 - val_loss: 0.4135 - val_acc: 0.9056\n",
            "\n",
            "Epoch 00010: val_acc improved from -inf to 0.90560, saving model to /gdrive/My Drive/CIFAR_10/__best_val_accuracy__.h5\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1235 - acc: 0.9564 - val_loss: 0.4153 - val_acc: 0.9055\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1217 - acc: 0.9577 - val_loss: 0.4183 - val_acc: 0.9049\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1222 - acc: 0.9567 - val_loss: 0.4109 - val_acc: 0.9069\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1220 - acc: 0.9566 - val_loss: 0.3925 - val_acc: 0.9087\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1215 - acc: 0.9573 - val_loss: 0.4102 - val_acc: 0.9070\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 288s 184ms/step - loss: 0.1212 - acc: 0.9567 - val_loss: 0.4058 - val_acc: 0.9089\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1183 - acc: 0.9577 - val_loss: 0.3967 - val_acc: 0.9074\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1148 - acc: 0.9599 - val_loss: 0.3980 - val_acc: 0.9081\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1178 - acc: 0.9582 - val_loss: 0.4025 - val_acc: 0.9090\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1159 - acc: 0.9586 - val_loss: 0.4203 - val_acc: 0.9060\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.90560 to 0.90600, saving model to /gdrive/My Drive/CIFAR_10/__best_val_accuracy__.h5\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1126 - acc: 0.9605 - val_loss: 0.4274 - val_acc: 0.9031\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1179 - acc: 0.9582 - val_loss: 0.4156 - val_acc: 0.9043\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 284s 181ms/step - loss: 0.1173 - acc: 0.9586 - val_loss: 0.3945 - val_acc: 0.9090\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1163 - acc: 0.9595 - val_loss: 0.4311 - val_acc: 0.9046\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 284s 181ms/step - loss: 0.1144 - acc: 0.9597 - val_loss: 0.4188 - val_acc: 0.9063\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1154 - acc: 0.9590 - val_loss: 0.4318 - val_acc: 0.9034\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1148 - acc: 0.9600 - val_loss: 0.4303 - val_acc: 0.9044\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1136 - acc: 0.9598 - val_loss: 0.4212 - val_acc: 0.9043\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1143 - acc: 0.9593 - val_loss: 0.4411 - val_acc: 0.9018\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1150 - acc: 0.9593 - val_loss: 0.3996 - val_acc: 0.9093\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.90600 to 0.90930, saving model to /gdrive/My Drive/CIFAR_10/__best_val_accuracy__.h5\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1133 - acc: 0.9595 - val_loss: 0.4134 - val_acc: 0.9080\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1090 - acc: 0.9611 - val_loss: 0.4137 - val_acc: 0.9087\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1128 - acc: 0.9609 - val_loss: 0.4183 - val_acc: 0.9099\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1085 - acc: 0.9624 - val_loss: 0.4132 - val_acc: 0.9106\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1073 - acc: 0.9616 - val_loss: 0.4158 - val_acc: 0.9072\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1113 - acc: 0.9606 - val_loss: 0.4276 - val_acc: 0.9064\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1055 - acc: 0.9614 - val_loss: 0.4181 - val_acc: 0.9059\n",
            "Epoch 38/50\n",
            "1182/1563 [=====================>........] - ETA: 1:04 - loss: 0.1111 - acc: 0.9601Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W1NjYzvgciU4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 91.23%"
      ]
    },
    {
      "metadata": {
        "id": "G-n6BKTkAdLO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "time = str(datetime.now())\n",
        "\n",
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__best_val_accuracy__.h5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "filepath = f\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__{str(datetime.now())}__.h5\"\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='auto', \n",
        "                                period=4)\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=17, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.001)\n",
        "\n",
        "\n",
        "# earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', \n",
        "# #                                           min_delta=6.0, \n",
        "#                                           verbose=1, \n",
        "#                                           mode='max')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#######################################\n",
        "\n",
        "class EarlyStoppingByLossVal(keras.callbacks.Callback):\n",
        "    def __init__(self, monitor='loss', value=0.01, verbose=0):\n",
        "        super(keras.callbacks.Callback, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.value = value\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            print(\"Early stopping requires %s available!\" % self.monitor)\n",
        "            exit()\n",
        "\n",
        "        if current < self.value:\n",
        "            if self.verbose > 0:\n",
        "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
        "            self.model.stop_training = True\n",
        "            \n",
        "########################################\n",
        "\n",
        "earlystop = EarlyStoppingByLossVal(monitor = \"val_acc\" , value = 92 , verbose = 1)\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"22_455AM__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#                     callbacks=[reduce_lr , csv_logger , model_checkpoint ] ,\n",
        "#                     batch_size=64,\n",
        "#                     epochs=100,\n",
        "#                     verbose=1,\n",
        "#                     validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# score = model.evaluate(x_test, y_test, verbose=1)\n",
        "# print('Test loss:', score[0])\n",
        "# print('Test accuracy:', score[1])\n",
        "\n",
        "# model.save_weights(f\"/gdrive/My Drive/CIFAR_10/__23_456AM__{score[1]}.h5\")\n",
        "# print(\"model saved\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DyES3mSVO3uE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trying to reach from 91 to 92. "
      ]
    },
    {
      "metadata": {
        "id": "PvH6-8VJO3bu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "51p_eEn7AdIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "86253ad3-faaf-4261-db60-f90ab04e7783"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=1.0, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.3, \n",
        "                                  patience=2, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "#                                   min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.0001)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "                    callbacks=[reduce_lr , csv_logger , model_checkpoint ] ,\n",
        "                    batch_size=64,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.0050 - acc: 0.9986 - val_loss: 0.5394 - val_acc: 0.9141\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 230s 5ms/step - loss: 0.0051 - acc: 0.9987 - val_loss: 0.5357 - val_acc: 0.9140\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 231s 5ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.5440 - val_acc: 0.9131\n",
            "Epoch 4/10\n",
            "14848/50000 [=======>......................] - ETA: 2:31 - loss: 0.0049 - acc: 0.9988"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-X7a5I_iAdFa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmO9dHVHAdB0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qaTK_PS_68wT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "06b6c617-e1d2-439a-f81c-9ecd661ed427"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive' )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ItMd7jj8eO5S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "df7ec5c1-901f-472c-9621-fe8e30bafa39"
      },
      "cell_type": "code",
      "source": [
        "! ls \"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " __100_.h5\n",
            " __best_val_accuracy__.h5\n",
            " DNST_CIFAR10_AUG.ipynb\n",
            " epoch5_reduceLR_factor02_patience20_minlr0001_decay1e9.h5\n",
            " __epochs5_valaccuracy9080__4PM.h5\n",
            "'__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PMgCE2NnenQ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}