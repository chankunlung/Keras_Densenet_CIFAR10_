{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_DENSENET_92190_ACURACY.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hSp3AkdIXOb5",
        "colab_type": "code",
        "outputId": "8f417658-3e9e-4976-fd96-1b46756fe41a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kI8pfH2ABgB1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "----\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "T0go040tBVDY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 92.92 % ACCURACY CODE.\n",
        "## Total params: 995,230\n",
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "9-3evHVNbZ7C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Oct 22 \n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "rYiWOAWMbVmN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - 55 Epochs\n",
        "\n",
        "## Validation Accuracy - 87%"
      ]
    },
    {
      "metadata": {
        "id": "M7bXaefq4R6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#keras library\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "\n",
        "# densenet \n",
        "\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "#############################################################################\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 55\n",
        "l = 12 #L\n",
        "num_filter = 36 #k\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "#############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing data\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "#optimizer\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  if epoch <= 150:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 150 and epoch <=225:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=5)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=epochs , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5fbkIBBbkVd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 56 - 100 Epoch\n",
        "\n",
        "## Validation Accuracy - 90%"
      ]
    },
    {
      "metadata": {
        "id": "y5M__NEB4R2P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.55-0.87.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 55 \n",
        "  if epoch <= 150:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 150 and epoch <=225:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=5)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=45 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pyhemMmQO79F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 100 - 150 Epoch\n",
        "\n",
        "## Validation Accuracy - 91.99%"
      ]
    },
    {
      "metadata": {
        "id": "x92PADojXqyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4085
        },
        "outputId": "92651510-2315-43e3-bd3e-b219f9a07648"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.45-0.90.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 100 \n",
        "  if epoch <= 120:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 120 and epoch <=210:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=45 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "learning rate ---> 0.1 Epoch ---> 100\n",
            "782/782 [==============================] - 311s 398ms/step - loss: 0.2192 - acc: 0.9221 - val_loss: 0.3970 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.89070, saving model to /gdrive/My Drive/CIFAR_10/weights.01-0.89.hdf5\n",
            "Epoch 2/45\n",
            "learning rate ---> 0.1 Epoch ---> 101\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2178 - acc: 0.9235 - val_loss: 0.4981 - val_acc: 0.8796\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.89070\n",
            "Epoch 3/45\n",
            "learning rate ---> 0.1 Epoch ---> 102\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2166 - acc: 0.9241 - val_loss: 0.3381 - val_acc: 0.8995\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.89070 to 0.89950, saving model to /gdrive/My Drive/CIFAR_10/weights.03-0.90.hdf5\n",
            "Epoch 4/45\n",
            "learning rate ---> 0.1 Epoch ---> 103\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2136 - acc: 0.9255 - val_loss: 0.3725 - val_acc: 0.9003\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.89950 to 0.90030, saving model to /gdrive/My Drive/CIFAR_10/weights.04-0.90.hdf5\n",
            "Epoch 5/45\n",
            "learning rate ---> 0.1 Epoch ---> 104\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2153 - acc: 0.9232 - val_loss: 0.4007 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.90030\n",
            "Epoch 6/45\n",
            "learning rate ---> 0.1 Epoch ---> 105\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2105 - acc: 0.9265 - val_loss: 0.4289 - val_acc: 0.8861\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.90030\n",
            "Epoch 7/45\n",
            "learning rate ---> 0.1 Epoch ---> 106\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2080 - acc: 0.9275 - val_loss: 0.4266 - val_acc: 0.8850\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.90030\n",
            "Epoch 8/45\n",
            "learning rate ---> 0.1 Epoch ---> 107\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2070 - acc: 0.9271 - val_loss: 0.3861 - val_acc: 0.8960\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.90030\n",
            "Epoch 9/45\n",
            "learning rate ---> 0.1 Epoch ---> 108\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2063 - acc: 0.9269 - val_loss: 0.4347 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.90030\n",
            "Epoch 10/45\n",
            "learning rate ---> 0.1 Epoch ---> 109\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2011 - acc: 0.9303 - val_loss: 0.3889 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.90030\n",
            "Epoch 11/45\n",
            "learning rate ---> 0.1 Epoch ---> 110\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2084 - acc: 0.9276 - val_loss: 0.4106 - val_acc: 0.8974\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.90030\n",
            "Epoch 12/45\n",
            "learning rate ---> 0.1 Epoch ---> 111\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2023 - acc: 0.9291 - val_loss: 0.3885 - val_acc: 0.8978\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.90030\n",
            "Epoch 13/45\n",
            "learning rate ---> 0.1 Epoch ---> 112\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2062 - acc: 0.9273 - val_loss: 0.3962 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.90030\n",
            "Epoch 14/45\n",
            "learning rate ---> 0.1 Epoch ---> 113\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2014 - acc: 0.9301 - val_loss: 0.4303 - val_acc: 0.8925\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.90030\n",
            "Epoch 15/45\n",
            "learning rate ---> 0.1 Epoch ---> 114\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2010 - acc: 0.9301 - val_loss: 0.3969 - val_acc: 0.8951\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.90030\n",
            "Epoch 16/45\n",
            "learning rate ---> 0.1 Epoch ---> 115\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1958 - acc: 0.9309 - val_loss: 0.3985 - val_acc: 0.8961\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.90030\n",
            "Epoch 17/45\n",
            "learning rate ---> 0.1 Epoch ---> 116\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2007 - acc: 0.9298 - val_loss: 0.3586 - val_acc: 0.9019\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.90030 to 0.90190, saving model to /gdrive/My Drive/CIFAR_10/weights.17-0.90.hdf5\n",
            "Epoch 18/45\n",
            "learning rate ---> 0.1 Epoch ---> 117\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1942 - acc: 0.9316 - val_loss: 0.5048 - val_acc: 0.8779\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.90190\n",
            "Epoch 19/45\n",
            "learning rate ---> 0.1 Epoch ---> 118\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1979 - acc: 0.9309 - val_loss: 0.4842 - val_acc: 0.8773\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.90190\n",
            "Epoch 20/45\n",
            "learning rate ---> 0.1 Epoch ---> 119\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1929 - acc: 0.9328 - val_loss: 0.3742 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.90190\n",
            "Epoch 21/45\n",
            "learning rate ---> 0.1 Epoch ---> 120\n",
            "782/782 [==============================] - 297s 379ms/step - loss: 0.1877 - acc: 0.9339 - val_loss: 0.3858 - val_acc: 0.9032\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.90190 to 0.90320, saving model to /gdrive/My Drive/CIFAR_10/weights.21-0.90.hdf5\n",
            "Epoch 22/45\n",
            "learning rate ---> 0.01 Epoch ---> 121\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1597 - acc: 0.9445 - val_loss: 0.3299 - val_acc: 0.9151\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.90320 to 0.91510, saving model to /gdrive/My Drive/CIFAR_10/weights.22-0.92.hdf5\n",
            "Epoch 23/45\n",
            "learning rate ---> 0.01 Epoch ---> 122\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1446 - acc: 0.9490 - val_loss: 0.3265 - val_acc: 0.9176\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.91510 to 0.91760, saving model to /gdrive/My Drive/CIFAR_10/weights.23-0.92.hdf5\n",
            "Epoch 24/45\n",
            "learning rate ---> 0.01 Epoch ---> 123\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1431 - acc: 0.9493 - val_loss: 0.3435 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.91760\n",
            "Epoch 25/45\n",
            "learning rate ---> 0.01 Epoch ---> 124\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1368 - acc: 0.9520 - val_loss: 0.3323 - val_acc: 0.9175\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.91760\n",
            "Epoch 26/45\n",
            "learning rate ---> 0.01 Epoch ---> 125\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1346 - acc: 0.9516 - val_loss: 0.3407 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.91760\n",
            "Epoch 27/45\n",
            "learning rate ---> 0.01 Epoch ---> 126\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1291 - acc: 0.9545 - val_loss: 0.3389 - val_acc: 0.9163\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.91760\n",
            "Epoch 28/45\n",
            "learning rate ---> 0.01 Epoch ---> 127\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1314 - acc: 0.9529 - val_loss: 0.3398 - val_acc: 0.9151\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.91760\n",
            "Epoch 29/45\n",
            "learning rate ---> 0.01 Epoch ---> 128\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1307 - acc: 0.9532 - val_loss: 0.3335 - val_acc: 0.9179\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.91760 to 0.91790, saving model to /gdrive/My Drive/CIFAR_10/weights.29-0.92.hdf5\n",
            "Epoch 30/45\n",
            "learning rate ---> 0.01 Epoch ---> 129\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1272 - acc: 0.9553 - val_loss: 0.3363 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.91790\n",
            "Epoch 31/45\n",
            "learning rate ---> 0.01 Epoch ---> 130\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1260 - acc: 0.9557 - val_loss: 0.3279 - val_acc: 0.9184\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.91790 to 0.91840, saving model to /gdrive/My Drive/CIFAR_10/weights.31-0.92.hdf5\n",
            "Epoch 32/45\n",
            "learning rate ---> 0.01 Epoch ---> 131\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1263 - acc: 0.9554 - val_loss: 0.3402 - val_acc: 0.9185\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.91840 to 0.91850, saving model to /gdrive/My Drive/CIFAR_10/weights.32-0.92.hdf5\n",
            "Epoch 33/45\n",
            "learning rate ---> 0.01 Epoch ---> 132\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1235 - acc: 0.9560 - val_loss: 0.3451 - val_acc: 0.9161\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.91850\n",
            "Epoch 34/45\n",
            "learning rate ---> 0.01 Epoch ---> 133\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1269 - acc: 0.9550 - val_loss: 0.3301 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.91850 to 0.91880, saving model to /gdrive/My Drive/CIFAR_10/weights.34-0.92.hdf5\n",
            "Epoch 35/45\n",
            "learning rate ---> 0.01 Epoch ---> 134\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1269 - acc: 0.9559 - val_loss: 0.3400 - val_acc: 0.9176\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.91880\n",
            "Epoch 36/45\n",
            "learning rate ---> 0.01 Epoch ---> 135\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1219 - acc: 0.9566 - val_loss: 0.3373 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.91880\n",
            "Epoch 37/45\n",
            "learning rate ---> 0.01 Epoch ---> 136\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1214 - acc: 0.9571 - val_loss: 0.3317 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.91880 to 0.91890, saving model to /gdrive/My Drive/CIFAR_10/weights.37-0.92.hdf5\n",
            "Epoch 38/45\n",
            "learning rate ---> 0.01 Epoch ---> 137\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1204 - acc: 0.9569 - val_loss: 0.3411 - val_acc: 0.9183\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.91890\n",
            "Epoch 39/45\n",
            "learning rate ---> 0.01 Epoch ---> 138\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1228 - acc: 0.9563 - val_loss: 0.3301 - val_acc: 0.9195\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.91890 to 0.91950, saving model to /gdrive/My Drive/CIFAR_10/weights.39-0.92.hdf5\n",
            "Epoch 40/45\n",
            "learning rate ---> 0.01 Epoch ---> 139\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1207 - acc: 0.9570 - val_loss: 0.3422 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.91950\n",
            "Epoch 41/45\n",
            "learning rate ---> 0.01 Epoch ---> 140\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1171 - acc: 0.9586 - val_loss: 0.3422 - val_acc: 0.9164\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.91950\n",
            "Epoch 42/45\n",
            "learning rate ---> 0.01 Epoch ---> 141\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1179 - acc: 0.9573 - val_loss: 0.3484 - val_acc: 0.9182\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.91950\n",
            "Epoch 43/45\n",
            "learning rate ---> 0.01 Epoch ---> 142\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1199 - acc: 0.9576 - val_loss: 0.3451 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.91950\n",
            "Epoch 44/45\n",
            "learning rate ---> 0.01 Epoch ---> 143\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1180 - acc: 0.9572 - val_loss: 0.3439 - val_acc: 0.9171\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.91950\n",
            "Epoch 45/45\n",
            "learning rate ---> 0.01 Epoch ---> 144\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1180 - acc: 0.9585 - val_loss: 0.3416 - val_acc: 0.9199\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.91950 to 0.91990, saving model to /gdrive/My Drive/CIFAR_10/weights.45-0.92.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84d3660f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Pw_68Y0QjwOC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 150 - 180 Epochs\n",
        "\n",
        "## Validation Accuracy - 92.19%"
      ]
    },
    {
      "metadata": {
        "id": "ghlJJ6JnXqv8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1890
        },
        "outputId": "15c99a42-7412-490c-8cb1-d838008d99ea"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.45-0.92.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 150 \n",
        "  if epoch <= 120:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 120 and epoch <=210:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=30 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "learning rate ---> 0.01 Epoch ---> 150\n",
            "782/782 [==============================] - 315s 402ms/step - loss: 0.1195 - acc: 0.9584 - val_loss: 0.3483 - val_acc: 0.9168\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91680, saving model to /gdrive/My Drive/CIFAR_10/weights.01-0.92.hdf5\n",
            "Epoch 2/30\n",
            "learning rate ---> 0.01 Epoch ---> 151\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1170 - acc: 0.9584 - val_loss: 0.3473 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.91680 to 0.91730, saving model to /gdrive/My Drive/CIFAR_10/weights.02-0.92.hdf5\n",
            "Epoch 3/30\n",
            "learning rate ---> 0.01 Epoch ---> 152\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1156 - acc: 0.9589 - val_loss: 0.3588 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.91730\n",
            "Epoch 4/30\n",
            "learning rate ---> 0.01 Epoch ---> 153\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1175 - acc: 0.9584 - val_loss: 0.3484 - val_acc: 0.9159\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.91730\n",
            "Epoch 5/30\n",
            "learning rate ---> 0.01 Epoch ---> 154\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1154 - acc: 0.9587 - val_loss: 0.3427 - val_acc: 0.9172\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.91730\n",
            "Epoch 6/30\n",
            "learning rate ---> 0.01 Epoch ---> 155\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1115 - acc: 0.9603 - val_loss: 0.3558 - val_acc: 0.9177\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.91730 to 0.91770, saving model to /gdrive/My Drive/CIFAR_10/weights.06-0.92.hdf5\n",
            "Epoch 7/30\n",
            "learning rate ---> 0.01 Epoch ---> 156\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1153 - acc: 0.9594 - val_loss: 0.3610 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.91770\n",
            "Epoch 8/30\n",
            "learning rate ---> 0.01 Epoch ---> 157\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1119 - acc: 0.9594 - val_loss: 0.3483 - val_acc: 0.9181\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.91770 to 0.91810, saving model to /gdrive/My Drive/CIFAR_10/weights.08-0.92.hdf5\n",
            "Epoch 9/30\n",
            "learning rate ---> 0.01 Epoch ---> 158\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1172 - acc: 0.9585 - val_loss: 0.3544 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.91810\n",
            "Epoch 10/30\n",
            "learning rate ---> 0.01 Epoch ---> 159\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1132 - acc: 0.9590 - val_loss: 0.3542 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.91810\n",
            "Epoch 11/30\n",
            "learning rate ---> 0.01 Epoch ---> 160\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1127 - acc: 0.9591 - val_loss: 0.3500 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.91810\n",
            "Epoch 12/30\n",
            "learning rate ---> 0.01 Epoch ---> 161\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.1128 - acc: 0.9601 - val_loss: 0.3529 - val_acc: 0.9169\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.91810\n",
            "Epoch 13/30\n",
            "learning rate ---> 0.01 Epoch ---> 162\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1117 - acc: 0.9599 - val_loss: 0.3484 - val_acc: 0.9198\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.91810 to 0.91980, saving model to /gdrive/My Drive/CIFAR_10/weights.13-0.92.hdf5\n",
            "Epoch 14/30\n",
            "learning rate ---> 0.01 Epoch ---> 163\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1118 - acc: 0.9607 - val_loss: 0.3561 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.91980\n",
            "Epoch 15/30\n",
            "learning rate ---> 0.01 Epoch ---> 164\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1096 - acc: 0.9610 - val_loss: 0.3546 - val_acc: 0.9184\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.91980\n",
            "Epoch 16/30\n",
            "learning rate ---> 0.01 Epoch ---> 165\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1110 - acc: 0.9598 - val_loss: 0.3523 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.91980 to 0.92010, saving model to /gdrive/My Drive/CIFAR_10/weights.16-0.92.hdf5\n",
            "Epoch 17/30\n",
            "learning rate ---> 0.01 Epoch ---> 166\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1123 - acc: 0.9601 - val_loss: 0.3424 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.92010 to 0.92190, saving model to /gdrive/My Drive/CIFAR_10/weights.17-0.92.hdf5\n",
            "Epoch 18/30\n",
            "learning rate ---> 0.01 Epoch ---> 167\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1109 - acc: 0.9601 - val_loss: 0.3477 - val_acc: 0.9197\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.92190\n",
            "Epoch 19/30\n",
            "learning rate ---> 0.01 Epoch ---> 168\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1124 - acc: 0.9596 - val_loss: 0.3443 - val_acc: 0.9192\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.92190\n",
            "Epoch 20/30\n",
            "learning rate ---> 0.01 Epoch ---> 169\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1120 - acc: 0.9601 - val_loss: 0.3443 - val_acc: 0.9205\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.92190\n",
            "Epoch 21/30\n",
            "learning rate ---> 0.01 Epoch ---> 170\n",
            "717/782 [==========================>...] - ETA: 23s - loss: 0.1101 - acc: 0.9604"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZXJgKPoIf1Yd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Refresh"
      ]
    },
    {
      "metadata": {
        "id": "L2fJyCFTkEo2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10493
        },
        "outputId": "974f2d8c-b717-4f34-c28e-b0b17c285b24"
      },
      "cell_type": "code",
      "source": [
        "#keras library\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "\n",
        "# densenet \n",
        "\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "#############################################################################\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "l = 12 #L\n",
        "num_filter = 36 #k\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "#############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing data\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 44s 0us/step\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 36)   972         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 36)   144         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 36)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 18)   5832        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 18)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 54)   0           conv2d_1[0][0]                   \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 54)   216         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 54)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 18)   8748        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 18)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 72)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 72)   288         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 72)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 18)   11664       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 18)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 90)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 90)   360         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 90)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 18)   14580       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 18)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 108)  0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 108)  432         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 108)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 18)   17496       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 18)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 126)  0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 126)  504         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 126)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 18)   20412       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 18)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 144)  0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 144)  576         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 144)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 18)   23328       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 18)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 162)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 162)  648         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 162)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 18)   26244       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 18)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 180)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 180)  720         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 180)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 18)   29160       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 18)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 198)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 198)  792         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 198)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 18)   32076       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 18)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 216)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 216)  864         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 216)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 18)   34992       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 18)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 234)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 234)  936         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 234)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 18)   37908       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 18)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 252)  0           concatenate_11[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 252)  1008        concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 252)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 18)   4536        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 18)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 18)   0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 18)   72          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 18)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 18)   2916        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 18)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 36)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 36)   144         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 36)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 18)   5832        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 18)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 54)   0           concatenate_13[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 54)   216         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 54)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 18)   8748        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 16, 16, 18)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 72)   0           concatenate_14[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 72)   288         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 72)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 18)   11664       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 18)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 90)   0           concatenate_15[0][0]             \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 90)   360         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 90)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 18)   14580       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 18)   0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 108)  0           concatenate_16[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 108)  432         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 108)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 18)   17496       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 16, 16, 18)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 126)  0           concatenate_17[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 126)  504         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 126)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 18)   20412       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 18)   0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 144)  0           concatenate_18[0][0]             \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 144)  576         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 144)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 18)   23328       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 16, 16, 18)   0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 162)  0           concatenate_19[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 162)  648         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 162)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 18)   26244       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 18)   0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 180)  0           concatenate_20[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 180)  720         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 180)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 18)   29160       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16, 16, 18)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 198)  0           concatenate_21[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 198)  792         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 198)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 18)   32076       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 16, 16, 18)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 216)  0           concatenate_22[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 216)  864         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 216)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 18)   34992       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 16, 16, 18)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 234)  0           concatenate_23[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 234)  936         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 234)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 18)   4212        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 18)   0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 18)     0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 18)     72          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 18)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 18)     2916        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 8, 8, 18)     0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 36)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 36)     144         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 36)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 18)     5832        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 8, 8, 18)     0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 54)     0           concatenate_25[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 54)     216         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 54)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 18)     8748        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 8, 8, 18)     0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 72)     0           concatenate_26[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 72)     288         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 72)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 18)     11664       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 8, 8, 18)     0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 90)     0           concatenate_27[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 90)     360         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 90)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 18)     14580       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 8, 8, 18)     0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 108)    0           concatenate_28[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 108)    432         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 108)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 18)     17496       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 8, 8, 18)     0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 126)    0           concatenate_29[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 126)    504         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 126)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 18)     20412       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 8, 8, 18)     0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 144)    0           concatenate_30[0][0]             \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 144)    576         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 144)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 18)     23328       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 8, 8, 18)     0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 162)    0           concatenate_31[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 162)    648         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 162)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 18)     26244       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 8, 8, 18)     0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 180)    0           concatenate_32[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 180)    720         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 180)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 18)     29160       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 8, 8, 18)     0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 198)    0           concatenate_33[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 198)    792         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 198)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 18)     32076       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 8, 8, 18)     0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 216)    0           concatenate_34[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 216)    864         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 216)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 18)     34992       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 8, 18)     0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 234)    0           concatenate_35[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 234)    936         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 234)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 18)     4212        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 8, 8, 18)     0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 4, 4, 18)     0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 18)     72          average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 18)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 18)     2916        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 4, 4, 18)     0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 36)     0           average_pooling2d_3[0][0]        \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 36)     144         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 36)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 18)     5832        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 4, 4, 18)     0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 54)     0           concatenate_37[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 54)     216         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 54)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 18)     8748        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 4, 4, 18)     0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 72)     0           concatenate_38[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 72)     288         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 72)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 18)     11664       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 4, 4, 18)     0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 90)     0           concatenate_39[0][0]             \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 90)     360         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 90)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 18)     14580       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 4, 4, 18)     0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 108)    0           concatenate_40[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 108)    432         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 108)    0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 18)     17496       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 4, 4, 18)     0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 126)    0           concatenate_41[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 126)    504         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 126)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 18)     20412       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 4, 4, 18)     0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 144)    0           concatenate_42[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 144)    576         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 144)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 18)     23328       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 4, 4, 18)     0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 162)    0           concatenate_43[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 162)    648         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 162)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 18)     26244       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 4, 4, 18)     0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 180)    0           concatenate_44[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 180)    720         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 180)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 18)     29160       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 4, 4, 18)     0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 198)    0           concatenate_45[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 198)    792         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 198)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 18)     32076       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 4, 4, 18)     0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 216)    0           concatenate_46[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 216)    864         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 216)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 18)     34992       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 4, 4, 18)     0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 4, 4, 234)    0           concatenate_47[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 4, 234)    936         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 4, 234)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 2, 2, 234)    0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 936)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           9370        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 995,230\n",
            "Trainable params: 981,658\n",
            "Non-trainable params: 13,572\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lxnyrCdPAQ6j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Evaluate"
      ]
    },
    {
      "metadata": {
        "id": "ef2Qryab-aH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "47de03e2-a792-48b6-ea79-300ea9a2a1d3"
      },
      "cell_type": "code",
      "source": [
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.17-0.92.hdf5\")\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 24s 2ms/step\n",
            "Test loss: 0.34242540892288087\n",
            "Test accuracy: 0.9219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKZOKxOEwhiS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experimenting to increase accuracy"
      ]
    },
    {
      "metadata": {
        "id": "uRh3mGoa-SPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Multiple version of models between 180-220. We will select the best.\n",
        "\n",
        "\n",
        "## /gdrive/My Drive/CIFAR_10/weights.01-0.92.hdf5-92.25%\n",
        "## /gdrive/My Drive/CIFAR_10/weights.04-0.92.hdf5-92.24%\n",
        "##/gdrive/My Drive/CIFAR_10/__24__weights.01-0.92.hdf5-92.27%"
      ]
    },
    {
      "metadata": {
        "id": "8fR0dsBk-qK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10991
        },
        "outputId": "ecf6a090-edde-414f-80f7-040bfcc2c559"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "\n",
        "# densenet \n",
        "\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "#############################################################################\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "l = 12 #L\n",
        "num_filter = 36 #k\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "#############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing data\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 37s 0us/step\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 36)   972         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 36)   144         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 36)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 18)   5832        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 18)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 54)   0           conv2d_1[0][0]                   \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 54)   216         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 54)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 18)   8748        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 18)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 72)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 72)   288         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 72)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 18)   11664       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 18)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 90)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 90)   360         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 90)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 18)   14580       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 18)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 108)  0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 108)  432         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 108)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 18)   17496       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 18)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 126)  0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 126)  504         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 126)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 18)   20412       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 18)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 144)  0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 144)  576         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 144)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 18)   23328       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 18)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 162)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 162)  648         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 162)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 18)   26244       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 18)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 180)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 180)  720         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 180)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 18)   29160       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 18)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 198)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 198)  792         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 198)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 18)   32076       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 18)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 216)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 216)  864         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 216)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 18)   34992       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 18)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 234)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 234)  936         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 234)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 18)   37908       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 18)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 252)  0           concatenate_11[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 252)  1008        concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 252)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 18)   4536        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 18)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 18)   0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 18)   72          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 18)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 18)   2916        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 18)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 36)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 36)   144         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 36)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 18)   5832        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 18)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 54)   0           concatenate_13[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 54)   216         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 54)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 18)   8748        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 16, 16, 18)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 72)   0           concatenate_14[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 72)   288         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 72)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 18)   11664       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 18)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 90)   0           concatenate_15[0][0]             \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 90)   360         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 90)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 18)   14580       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 18)   0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 108)  0           concatenate_16[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 108)  432         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 108)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 18)   17496       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 16, 16, 18)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 126)  0           concatenate_17[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 126)  504         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 126)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 18)   20412       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 18)   0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 144)  0           concatenate_18[0][0]             \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 144)  576         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 144)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 18)   23328       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 16, 16, 18)   0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 162)  0           concatenate_19[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 162)  648         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 162)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 18)   26244       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 18)   0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 180)  0           concatenate_20[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 180)  720         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 180)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 18)   29160       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16, 16, 18)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 198)  0           concatenate_21[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 198)  792         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 198)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 18)   32076       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 16, 16, 18)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 216)  0           concatenate_22[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 216)  864         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 216)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 18)   34992       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 16, 16, 18)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 234)  0           concatenate_23[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 234)  936         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 234)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 18)   4212        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 18)   0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 18)     0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 18)     72          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 18)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 18)     2916        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 8, 8, 18)     0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 36)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 36)     144         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 36)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 18)     5832        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 8, 8, 18)     0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 54)     0           concatenate_25[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 54)     216         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 54)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 18)     8748        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 8, 8, 18)     0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 72)     0           concatenate_26[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 72)     288         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 72)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 18)     11664       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 8, 8, 18)     0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 90)     0           concatenate_27[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 90)     360         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 90)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 18)     14580       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 8, 8, 18)     0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 108)    0           concatenate_28[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 108)    432         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 108)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 18)     17496       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 8, 8, 18)     0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 126)    0           concatenate_29[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 126)    504         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 126)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 18)     20412       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 8, 8, 18)     0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 144)    0           concatenate_30[0][0]             \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 144)    576         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 144)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 18)     23328       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 8, 8, 18)     0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 162)    0           concatenate_31[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 162)    648         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 162)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 18)     26244       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 8, 8, 18)     0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 180)    0           concatenate_32[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 180)    720         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 180)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 18)     29160       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 8, 8, 18)     0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 198)    0           concatenate_33[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 198)    792         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 198)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 18)     32076       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 8, 8, 18)     0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 216)    0           concatenate_34[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 216)    864         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 216)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 18)     34992       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 8, 18)     0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 234)    0           concatenate_35[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 234)    936         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 234)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 18)     4212        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 8, 8, 18)     0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 4, 4, 18)     0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 18)     72          average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 18)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 18)     2916        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 4, 4, 18)     0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 36)     0           average_pooling2d_3[0][0]        \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 36)     144         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 36)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 18)     5832        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 4, 4, 18)     0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 54)     0           concatenate_37[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 54)     216         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 54)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 18)     8748        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 4, 4, 18)     0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 72)     0           concatenate_38[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 72)     288         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 72)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 18)     11664       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 4, 4, 18)     0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 90)     0           concatenate_39[0][0]             \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 90)     360         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 90)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 18)     14580       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 4, 4, 18)     0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 108)    0           concatenate_40[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 108)    432         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 108)    0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 18)     17496       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 4, 4, 18)     0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 126)    0           concatenate_41[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 126)    504         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 126)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 18)     20412       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 4, 4, 18)     0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 144)    0           concatenate_42[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 144)    576         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 144)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 18)     23328       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 4, 4, 18)     0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 162)    0           concatenate_43[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 162)    648         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 162)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 18)     26244       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 4, 4, 18)     0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 180)    0           concatenate_44[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 180)    720         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 180)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 18)     29160       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 4, 4, 18)     0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 198)    0           concatenate_45[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 198)    792         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 198)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 18)     32076       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 4, 4, 18)     0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 216)    0           concatenate_46[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 216)    864         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 216)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 18)     34992       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 4, 4, 18)     0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 4, 4, 234)    0           concatenate_47[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 4, 234)    936         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 4, 234)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 2, 2, 234)    0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 936)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           9370        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 995,230\n",
            "Trainable params: 981,658\n",
            "Non-trainable params: 13,572\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fLmo8NSEBiIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 180 - 220 Epochs --------> Accuracy - 92.92%"
      ]
    },
    {
      "metadata": {
        "id": "ZMfnkT-2wmFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3057
        },
        "outputId": "671a12fa-b4aa-4ad2-ceb6-34f98ea07b44"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.17-0.92.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "#     width_shift_range=0.125,\n",
        "#     height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=128),\n",
        "                    callbacks=[ csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs= 40 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "391/391 [==============================] - 276s 705ms/step - loss: 0.0772 - acc: 0.9726 - val_loss: 0.3066 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92630, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n",
            "Epoch 2/40\n",
            "391/391 [==============================] - 262s 670ms/step - loss: 0.0745 - acc: 0.9739 - val_loss: 0.3076 - val_acc: 0.9272\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.92630 to 0.92720, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.02-0.93.hdf5\n",
            "Epoch 3/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0708 - acc: 0.9752 - val_loss: 0.3173 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.92720\n",
            "Epoch 4/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0687 - acc: 0.9763 - val_loss: 0.3191 - val_acc: 0.9255\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.92720\n",
            "Epoch 5/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0702 - acc: 0.9754 - val_loss: 0.3198 - val_acc: 0.9261\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.92720\n",
            "Epoch 6/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0682 - acc: 0.9764 - val_loss: 0.3231 - val_acc: 0.9266\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.92720\n",
            "Epoch 7/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0692 - acc: 0.9749 - val_loss: 0.3205 - val_acc: 0.9275\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.92720 to 0.92750, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.07-0.93.hdf5\n",
            "Epoch 8/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0658 - acc: 0.9772 - val_loss: 0.3264 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.92750\n",
            "Epoch 9/40\n",
            "391/391 [==============================] - 263s 674ms/step - loss: 0.0664 - acc: 0.9765 - val_loss: 0.3298 - val_acc: 0.9269\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.92750\n",
            "Epoch 10/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0688 - acc: 0.9752 - val_loss: 0.3267 - val_acc: 0.9267\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.92750\n",
            "Epoch 11/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0666 - acc: 0.9765 - val_loss: 0.3269 - val_acc: 0.9266\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.92750\n",
            "Epoch 12/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0638 - acc: 0.9780 - val_loss: 0.3281 - val_acc: 0.9267\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.92750\n",
            "Epoch 13/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0645 - acc: 0.9770 - val_loss: 0.3298 - val_acc: 0.9273\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.92750\n",
            "Epoch 14/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0624 - acc: 0.9768 - val_loss: 0.3334 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.92750\n",
            "Epoch 15/40\n",
            "391/391 [==============================] - 261s 668ms/step - loss: 0.0638 - acc: 0.9774 - val_loss: 0.3333 - val_acc: 0.9272\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.92750\n",
            "Epoch 16/40\n",
            "391/391 [==============================] - 261s 668ms/step - loss: 0.0651 - acc: 0.9781 - val_loss: 0.3332 - val_acc: 0.9269\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.92750\n",
            "Epoch 17/40\n",
            "391/391 [==============================] - 262s 670ms/step - loss: 0.0630 - acc: 0.9774 - val_loss: 0.3290 - val_acc: 0.9292\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.92750 to 0.92920, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\n",
            "Epoch 18/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0630 - acc: 0.9771 - val_loss: 0.3296 - val_acc: 0.9279\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.92920\n",
            "Epoch 19/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0643 - acc: 0.9769 - val_loss: 0.3349 - val_acc: 0.9278\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.92920\n",
            "Epoch 20/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0611 - acc: 0.9776 - val_loss: 0.3391 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.92920\n",
            "Epoch 21/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0619 - acc: 0.9779 - val_loss: 0.3346 - val_acc: 0.9278\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.92920\n",
            "Epoch 22/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0622 - acc: 0.9782 - val_loss: 0.3366 - val_acc: 0.9269\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.92920\n",
            "Epoch 23/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0628 - acc: 0.9781 - val_loss: 0.3336 - val_acc: 0.9275\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.92920\n",
            "Epoch 24/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0623 - acc: 0.9775 - val_loss: 0.3375 - val_acc: 0.9260\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.92920\n",
            "Epoch 25/40\n",
            "391/391 [==============================] - 264s 674ms/step - loss: 0.0625 - acc: 0.9773 - val_loss: 0.3336 - val_acc: 0.9285\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.92920\n",
            "Epoch 26/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0594 - acc: 0.9789 - val_loss: 0.3362 - val_acc: 0.9284\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.92920\n",
            "Epoch 27/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0621 - acc: 0.9781 - val_loss: 0.3363 - val_acc: 0.9275\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.92920\n",
            "Epoch 28/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0605 - acc: 0.9776 - val_loss: 0.3360 - val_acc: 0.9274\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.92920\n",
            "Epoch 29/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0589 - acc: 0.9787 - val_loss: 0.3369 - val_acc: 0.9273\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.92920\n",
            "Epoch 30/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0584 - acc: 0.9791 - val_loss: 0.3378 - val_acc: 0.9274\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.92920\n",
            "Epoch 31/40\n",
            "391/391 [==============================] - 263s 674ms/step - loss: 0.0579 - acc: 0.9793 - val_loss: 0.3420 - val_acc: 0.9269\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.92920\n",
            "Epoch 32/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0608 - acc: 0.9782 - val_loss: 0.3387 - val_acc: 0.9267\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.92920\n",
            "Epoch 33/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0596 - acc: 0.9779 - val_loss: 0.3406 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.92920\n",
            "Epoch 34/40\n",
            "391/391 [==============================] - 266s 680ms/step - loss: 0.0600 - acc: 0.9782 - val_loss: 0.3397 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.92920\n",
            "Epoch 35/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0593 - acc: 0.9786 - val_loss: 0.3405 - val_acc: 0.9270\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.92920\n",
            "Epoch 36/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0576 - acc: 0.9789 - val_loss: 0.3415 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.92920\n",
            "Epoch 37/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0577 - acc: 0.9796 - val_loss: 0.3388 - val_acc: 0.9280\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.92920\n",
            "Epoch 38/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0585 - acc: 0.9788 - val_loss: 0.3384 - val_acc: 0.9278\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.92920\n",
            "Epoch 39/40\n",
            "391/391 [==============================] - 264s 674ms/step - loss: 0.0578 - acc: 0.9788 - val_loss: 0.3378 - val_acc: 0.9280\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.92920\n",
            "Epoch 40/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0571 - acc: 0.9796 - val_loss: 0.3439 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.92920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9def47dc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "z3Fzd1AOB8O7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Evaluate"
      ]
    },
    {
      "metadata": {
        "id": "8VMy6OdAB-d7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "b2ca236a-7bce-4dd4-a1d8-ad14daa052e6"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\")\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 25s 2ms/step\n",
            "Test loss: 0.32899246576428415\n",
            "Test accuracy: 0.9292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "akn2T40yDxpP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 10 more Epochs remaining to get 94%"
      ]
    },
    {
      "metadata": {
        "id": "I6zXkOT6IqOF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Remove Augmentation for 1 Epoch."
      ]
    },
    {
      "metadata": {
        "id": "DnanV4qJSMaM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 9266"
      ]
    },
    {
      "metadata": {
        "id": "TeoaXwDww4Vc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "b81ab7d2-d12e-4f35-95f9-3d27fd9de869"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ csv_logger , model_checkpoint],\n",
        "          batch_size=64, \n",
        "          epochs=1, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 306s 6ms/step - loss: 0.0707 - acc: 0.9737 - val_loss: 0.3346 - val_acc: 0.9266\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92660, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc440c80940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "o-3HD4VmL0Qr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJScDWKeSKYn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 9247"
      ]
    },
    {
      "metadata": {
        "id": "-NUJwSssL0Lm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "627596c0-2c6f-462c-c226-e1fef89c2b22"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ csv_logger , model_checkpoint],\n",
        "          batch_size=32, \n",
        "          epochs=1, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 351s 7ms/step - loss: 0.0909 - acc: 0.9676 - val_loss: 0.3337 - val_acc: 0.9247\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92470, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.92.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc4aafcf550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "yUkyVNN3DxZi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "EFDAWkKfSH11",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 9275"
      ]
    },
    {
      "metadata": {
        "id": "u4WJh4xhOSdj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "546a5a35-37ac-4cac-db8f-b2bca98454f8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ csv_logger , model_checkpoint],\n",
        "          batch_size=128, \n",
        "          epochs=1, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 280s 6ms/step - loss: 0.0623 - acc: 0.9781 - val_loss: 0.3314 - val_acc: 0.9275\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92750, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc440c1f978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "6hQkHGngWmCB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 9301"
      ]
    },
    {
      "metadata": {
        "id": "59u3S62DOURg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "a5254225-6396-4245-d885-974e81e2b3e7"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ csv_logger , model_checkpoint],\n",
        "          batch_size=200, \n",
        "          epochs=1, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 265s 5ms/step - loss: 0.0623 - acc: 0.9784 - val_loss: 0.3289 - val_acc: 0.9301\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.93010, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc421400ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "NPnht2pVYZlM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load 9301% model "
      ]
    },
    {
      "metadata": {
        "id": "ME7HF7zWSQXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "e8e1ff5e-4586-4068-a597-23dce9a34207"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ csv_logger , model_checkpoint],\n",
        "          batch_size=250, \n",
        "          epochs=3, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "50000/50000 [==============================] - 257s 5ms/step - loss: 0.0586 - acc: 0.9792 - val_loss: 0.3304 - val_acc: 0.9289\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92890, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n",
            "Epoch 2/3\n",
            "50000/50000 [==============================] - 243s 5ms/step - loss: 0.0578 - acc: 0.9792 - val_loss: 0.3318 - val_acc: 0.9290\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.92890 to 0.92900, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.02-0.93.hdf5\n",
            "Epoch 3/3\n",
            "50000/50000 [==============================] - 243s 5ms/step - loss: 0.0581 - acc: 0.9790 - val_loss: 0.3339 - val_acc: 0.9282\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.92900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc4199f4f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "QM08OFLyX-dF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}