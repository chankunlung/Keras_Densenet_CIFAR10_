{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_DENSENET_92190_ACURACY.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hSp3AkdIXOb5",
        "colab_type": "code",
        "outputId": "aa433501-5c82-4408-9aa7-afd39b28938d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kI8pfH2ABgB1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "----\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "T0go040tBVDY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 92 % ACCURACY CODE.\n",
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "9-3evHVNbZ7C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Oct 22 \n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "rYiWOAWMbVmN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - 55 Epochs"
      ]
    },
    {
      "metadata": {
        "id": "M7bXaefq4R6b",
        "colab_type": "code",
        "outputId": "026193ac-8f66-4545-abb1-10cb504fa493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3564
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#keras library\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "\n",
        "# densenet \n",
        "\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "#############################################################################\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "l = 12 #L\n",
        "num_filter = 36 #k\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "#############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing data\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "#optimizer\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  if epoch <= 150:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 150 and epoch <=225:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=5)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=epochs , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "learning rate ---> 0.1 Epoch ---> 0\n",
            "782/782 [==============================] - 330s 422ms/step - loss: 2.2196 - acc: 0.2376 - val_loss: 1.8861 - val_acc: 0.3230\n",
            "Epoch 2/200\n",
            "learning rate ---> 0.1 Epoch ---> 1\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 1.7384 - acc: 0.3564 - val_loss: 1.8138 - val_acc: 0.3477\n",
            "Epoch 3/200\n",
            "learning rate ---> 0.1 Epoch ---> 2\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 1.5817 - acc: 0.4188 - val_loss: 1.8889 - val_acc: 0.3938\n",
            "Epoch 4/200\n",
            "learning rate ---> 0.1 Epoch ---> 3\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 1.4471 - acc: 0.4685 - val_loss: 1.3868 - val_acc: 0.5036\n",
            "Epoch 5/200\n",
            "learning rate ---> 0.1 Epoch ---> 4\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 1.3378 - acc: 0.5141 - val_loss: 1.3788 - val_acc: 0.5284\n",
            "\n",
            "Epoch 00005: val_acc improved from -inf to 0.52840, saving model to /gdrive/My Drive/CIFAR_10/weights.05-0.53.hdf5\n",
            "Epoch 6/200\n",
            "learning rate ---> 0.1 Epoch ---> 5\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 1.2294 - acc: 0.5543 - val_loss: 1.3756 - val_acc: 0.5569\n",
            "Epoch 7/200\n",
            "learning rate ---> 0.1 Epoch ---> 6\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 1.1421 - acc: 0.5916 - val_loss: 1.0520 - val_acc: 0.6383\n",
            "Epoch 8/200\n",
            "learning rate ---> 0.1 Epoch ---> 7\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 1.0690 - acc: 0.6156 - val_loss: 1.0588 - val_acc: 0.6506\n",
            "Epoch 9/200\n",
            "learning rate ---> 0.1 Epoch ---> 8\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 1.0045 - acc: 0.6418 - val_loss: 1.0737 - val_acc: 0.6534\n",
            "Epoch 10/200\n",
            "learning rate ---> 0.1 Epoch ---> 9\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.9555 - acc: 0.6611 - val_loss: 1.6840 - val_acc: 0.5618\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.52840 to 0.56180, saving model to /gdrive/My Drive/CIFAR_10/weights.10-0.56.hdf5\n",
            "Epoch 11/200\n",
            "learning rate ---> 0.1 Epoch ---> 10\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.9094 - acc: 0.6762 - val_loss: 1.1560 - val_acc: 0.6411\n",
            "Epoch 12/200\n",
            "learning rate ---> 0.1 Epoch ---> 11\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.8609 - acc: 0.6962 - val_loss: 1.2046 - val_acc: 0.6543\n",
            "Epoch 13/200\n",
            "learning rate ---> 0.1 Epoch ---> 12\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.8129 - acc: 0.7138 - val_loss: 1.4476 - val_acc: 0.6184\n",
            "Epoch 14/200\n",
            "learning rate ---> 0.1 Epoch ---> 13\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.7767 - acc: 0.7259 - val_loss: 0.8395 - val_acc: 0.7196\n",
            "Epoch 15/200\n",
            "learning rate ---> 0.1 Epoch ---> 14\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.7388 - acc: 0.7408 - val_loss: 1.4138 - val_acc: 0.6130\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.56180 to 0.61300, saving model to /gdrive/My Drive/CIFAR_10/weights.15-0.61.hdf5\n",
            "Epoch 16/200\n",
            "learning rate ---> 0.1 Epoch ---> 15\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.7078 - acc: 0.7519 - val_loss: 1.1292 - val_acc: 0.7006\n",
            "Epoch 17/200\n",
            "learning rate ---> 0.1 Epoch ---> 16\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.6729 - acc: 0.7662 - val_loss: 0.9223 - val_acc: 0.7307\n",
            "Epoch 18/200\n",
            "learning rate ---> 0.1 Epoch ---> 17\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.6489 - acc: 0.7722 - val_loss: 1.2616 - val_acc: 0.6693\n",
            "Epoch 19/200\n",
            "learning rate ---> 0.1 Epoch ---> 18\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.6242 - acc: 0.7812 - val_loss: 0.9176 - val_acc: 0.7352\n",
            "Epoch 20/200\n",
            "learning rate ---> 0.1 Epoch ---> 19\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.6103 - acc: 0.7882 - val_loss: 0.9337 - val_acc: 0.7206\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.61300 to 0.72060, saving model to /gdrive/My Drive/CIFAR_10/weights.20-0.72.hdf5\n",
            "Epoch 21/200\n",
            "learning rate ---> 0.1 Epoch ---> 20\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.5870 - acc: 0.7981 - val_loss: 0.7333 - val_acc: 0.7768\n",
            "Epoch 22/200\n",
            "learning rate ---> 0.1 Epoch ---> 21\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.5686 - acc: 0.8021 - val_loss: 0.7439 - val_acc: 0.7777\n",
            "Epoch 23/200\n",
            "learning rate ---> 0.1 Epoch ---> 22\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.5527 - acc: 0.8107 - val_loss: 0.7608 - val_acc: 0.7738\n",
            "Epoch 24/200\n",
            "learning rate ---> 0.1 Epoch ---> 23\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.5408 - acc: 0.8129 - val_loss: 0.8040 - val_acc: 0.7647\n",
            "Epoch 25/200\n",
            "learning rate ---> 0.1 Epoch ---> 24\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.5269 - acc: 0.8179 - val_loss: 0.7063 - val_acc: 0.7950\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.72060 to 0.79500, saving model to /gdrive/My Drive/CIFAR_10/weights.25-0.80.hdf5\n",
            "Epoch 26/200\n",
            "learning rate ---> 0.1 Epoch ---> 25\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.5117 - acc: 0.8223 - val_loss: 0.8185 - val_acc: 0.7613\n",
            "Epoch 27/200\n",
            "learning rate ---> 0.1 Epoch ---> 26\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4973 - acc: 0.8278 - val_loss: 0.7127 - val_acc: 0.7842\n",
            "Epoch 28/200\n",
            "learning rate ---> 0.1 Epoch ---> 27\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4891 - acc: 0.8297 - val_loss: 0.5742 - val_acc: 0.8173\n",
            "Epoch 29/200\n",
            "learning rate ---> 0.1 Epoch ---> 28\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.4764 - acc: 0.8344 - val_loss: 0.6344 - val_acc: 0.8188\n",
            "Epoch 30/200\n",
            "learning rate ---> 0.1 Epoch ---> 29\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.4667 - acc: 0.8400 - val_loss: 0.5617 - val_acc: 0.8225\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.79500 to 0.82250, saving model to /gdrive/My Drive/CIFAR_10/weights.30-0.82.hdf5\n",
            "Epoch 31/200\n",
            "learning rate ---> 0.1 Epoch ---> 30\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.4596 - acc: 0.8390 - val_loss: 0.7951 - val_acc: 0.7836\n",
            "Epoch 32/200\n",
            "learning rate ---> 0.1 Epoch ---> 31\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.4496 - acc: 0.8443 - val_loss: 0.5340 - val_acc: 0.8401\n",
            "Epoch 33/200\n",
            "learning rate ---> 0.1 Epoch ---> 32\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4395 - acc: 0.8481 - val_loss: 0.5614 - val_acc: 0.8272\n",
            "Epoch 34/200\n",
            "learning rate ---> 0.1 Epoch ---> 33\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4313 - acc: 0.8509 - val_loss: 0.5298 - val_acc: 0.8449\n",
            "Epoch 35/200\n",
            "learning rate ---> 0.1 Epoch ---> 34\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4264 - acc: 0.8531 - val_loss: 0.5493 - val_acc: 0.8374\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.82250 to 0.83740, saving model to /gdrive/My Drive/CIFAR_10/weights.35-0.84.hdf5\n",
            "Epoch 36/200\n",
            "learning rate ---> 0.1 Epoch ---> 35\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4209 - acc: 0.8536 - val_loss: 0.5053 - val_acc: 0.8475\n",
            "Epoch 37/200\n",
            "learning rate ---> 0.1 Epoch ---> 36\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4127 - acc: 0.8577 - val_loss: 0.5009 - val_acc: 0.8495\n",
            "Epoch 38/200\n",
            "learning rate ---> 0.1 Epoch ---> 37\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4027 - acc: 0.8611 - val_loss: 0.5702 - val_acc: 0.8346\n",
            "Epoch 39/200\n",
            "learning rate ---> 0.1 Epoch ---> 38\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.3916 - acc: 0.8663 - val_loss: 0.5686 - val_acc: 0.8356\n",
            "Epoch 40/200\n",
            "learning rate ---> 0.1 Epoch ---> 39\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.3903 - acc: 0.8643 - val_loss: 0.7390 - val_acc: 0.8015\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.83740\n",
            "Epoch 41/200\n",
            "learning rate ---> 0.1 Epoch ---> 40\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3871 - acc: 0.8669 - val_loss: 0.5167 - val_acc: 0.8479\n",
            "Epoch 42/200\n",
            "learning rate ---> 0.1 Epoch ---> 41\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.3801 - acc: 0.8695 - val_loss: 0.5619 - val_acc: 0.8384\n",
            "Epoch 43/200\n",
            "learning rate ---> 0.1 Epoch ---> 42\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.3752 - acc: 0.8701 - val_loss: 0.5363 - val_acc: 0.8384\n",
            "Epoch 44/200\n",
            "learning rate ---> 0.1 Epoch ---> 43\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3672 - acc: 0.8730 - val_loss: 0.5169 - val_acc: 0.8485\n",
            "Epoch 45/200\n",
            "learning rate ---> 0.1 Epoch ---> 44\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3681 - acc: 0.8733 - val_loss: 0.5378 - val_acc: 0.8452\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.83740 to 0.84520, saving model to /gdrive/My Drive/CIFAR_10/weights.45-0.85.hdf5\n",
            "Epoch 46/200\n",
            "learning rate ---> 0.1 Epoch ---> 45\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3582 - acc: 0.8759 - val_loss: 0.4902 - val_acc: 0.8519\n",
            "Epoch 47/200\n",
            "learning rate ---> 0.1 Epoch ---> 46\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3531 - acc: 0.8764 - val_loss: 0.6048 - val_acc: 0.8305\n",
            "Epoch 48/200\n",
            "learning rate ---> 0.1 Epoch ---> 47\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3493 - acc: 0.8774 - val_loss: 0.5341 - val_acc: 0.8495\n",
            "Epoch 49/200\n",
            "learning rate ---> 0.1 Epoch ---> 48\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3468 - acc: 0.8794 - val_loss: 0.5205 - val_acc: 0.8455\n",
            "Epoch 50/200\n",
            "learning rate ---> 0.1 Epoch ---> 49\n",
            "782/782 [==============================] - 308s 394ms/step - loss: 0.3425 - acc: 0.8807 - val_loss: 0.4534 - val_acc: 0.8685\n",
            "\n",
            "Epoch 00050: val_acc improved from 0.84520 to 0.86850, saving model to /gdrive/My Drive/CIFAR_10/weights.50-0.87.hdf5\n",
            "Epoch 51/200\n",
            "learning rate ---> 0.1 Epoch ---> 50\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3391 - acc: 0.8818 - val_loss: 0.4850 - val_acc: 0.8579\n",
            "Epoch 52/200\n",
            "learning rate ---> 0.1 Epoch ---> 51\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3336 - acc: 0.8839 - val_loss: 0.4691 - val_acc: 0.8612\n",
            "Epoch 53/200\n",
            "learning rate ---> 0.1 Epoch ---> 52\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3279 - acc: 0.8863 - val_loss: 0.5298 - val_acc: 0.8474\n",
            "Epoch 54/200\n",
            "learning rate ---> 0.1 Epoch ---> 53\n",
            "782/782 [==============================] - 304s 388ms/step - loss: 0.3237 - acc: 0.8867 - val_loss: 0.4639 - val_acc: 0.8673\n",
            "Epoch 55/200\n",
            "learning rate ---> 0.1 Epoch ---> 54\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3266 - acc: 0.8869 - val_loss: 0.4372 - val_acc: 0.8737\n",
            "\n",
            "Epoch 00055: val_acc improved from 0.86850 to 0.87370, saving model to /gdrive/My Drive/CIFAR_10/weights.55-0.87.hdf5\n",
            "Epoch 56/200\n",
            "learning rate ---> 0.1 Epoch ---> 55\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3144 - acc: 0.8923 - val_loss: 0.4361 - val_acc: 0.8777\n",
            "Epoch 57/200\n",
            "learning rate ---> 0.1 Epoch ---> 56\n",
            "782/782 [==============================] - 304s 388ms/step - loss: 0.3159 - acc: 0.8897 - val_loss: 0.4662 - val_acc: 0.8630\n",
            "Epoch 58/200\n",
            "learning rate ---> 0.1 Epoch ---> 57\n",
            "179/782 [=====>........................] - ETA: 3:39 - loss: 0.2986 - acc: 0.8987"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g5fbkIBBbkVd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 56 - 100 Epoch"
      ]
    },
    {
      "metadata": {
        "id": "y5M__NEB4R2P",
        "colab_type": "code",
        "outputId": "53eda051-59b6-40cd-9a99-f4317bc3391d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2808
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.55-0.87.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 55 \n",
        "  if epoch <= 150:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 150 and epoch <=225:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=5)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=45 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "learning rate ---> 0.1 Epoch ---> 55\n",
            "782/782 [==============================] - 310s 396ms/step - loss: 0.3207 - acc: 0.8883 - val_loss: 0.6701 - val_acc: 0.8259\n",
            "Epoch 2/45\n",
            "learning rate ---> 0.1 Epoch ---> 56\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3173 - acc: 0.8905 - val_loss: 0.4855 - val_acc: 0.8667\n",
            "Epoch 3/45\n",
            "learning rate ---> 0.1 Epoch ---> 57\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3095 - acc: 0.8931 - val_loss: 0.7172 - val_acc: 0.8320\n",
            "Epoch 4/45\n",
            "learning rate ---> 0.1 Epoch ---> 58\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3087 - acc: 0.8924 - val_loss: 0.4474 - val_acc: 0.8726\n",
            "Epoch 5/45\n",
            "learning rate ---> 0.1 Epoch ---> 59\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3043 - acc: 0.8946 - val_loss: 0.5453 - val_acc: 0.8566\n",
            "\n",
            "Epoch 00005: val_acc improved from -inf to 0.85660, saving model to /gdrive/My Drive/CIFAR_10/weights.05-0.86.hdf5\n",
            "Epoch 6/45\n",
            "learning rate ---> 0.1 Epoch ---> 60\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3050 - acc: 0.8947 - val_loss: 0.4403 - val_acc: 0.8766\n",
            "Epoch 7/45\n",
            "learning rate ---> 0.1 Epoch ---> 61\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2991 - acc: 0.8955 - val_loss: 0.4877 - val_acc: 0.8668\n",
            "Epoch 8/45\n",
            "learning rate ---> 0.1 Epoch ---> 62\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2964 - acc: 0.8966 - val_loss: 0.4433 - val_acc: 0.8751\n",
            "Epoch 9/45\n",
            "learning rate ---> 0.1 Epoch ---> 63\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2996 - acc: 0.8963 - val_loss: 0.5215 - val_acc: 0.8559\n",
            "Epoch 10/45\n",
            "learning rate ---> 0.1 Epoch ---> 64\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2908 - acc: 0.8996 - val_loss: 0.4281 - val_acc: 0.8764\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.85660 to 0.87640, saving model to /gdrive/My Drive/CIFAR_10/weights.10-0.88.hdf5\n",
            "Epoch 11/45\n",
            "learning rate ---> 0.1 Epoch ---> 65\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2877 - acc: 0.9003 - val_loss: 0.4226 - val_acc: 0.8826\n",
            "Epoch 12/45\n",
            "learning rate ---> 0.1 Epoch ---> 66\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2898 - acc: 0.8996 - val_loss: 0.4707 - val_acc: 0.8706\n",
            "Epoch 13/45\n",
            "learning rate ---> 0.1 Epoch ---> 67\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2829 - acc: 0.9021 - val_loss: 0.4225 - val_acc: 0.8775\n",
            "Epoch 14/45\n",
            "learning rate ---> 0.1 Epoch ---> 68\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2805 - acc: 0.9024 - val_loss: 0.4403 - val_acc: 0.8727\n",
            "Epoch 15/45\n",
            "learning rate ---> 0.1 Epoch ---> 69\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2752 - acc: 0.9054 - val_loss: 0.5334 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.87640\n",
            "Epoch 16/45\n",
            "learning rate ---> 0.1 Epoch ---> 70\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2716 - acc: 0.9041 - val_loss: 0.4868 - val_acc: 0.8682\n",
            "Epoch 17/45\n",
            "learning rate ---> 0.1 Epoch ---> 71\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2756 - acc: 0.9030 - val_loss: 0.4488 - val_acc: 0.8802\n",
            "Epoch 18/45\n",
            "learning rate ---> 0.1 Epoch ---> 72\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2728 - acc: 0.9052 - val_loss: 0.4315 - val_acc: 0.8806\n",
            "Epoch 19/45\n",
            "learning rate ---> 0.1 Epoch ---> 73\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2656 - acc: 0.9060 - val_loss: 0.5242 - val_acc: 0.8622\n",
            "Epoch 20/45\n",
            "learning rate ---> 0.1 Epoch ---> 74\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2685 - acc: 0.9061 - val_loss: 0.4306 - val_acc: 0.8800\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.87640 to 0.88000, saving model to /gdrive/My Drive/CIFAR_10/weights.20-0.88.hdf5\n",
            "Epoch 21/45\n",
            "learning rate ---> 0.1 Epoch ---> 75\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2622 - acc: 0.9087 - val_loss: 0.4110 - val_acc: 0.8834\n",
            "Epoch 22/45\n",
            "learning rate ---> 0.1 Epoch ---> 76\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2568 - acc: 0.9105 - val_loss: 0.4648 - val_acc: 0.8709\n",
            "Epoch 23/45\n",
            "learning rate ---> 0.1 Epoch ---> 77\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2604 - acc: 0.9094 - val_loss: 0.4069 - val_acc: 0.8827\n",
            "Epoch 24/45\n",
            "learning rate ---> 0.1 Epoch ---> 78\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2582 - acc: 0.9106 - val_loss: 0.5202 - val_acc: 0.8628\n",
            "Epoch 25/45\n",
            "learning rate ---> 0.1 Epoch ---> 79\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2541 - acc: 0.9114 - val_loss: 0.3631 - val_acc: 0.8940\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.88000 to 0.89400, saving model to /gdrive/My Drive/CIFAR_10/weights.25-0.89.hdf5\n",
            "Epoch 26/45\n",
            "learning rate ---> 0.1 Epoch ---> 80\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2537 - acc: 0.9111 - val_loss: 0.4581 - val_acc: 0.8738\n",
            "Epoch 27/45\n",
            "learning rate ---> 0.1 Epoch ---> 81\n",
            "782/782 [==============================] - 297s 379ms/step - loss: 0.2524 - acc: 0.9121 - val_loss: 0.6053 - val_acc: 0.8540\n",
            "Epoch 28/45\n",
            "learning rate ---> 0.1 Epoch ---> 82\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2487 - acc: 0.9133 - val_loss: 0.4967 - val_acc: 0.8721\n",
            "Epoch 29/45\n",
            "learning rate ---> 0.1 Epoch ---> 83\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2477 - acc: 0.9143 - val_loss: 0.4230 - val_acc: 0.8830\n",
            "Epoch 30/45\n",
            "learning rate ---> 0.1 Epoch ---> 84\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2427 - acc: 0.9141 - val_loss: 0.4447 - val_acc: 0.8791\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.89400\n",
            "Epoch 31/45\n",
            "learning rate ---> 0.1 Epoch ---> 85\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2414 - acc: 0.9146 - val_loss: 0.5211 - val_acc: 0.8651\n",
            "Epoch 32/45\n",
            "learning rate ---> 0.1 Epoch ---> 86\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2419 - acc: 0.9155 - val_loss: 0.4193 - val_acc: 0.8849\n",
            "Epoch 33/45\n",
            "learning rate ---> 0.1 Epoch ---> 87\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2374 - acc: 0.9161 - val_loss: 0.4977 - val_acc: 0.8685\n",
            "Epoch 34/45\n",
            "learning rate ---> 0.1 Epoch ---> 88\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.2379 - acc: 0.9166 - val_loss: 0.4493 - val_acc: 0.8764\n",
            "Epoch 35/45\n",
            "learning rate ---> 0.1 Epoch ---> 89\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.2370 - acc: 0.9171 - val_loss: 0.5013 - val_acc: 0.8623\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.89400\n",
            "Epoch 36/45\n",
            "learning rate ---> 0.1 Epoch ---> 90\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.2322 - acc: 0.9180 - val_loss: 0.4862 - val_acc: 0.8702\n",
            "Epoch 37/45\n",
            "learning rate ---> 0.1 Epoch ---> 91\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.2339 - acc: 0.9180 - val_loss: 0.3336 - val_acc: 0.9007\n",
            "Epoch 38/45\n",
            "learning rate ---> 0.1 Epoch ---> 92\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.2311 - acc: 0.9197 - val_loss: 0.3711 - val_acc: 0.8964\n",
            "Epoch 39/45\n",
            "learning rate ---> 0.1 Epoch ---> 93\n",
            "782/782 [==============================] - 298s 382ms/step - loss: 0.2294 - acc: 0.9195 - val_loss: 0.3534 - val_acc: 0.9043\n",
            "Epoch 40/45\n",
            "learning rate ---> 0.1 Epoch ---> 94\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.2273 - acc: 0.9198 - val_loss: 0.5664 - val_acc: 0.8676\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.89400\n",
            "Epoch 41/45\n",
            "learning rate ---> 0.1 Epoch ---> 95\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2223 - acc: 0.9224 - val_loss: 0.3694 - val_acc: 0.8997\n",
            "Epoch 42/45\n",
            "learning rate ---> 0.1 Epoch ---> 96\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2252 - acc: 0.9200 - val_loss: 0.3977 - val_acc: 0.8970\n",
            "Epoch 43/45\n",
            "learning rate ---> 0.1 Epoch ---> 97\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2194 - acc: 0.9225 - val_loss: 0.4290 - val_acc: 0.8793\n",
            "Epoch 44/45\n",
            "learning rate ---> 0.1 Epoch ---> 98\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2232 - acc: 0.9216 - val_loss: 0.4166 - val_acc: 0.8897\n",
            "Epoch 45/45\n",
            "learning rate ---> 0.1 Epoch ---> 99\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2205 - acc: 0.9237 - val_loss: 0.3590 - val_acc: 0.8957\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.89400 to 0.89570, saving model to /gdrive/My Drive/CIFAR_10/weights.45-0.90.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84f03d0c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "pyhemMmQO79F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 100 - 150 Epoch"
      ]
    },
    {
      "metadata": {
        "id": "x92PADojXqyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4085
        },
        "outputId": "92651510-2315-43e3-bd3e-b219f9a07648"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.45-0.90.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 100 \n",
        "  if epoch <= 120:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 120 and epoch <=210:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=45 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "learning rate ---> 0.1 Epoch ---> 100\n",
            "782/782 [==============================] - 311s 398ms/step - loss: 0.2192 - acc: 0.9221 - val_loss: 0.3970 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.89070, saving model to /gdrive/My Drive/CIFAR_10/weights.01-0.89.hdf5\n",
            "Epoch 2/45\n",
            "learning rate ---> 0.1 Epoch ---> 101\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2178 - acc: 0.9235 - val_loss: 0.4981 - val_acc: 0.8796\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.89070\n",
            "Epoch 3/45\n",
            "learning rate ---> 0.1 Epoch ---> 102\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2166 - acc: 0.9241 - val_loss: 0.3381 - val_acc: 0.8995\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.89070 to 0.89950, saving model to /gdrive/My Drive/CIFAR_10/weights.03-0.90.hdf5\n",
            "Epoch 4/45\n",
            "learning rate ---> 0.1 Epoch ---> 103\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2136 - acc: 0.9255 - val_loss: 0.3725 - val_acc: 0.9003\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.89950 to 0.90030, saving model to /gdrive/My Drive/CIFAR_10/weights.04-0.90.hdf5\n",
            "Epoch 5/45\n",
            "learning rate ---> 0.1 Epoch ---> 104\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2153 - acc: 0.9232 - val_loss: 0.4007 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.90030\n",
            "Epoch 6/45\n",
            "learning rate ---> 0.1 Epoch ---> 105\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2105 - acc: 0.9265 - val_loss: 0.4289 - val_acc: 0.8861\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.90030\n",
            "Epoch 7/45\n",
            "learning rate ---> 0.1 Epoch ---> 106\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2080 - acc: 0.9275 - val_loss: 0.4266 - val_acc: 0.8850\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.90030\n",
            "Epoch 8/45\n",
            "learning rate ---> 0.1 Epoch ---> 107\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2070 - acc: 0.9271 - val_loss: 0.3861 - val_acc: 0.8960\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.90030\n",
            "Epoch 9/45\n",
            "learning rate ---> 0.1 Epoch ---> 108\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2063 - acc: 0.9269 - val_loss: 0.4347 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.90030\n",
            "Epoch 10/45\n",
            "learning rate ---> 0.1 Epoch ---> 109\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2011 - acc: 0.9303 - val_loss: 0.3889 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.90030\n",
            "Epoch 11/45\n",
            "learning rate ---> 0.1 Epoch ---> 110\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2084 - acc: 0.9276 - val_loss: 0.4106 - val_acc: 0.8974\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.90030\n",
            "Epoch 12/45\n",
            "learning rate ---> 0.1 Epoch ---> 111\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2023 - acc: 0.9291 - val_loss: 0.3885 - val_acc: 0.8978\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.90030\n",
            "Epoch 13/45\n",
            "learning rate ---> 0.1 Epoch ---> 112\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2062 - acc: 0.9273 - val_loss: 0.3962 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.90030\n",
            "Epoch 14/45\n",
            "learning rate ---> 0.1 Epoch ---> 113\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2014 - acc: 0.9301 - val_loss: 0.4303 - val_acc: 0.8925\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.90030\n",
            "Epoch 15/45\n",
            "learning rate ---> 0.1 Epoch ---> 114\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2010 - acc: 0.9301 - val_loss: 0.3969 - val_acc: 0.8951\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.90030\n",
            "Epoch 16/45\n",
            "learning rate ---> 0.1 Epoch ---> 115\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1958 - acc: 0.9309 - val_loss: 0.3985 - val_acc: 0.8961\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.90030\n",
            "Epoch 17/45\n",
            "learning rate ---> 0.1 Epoch ---> 116\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2007 - acc: 0.9298 - val_loss: 0.3586 - val_acc: 0.9019\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.90030 to 0.90190, saving model to /gdrive/My Drive/CIFAR_10/weights.17-0.90.hdf5\n",
            "Epoch 18/45\n",
            "learning rate ---> 0.1 Epoch ---> 117\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1942 - acc: 0.9316 - val_loss: 0.5048 - val_acc: 0.8779\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.90190\n",
            "Epoch 19/45\n",
            "learning rate ---> 0.1 Epoch ---> 118\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1979 - acc: 0.9309 - val_loss: 0.4842 - val_acc: 0.8773\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.90190\n",
            "Epoch 20/45\n",
            "learning rate ---> 0.1 Epoch ---> 119\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1929 - acc: 0.9328 - val_loss: 0.3742 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.90190\n",
            "Epoch 21/45\n",
            "learning rate ---> 0.1 Epoch ---> 120\n",
            "782/782 [==============================] - 297s 379ms/step - loss: 0.1877 - acc: 0.9339 - val_loss: 0.3858 - val_acc: 0.9032\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.90190 to 0.90320, saving model to /gdrive/My Drive/CIFAR_10/weights.21-0.90.hdf5\n",
            "Epoch 22/45\n",
            "learning rate ---> 0.01 Epoch ---> 121\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1597 - acc: 0.9445 - val_loss: 0.3299 - val_acc: 0.9151\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.90320 to 0.91510, saving model to /gdrive/My Drive/CIFAR_10/weights.22-0.92.hdf5\n",
            "Epoch 23/45\n",
            "learning rate ---> 0.01 Epoch ---> 122\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1446 - acc: 0.9490 - val_loss: 0.3265 - val_acc: 0.9176\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.91510 to 0.91760, saving model to /gdrive/My Drive/CIFAR_10/weights.23-0.92.hdf5\n",
            "Epoch 24/45\n",
            "learning rate ---> 0.01 Epoch ---> 123\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1431 - acc: 0.9493 - val_loss: 0.3435 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.91760\n",
            "Epoch 25/45\n",
            "learning rate ---> 0.01 Epoch ---> 124\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1368 - acc: 0.9520 - val_loss: 0.3323 - val_acc: 0.9175\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.91760\n",
            "Epoch 26/45\n",
            "learning rate ---> 0.01 Epoch ---> 125\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1346 - acc: 0.9516 - val_loss: 0.3407 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.91760\n",
            "Epoch 27/45\n",
            "learning rate ---> 0.01 Epoch ---> 126\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1291 - acc: 0.9545 - val_loss: 0.3389 - val_acc: 0.9163\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.91760\n",
            "Epoch 28/45\n",
            "learning rate ---> 0.01 Epoch ---> 127\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1314 - acc: 0.9529 - val_loss: 0.3398 - val_acc: 0.9151\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.91760\n",
            "Epoch 29/45\n",
            "learning rate ---> 0.01 Epoch ---> 128\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1307 - acc: 0.9532 - val_loss: 0.3335 - val_acc: 0.9179\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.91760 to 0.91790, saving model to /gdrive/My Drive/CIFAR_10/weights.29-0.92.hdf5\n",
            "Epoch 30/45\n",
            "learning rate ---> 0.01 Epoch ---> 129\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1272 - acc: 0.9553 - val_loss: 0.3363 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.91790\n",
            "Epoch 31/45\n",
            "learning rate ---> 0.01 Epoch ---> 130\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1260 - acc: 0.9557 - val_loss: 0.3279 - val_acc: 0.9184\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.91790 to 0.91840, saving model to /gdrive/My Drive/CIFAR_10/weights.31-0.92.hdf5\n",
            "Epoch 32/45\n",
            "learning rate ---> 0.01 Epoch ---> 131\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1263 - acc: 0.9554 - val_loss: 0.3402 - val_acc: 0.9185\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.91840 to 0.91850, saving model to /gdrive/My Drive/CIFAR_10/weights.32-0.92.hdf5\n",
            "Epoch 33/45\n",
            "learning rate ---> 0.01 Epoch ---> 132\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1235 - acc: 0.9560 - val_loss: 0.3451 - val_acc: 0.9161\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.91850\n",
            "Epoch 34/45\n",
            "learning rate ---> 0.01 Epoch ---> 133\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1269 - acc: 0.9550 - val_loss: 0.3301 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.91850 to 0.91880, saving model to /gdrive/My Drive/CIFAR_10/weights.34-0.92.hdf5\n",
            "Epoch 35/45\n",
            "learning rate ---> 0.01 Epoch ---> 134\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1269 - acc: 0.9559 - val_loss: 0.3400 - val_acc: 0.9176\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.91880\n",
            "Epoch 36/45\n",
            "learning rate ---> 0.01 Epoch ---> 135\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1219 - acc: 0.9566 - val_loss: 0.3373 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.91880\n",
            "Epoch 37/45\n",
            "learning rate ---> 0.01 Epoch ---> 136\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1214 - acc: 0.9571 - val_loss: 0.3317 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.91880 to 0.91890, saving model to /gdrive/My Drive/CIFAR_10/weights.37-0.92.hdf5\n",
            "Epoch 38/45\n",
            "learning rate ---> 0.01 Epoch ---> 137\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1204 - acc: 0.9569 - val_loss: 0.3411 - val_acc: 0.9183\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.91890\n",
            "Epoch 39/45\n",
            "learning rate ---> 0.01 Epoch ---> 138\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1228 - acc: 0.9563 - val_loss: 0.3301 - val_acc: 0.9195\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.91890 to 0.91950, saving model to /gdrive/My Drive/CIFAR_10/weights.39-0.92.hdf5\n",
            "Epoch 40/45\n",
            "learning rate ---> 0.01 Epoch ---> 139\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1207 - acc: 0.9570 - val_loss: 0.3422 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.91950\n",
            "Epoch 41/45\n",
            "learning rate ---> 0.01 Epoch ---> 140\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1171 - acc: 0.9586 - val_loss: 0.3422 - val_acc: 0.9164\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.91950\n",
            "Epoch 42/45\n",
            "learning rate ---> 0.01 Epoch ---> 141\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1179 - acc: 0.9573 - val_loss: 0.3484 - val_acc: 0.9182\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.91950\n",
            "Epoch 43/45\n",
            "learning rate ---> 0.01 Epoch ---> 142\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1199 - acc: 0.9576 - val_loss: 0.3451 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.91950\n",
            "Epoch 44/45\n",
            "learning rate ---> 0.01 Epoch ---> 143\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1180 - acc: 0.9572 - val_loss: 0.3439 - val_acc: 0.9171\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.91950\n",
            "Epoch 45/45\n",
            "learning rate ---> 0.01 Epoch ---> 144\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1180 - acc: 0.9585 - val_loss: 0.3416 - val_acc: 0.9199\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.91950 to 0.91990, saving model to /gdrive/My Drive/CIFAR_10/weights.45-0.92.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84d3660f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Pw_68Y0QjwOC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 150 - 180 Epochs"
      ]
    },
    {
      "metadata": {
        "id": "ghlJJ6JnXqv8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1871
        },
        "outputId": "15c99a42-7412-490c-8cb1-d838008d99ea"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.45-0.92.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 150 \n",
        "  if epoch <= 120:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 120 and epoch <=210:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=30 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "learning rate ---> 0.01 Epoch ---> 150\n",
            "782/782 [==============================] - 315s 402ms/step - loss: 0.1195 - acc: 0.9584 - val_loss: 0.3483 - val_acc: 0.9168\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91680, saving model to /gdrive/My Drive/CIFAR_10/weights.01-0.92.hdf5\n",
            "Epoch 2/30\n",
            "learning rate ---> 0.01 Epoch ---> 151\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1170 - acc: 0.9584 - val_loss: 0.3473 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.91680 to 0.91730, saving model to /gdrive/My Drive/CIFAR_10/weights.02-0.92.hdf5\n",
            "Epoch 3/30\n",
            "learning rate ---> 0.01 Epoch ---> 152\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1156 - acc: 0.9589 - val_loss: 0.3588 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.91730\n",
            "Epoch 4/30\n",
            "learning rate ---> 0.01 Epoch ---> 153\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1175 - acc: 0.9584 - val_loss: 0.3484 - val_acc: 0.9159\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.91730\n",
            "Epoch 5/30\n",
            "learning rate ---> 0.01 Epoch ---> 154\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1154 - acc: 0.9587 - val_loss: 0.3427 - val_acc: 0.9172\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.91730\n",
            "Epoch 6/30\n",
            "learning rate ---> 0.01 Epoch ---> 155\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1115 - acc: 0.9603 - val_loss: 0.3558 - val_acc: 0.9177\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.91730 to 0.91770, saving model to /gdrive/My Drive/CIFAR_10/weights.06-0.92.hdf5\n",
            "Epoch 7/30\n",
            "learning rate ---> 0.01 Epoch ---> 156\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1153 - acc: 0.9594 - val_loss: 0.3610 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.91770\n",
            "Epoch 8/30\n",
            "learning rate ---> 0.01 Epoch ---> 157\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1119 - acc: 0.9594 - val_loss: 0.3483 - val_acc: 0.9181\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.91770 to 0.91810, saving model to /gdrive/My Drive/CIFAR_10/weights.08-0.92.hdf5\n",
            "Epoch 9/30\n",
            "learning rate ---> 0.01 Epoch ---> 158\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1172 - acc: 0.9585 - val_loss: 0.3544 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.91810\n",
            "Epoch 10/30\n",
            "learning rate ---> 0.01 Epoch ---> 159\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1132 - acc: 0.9590 - val_loss: 0.3542 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.91810\n",
            "Epoch 11/30\n",
            "learning rate ---> 0.01 Epoch ---> 160\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1127 - acc: 0.9591 - val_loss: 0.3500 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.91810\n",
            "Epoch 12/30\n",
            "learning rate ---> 0.01 Epoch ---> 161\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.1128 - acc: 0.9601 - val_loss: 0.3529 - val_acc: 0.9169\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.91810\n",
            "Epoch 13/30\n",
            "learning rate ---> 0.01 Epoch ---> 162\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1117 - acc: 0.9599 - val_loss: 0.3484 - val_acc: 0.9198\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.91810 to 0.91980, saving model to /gdrive/My Drive/CIFAR_10/weights.13-0.92.hdf5\n",
            "Epoch 14/30\n",
            "learning rate ---> 0.01 Epoch ---> 163\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1118 - acc: 0.9607 - val_loss: 0.3561 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.91980\n",
            "Epoch 15/30\n",
            "learning rate ---> 0.01 Epoch ---> 164\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1096 - acc: 0.9610 - val_loss: 0.3546 - val_acc: 0.9184\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.91980\n",
            "Epoch 16/30\n",
            "learning rate ---> 0.01 Epoch ---> 165\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1110 - acc: 0.9598 - val_loss: 0.3523 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.91980 to 0.92010, saving model to /gdrive/My Drive/CIFAR_10/weights.16-0.92.hdf5\n",
            "Epoch 17/30\n",
            "learning rate ---> 0.01 Epoch ---> 166\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1123 - acc: 0.9601 - val_loss: 0.3424 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.92010 to 0.92190, saving model to /gdrive/My Drive/CIFAR_10/weights.17-0.92.hdf5\n",
            "Epoch 18/30\n",
            "learning rate ---> 0.01 Epoch ---> 167\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1109 - acc: 0.9601 - val_loss: 0.3477 - val_acc: 0.9197\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.92190\n",
            "Epoch 19/30\n",
            "learning rate ---> 0.01 Epoch ---> 168\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1124 - acc: 0.9596 - val_loss: 0.3443 - val_acc: 0.9192\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.92190\n",
            "Epoch 20/30\n",
            "learning rate ---> 0.01 Epoch ---> 169\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1120 - acc: 0.9601 - val_loss: 0.3443 - val_acc: 0.9205\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.92190\n",
            "Epoch 21/30\n",
            "learning rate ---> 0.01 Epoch ---> 170\n",
            "717/782 [==========================>...] - ETA: 23s - loss: 0.1101 - acc: 0.9604"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L2fJyCFTkEo2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10493
        },
        "outputId": "974f2d8c-b717-4f34-c28e-b0b17c285b24"
      },
      "cell_type": "code",
      "source": [
        "#keras library\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "\n",
        "# densenet \n",
        "\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "#############################################################################\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "l = 12 #L\n",
        "num_filter = 36 #k\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "#############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing data\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 44s 0us/step\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 36)   972         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 36)   144         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 36)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 18)   5832        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 18)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 54)   0           conv2d_1[0][0]                   \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 54)   216         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 54)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 18)   8748        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 18)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 72)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 72)   288         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 72)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 18)   11664       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 18)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 90)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 90)   360         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 90)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 18)   14580       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 18)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 108)  0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 108)  432         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 108)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 18)   17496       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 18)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 126)  0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 126)  504         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 126)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 18)   20412       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 18)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 144)  0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 144)  576         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 144)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 18)   23328       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 18)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 162)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 162)  648         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 162)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 18)   26244       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 18)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 180)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 180)  720         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 180)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 18)   29160       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 18)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 198)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 198)  792         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 198)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 18)   32076       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 18)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 216)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 216)  864         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 216)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 18)   34992       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 18)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 234)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 234)  936         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 234)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 18)   37908       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 18)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 252)  0           concatenate_11[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 252)  1008        concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 252)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 18)   4536        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 18)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 18)   0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 18)   72          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 18)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 18)   2916        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 18)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 36)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 36)   144         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 36)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 18)   5832        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 18)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 54)   0           concatenate_13[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 54)   216         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 54)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 18)   8748        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 16, 16, 18)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 72)   0           concatenate_14[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 72)   288         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 72)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 18)   11664       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 18)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 90)   0           concatenate_15[0][0]             \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 90)   360         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 90)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 18)   14580       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 18)   0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 108)  0           concatenate_16[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 108)  432         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 108)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 18)   17496       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 16, 16, 18)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 126)  0           concatenate_17[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 126)  504         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 126)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 18)   20412       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 18)   0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 144)  0           concatenate_18[0][0]             \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 144)  576         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 144)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 18)   23328       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 16, 16, 18)   0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 162)  0           concatenate_19[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 162)  648         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 162)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 18)   26244       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 18)   0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 180)  0           concatenate_20[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 180)  720         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 180)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 18)   29160       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16, 16, 18)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 198)  0           concatenate_21[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 198)  792         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 198)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 18)   32076       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 16, 16, 18)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 216)  0           concatenate_22[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 216)  864         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 216)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 18)   34992       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 16, 16, 18)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 234)  0           concatenate_23[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 234)  936         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 234)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 18)   4212        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 18)   0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 18)     0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 18)     72          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 18)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 18)     2916        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 8, 8, 18)     0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 36)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 36)     144         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 36)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 18)     5832        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 8, 8, 18)     0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 54)     0           concatenate_25[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 54)     216         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 54)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 18)     8748        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 8, 8, 18)     0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 72)     0           concatenate_26[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 72)     288         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 72)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 18)     11664       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 8, 8, 18)     0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 90)     0           concatenate_27[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 90)     360         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 90)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 18)     14580       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 8, 8, 18)     0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 108)    0           concatenate_28[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 108)    432         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 108)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 18)     17496       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 8, 8, 18)     0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 126)    0           concatenate_29[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 126)    504         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 126)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 18)     20412       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 8, 8, 18)     0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 144)    0           concatenate_30[0][0]             \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 144)    576         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 144)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 18)     23328       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 8, 8, 18)     0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 162)    0           concatenate_31[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 162)    648         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 162)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 18)     26244       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 8, 8, 18)     0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 180)    0           concatenate_32[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 180)    720         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 180)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 18)     29160       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 8, 8, 18)     0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 198)    0           concatenate_33[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 198)    792         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 198)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 18)     32076       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 8, 8, 18)     0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 216)    0           concatenate_34[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 216)    864         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 216)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 18)     34992       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 8, 18)     0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 234)    0           concatenate_35[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 234)    936         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 234)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 18)     4212        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 8, 8, 18)     0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 4, 4, 18)     0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 18)     72          average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 18)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 18)     2916        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 4, 4, 18)     0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 36)     0           average_pooling2d_3[0][0]        \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 36)     144         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 36)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 18)     5832        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 4, 4, 18)     0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 54)     0           concatenate_37[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 54)     216         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 54)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 18)     8748        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 4, 4, 18)     0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 72)     0           concatenate_38[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 72)     288         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 72)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 18)     11664       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 4, 4, 18)     0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 90)     0           concatenate_39[0][0]             \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 90)     360         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 90)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 18)     14580       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 4, 4, 18)     0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 108)    0           concatenate_40[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 108)    432         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 108)    0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 18)     17496       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 4, 4, 18)     0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 126)    0           concatenate_41[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 126)    504         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 126)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 18)     20412       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 4, 4, 18)     0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 144)    0           concatenate_42[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 144)    576         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 144)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 18)     23328       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 4, 4, 18)     0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 162)    0           concatenate_43[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 162)    648         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 162)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 18)     26244       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 4, 4, 18)     0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 180)    0           concatenate_44[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 180)    720         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 180)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 18)     29160       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 4, 4, 18)     0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 198)    0           concatenate_45[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 198)    792         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 198)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 18)     32076       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 4, 4, 18)     0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 216)    0           concatenate_46[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 216)    864         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 216)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 18)     34992       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 4, 4, 18)     0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 4, 4, 234)    0           concatenate_47[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 4, 234)    936         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 4, 234)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 2, 2, 234)    0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 936)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           9370        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 995,230\n",
            "Trainable params: 981,658\n",
            "Non-trainable params: 13,572\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lxnyrCdPAQ6j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Evaluate"
      ]
    },
    {
      "metadata": {
        "id": "ef2Qryab-aH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "47de03e2-a792-48b6-ea79-300ea9a2a1d3"
      },
      "cell_type": "code",
      "source": [
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.17-0.92.hdf5\")\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 24s 2ms/step\n",
            "Test loss: 0.34242540892288087\n",
            "Test accuracy: 0.9219\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}