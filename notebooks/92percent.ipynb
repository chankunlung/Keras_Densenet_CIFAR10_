{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kmzDpcDMXLrN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hSp3AkdIXOb5",
        "colab_type": "code",
        "outputId": "fbdb78d2-2ddf-4d99-8041-d817693dd59d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vs3EE0hk464w",
        "colab_type": "code",
        "outputId": "94d3c99b-5a86-4c43-b626-45f0fe076634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8458
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "l = 10\n",
        "num_filter = 40\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "\n",
        "\n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# img_height = 32 \n",
        "# img_width = 32\n",
        "# channel = 3\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate=0)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate=0)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "# Fourth_Block = add_denseblock(Third_Transition, num_filter, dropout_rate)\n",
        "# Fourth_Transition = add_transition(Fourth_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "# top_5_accuracy = keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "\n",
        "\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "filepath = f'/gdrive/My Drive/CIFAR_10/__{epochs}_.h5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='auto', \n",
        "                                period=5)\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_center=True,\n",
        "#     featurewise_std_normalization=True,\n",
        "#     brightness_range = [0.2 , 0.6],\n",
        "#     rotation_range=15,\n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "     fill_mode='constant',\n",
        "#     zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=epochs , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True)\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#                     batch_size=batch_size,\n",
        "#                     epochs=epochs,\n",
        "#                     verbose=1,\n",
        "#                     validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"version_1_Test_acuracy_0.8616_DNST_model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 32, 32, 40)   1080        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 32, 32, 40)   160         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 32, 32, 40)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 32, 32, 20)   7200        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 32, 32, 60)   0           conv2d_45[0][0]                  \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 32, 32, 60)   240         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 32, 32, 60)   0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 32, 32, 20)   10800       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 32, 32, 80)   0           concatenate_41[0][0]             \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 32, 32, 80)   320         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 32, 32, 80)   0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 32, 32, 20)   14400       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 32, 32, 100)  0           concatenate_42[0][0]             \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 32, 32, 100)  400         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 32, 32, 100)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 32, 32, 20)   18000       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 32, 32, 120)  0           concatenate_43[0][0]             \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 32, 32, 120)  480         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 32, 32, 120)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 32, 32, 20)   21600       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 32, 32, 140)  0           concatenate_44[0][0]             \n",
            "                                                                 conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 32, 32, 140)  560         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 32, 32, 140)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 32, 32, 20)   25200       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 32, 32, 160)  0           concatenate_45[0][0]             \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 32, 32, 160)  640         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 32, 32, 160)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 32, 32, 20)   28800       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 32, 32, 180)  0           concatenate_46[0][0]             \n",
            "                                                                 conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 32, 32, 180)  720         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 32, 32, 180)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 32, 32, 20)   32400       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 32, 32, 200)  0           concatenate_47[0][0]             \n",
            "                                                                 conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 32, 32, 200)  800         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 32, 32, 200)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 32, 32, 20)   36000       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_49 (Concatenate)    (None, 32, 32, 220)  0           concatenate_48[0][0]             \n",
            "                                                                 conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 32, 32, 220)  880         concatenate_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 32, 32, 220)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 32, 32, 20)   39600       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_50 (Concatenate)    (None, 32, 32, 240)  0           concatenate_49[0][0]             \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 32, 32, 240)  960         concatenate_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 32, 32, 240)  0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 32, 32, 20)   4800        activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 16, 16, 20)   0           conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 16, 16, 20)   80          average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 16, 16, 20)   0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 16, 16, 20)   3600        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 16, 16, 20)   0           conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_51 (Concatenate)    (None, 16, 16, 40)   0           average_pooling2d_5[0][0]        \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 16, 16, 40)   160         concatenate_51[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 16, 16, 40)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 16, 16, 20)   7200        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 16, 16, 20)   0           conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_52 (Concatenate)    (None, 16, 16, 60)   0           concatenate_51[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 16, 16, 60)   240         concatenate_52[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 16, 16, 60)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 16, 16, 20)   10800       activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 16, 16, 20)   0           conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_53 (Concatenate)    (None, 16, 16, 80)   0           concatenate_52[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 16, 16, 80)   320         concatenate_53[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 16, 16, 80)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 16, 16, 20)   14400       activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 16, 16, 20)   0           conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_54 (Concatenate)    (None, 16, 16, 100)  0           concatenate_53[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 16, 16, 100)  400         concatenate_54[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 16, 16, 100)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 16, 16, 20)   18000       activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 16, 16, 20)   0           conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_55 (Concatenate)    (None, 16, 16, 120)  0           concatenate_54[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 16, 16, 120)  480         concatenate_55[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 16, 16, 120)  0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 16, 16, 20)   21600       activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 16, 16, 20)   0           conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_56 (Concatenate)    (None, 16, 16, 140)  0           concatenate_55[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 16, 16, 140)  560         concatenate_56[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 16, 16, 140)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 16, 16, 20)   25200       activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 16, 16, 20)   0           conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_57 (Concatenate)    (None, 16, 16, 160)  0           concatenate_56[0][0]             \n",
            "                                                                 dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 16, 16, 160)  640         concatenate_57[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 16, 16, 160)  0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 16, 16, 20)   28800       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 16, 16, 20)   0           conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_58 (Concatenate)    (None, 16, 16, 180)  0           concatenate_57[0][0]             \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 16, 16, 180)  720         concatenate_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 16, 16, 180)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 16, 16, 20)   32400       activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 16, 16, 20)   0           conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_59 (Concatenate)    (None, 16, 16, 200)  0           concatenate_58[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 16, 16, 200)  800         concatenate_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 16, 16, 200)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 16, 16, 20)   36000       activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 16, 16, 20)   0           conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_60 (Concatenate)    (None, 16, 16, 220)  0           concatenate_59[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 16, 16, 220)  880         concatenate_60[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 16, 16, 220)  0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 16, 16, 20)   4400        activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 16, 16, 20)   0           conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 8, 8, 20)     0           dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 8, 8, 20)     80          average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 8, 8, 20)     0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 8, 8, 20)     3600        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 8, 8, 20)     0           conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_61 (Concatenate)    (None, 8, 8, 40)     0           average_pooling2d_6[0][0]        \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 8, 8, 40)     160         concatenate_61[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 8, 8, 40)     0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 8, 8, 20)     7200        activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 8, 8, 20)     0           conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_62 (Concatenate)    (None, 8, 8, 60)     0           concatenate_61[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 8, 8, 60)     240         concatenate_62[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 8, 8, 60)     0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 8, 8, 20)     10800       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 8, 8, 20)     0           conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_63 (Concatenate)    (None, 8, 8, 80)     0           concatenate_62[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 8, 8, 80)     320         concatenate_63[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 8, 8, 80)     0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 8, 8, 20)     14400       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 8, 8, 20)     0           conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_64 (Concatenate)    (None, 8, 8, 100)    0           concatenate_63[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 8, 8, 100)    400         concatenate_64[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 8, 8, 100)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 8, 8, 20)     18000       activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 8, 8, 20)     0           conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_65 (Concatenate)    (None, 8, 8, 120)    0           concatenate_64[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 8, 8, 120)    480         concatenate_65[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 8, 8, 120)    0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 8, 8, 20)     21600       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 8, 8, 20)     0           conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_66 (Concatenate)    (None, 8, 8, 140)    0           concatenate_65[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 8, 8, 140)    560         concatenate_66[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 8, 8, 140)    0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 8, 8, 20)     25200       activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 8, 8, 20)     0           conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_67 (Concatenate)    (None, 8, 8, 160)    0           concatenate_66[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 8, 8, 160)    640         concatenate_67[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 8, 8, 160)    0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 20)     28800       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 8, 8, 20)     0           conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_68 (Concatenate)    (None, 8, 8, 180)    0           concatenate_67[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 180)    720         concatenate_68[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 180)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 20)     32400       activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 8, 8, 20)     0           conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_69 (Concatenate)    (None, 8, 8, 200)    0           concatenate_68[0][0]             \n",
            "                                                                 dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 200)    800         concatenate_69[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 200)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 8, 8, 20)     36000       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 8, 8, 20)     0           conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_70 (Concatenate)    (None, 8, 8, 220)    0           concatenate_69[0][0]             \n",
            "                                                                 dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 220)    880         concatenate_70[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 220)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 8, 8, 20)     4400        activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 8, 8, 20)     0           conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 4, 4, 20)     0           dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 4, 4, 20)     80          average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 4, 4, 20)     0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 4, 4, 20)     3600        activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 4, 4, 20)     0           conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_71 (Concatenate)    (None, 4, 4, 40)     0           average_pooling2d_7[0][0]        \n",
            "                                                                 dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 4, 4, 40)     160         concatenate_71[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 4, 4, 40)     0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 4, 4, 20)     7200        activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 4, 4, 20)     0           conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_72 (Concatenate)    (None, 4, 4, 60)     0           concatenate_71[0][0]             \n",
            "                                                                 dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 4, 4, 60)     240         concatenate_72[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 4, 4, 60)     0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 4, 4, 20)     10800       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 4, 4, 20)     0           conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_73 (Concatenate)    (None, 4, 4, 80)     0           concatenate_72[0][0]             \n",
            "                                                                 dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 4, 4, 80)     320         concatenate_73[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 4, 4, 80)     0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 4, 4, 20)     14400       activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 4, 4, 20)     0           conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_74 (Concatenate)    (None, 4, 4, 100)    0           concatenate_73[0][0]             \n",
            "                                                                 dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 4, 4, 100)    400         concatenate_74[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 4, 4, 100)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 4, 4, 20)     18000       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 4, 4, 20)     0           conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_75 (Concatenate)    (None, 4, 4, 120)    0           concatenate_74[0][0]             \n",
            "                                                                 dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 4, 4, 120)    480         concatenate_75[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 4, 4, 120)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 4, 4, 20)     21600       activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 4, 4, 20)     0           conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_76 (Concatenate)    (None, 4, 4, 140)    0           concatenate_75[0][0]             \n",
            "                                                                 dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 4, 4, 140)    560         concatenate_76[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 4, 4, 140)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 4, 4, 20)     25200       activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 4, 4, 20)     0           conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_77 (Concatenate)    (None, 4, 4, 160)    0           concatenate_76[0][0]             \n",
            "                                                                 dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 4, 4, 160)    640         concatenate_77[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 4, 4, 160)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 4, 4, 20)     28800       activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 4, 4, 20)     0           conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_78 (Concatenate)    (None, 4, 4, 180)    0           concatenate_77[0][0]             \n",
            "                                                                 dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 4, 4, 180)    720         concatenate_78[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 4, 4, 180)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 4, 4, 20)     32400       activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 4, 4, 20)     0           conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_79 (Concatenate)    (None, 4, 4, 200)    0           concatenate_78[0][0]             \n",
            "                                                                 dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 4, 4, 200)    800         concatenate_79[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 4, 4, 200)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 4, 4, 20)     36000       activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 4, 4, 20)     0           conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_80 (Concatenate)    (None, 4, 4, 220)    0           concatenate_79[0][0]             \n",
            "                                                                 dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 4, 4, 220)    880         concatenate_80[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 4, 4, 220)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 2, 2, 220)    0           activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 880)          0           average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           8810        flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 873,490\n",
            "Trainable params: 862,490\n",
            "Non-trainable params: 11,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J_Cxf4vSACTw",
        "colab_type": "code",
        "outputId": "02756e6b-e32d-4b8f-ed25-462e010ce4f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1400
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__100_.h5\")\n",
        "\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    callbacks=[csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=30 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "782/782 [==============================] - 241s 308ms/step - loss: 0.3818 - acc: 0.8668 - val_loss: 0.7583 - val_acc: 0.8081\n",
            "Epoch 2/30\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.3715 - acc: 0.8700 - val_loss: 0.6519 - val_acc: 0.8222\n",
            "Epoch 3/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.3637 - acc: 0.8735 - val_loss: 0.6595 - val_acc: 0.8304\n",
            "Epoch 4/30\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.3547 - acc: 0.8778 - val_loss: 0.6298 - val_acc: 0.8171\n",
            "Epoch 5/30\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.3397 - acc: 0.8827 - val_loss: 0.5769 - val_acc: 0.8460\n",
            "\n",
            "Epoch 00005: acc did not improve from 0.89532\n",
            "Epoch 6/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.3377 - acc: 0.8840 - val_loss: 0.6445 - val_acc: 0.8235\n",
            "Epoch 7/30\n",
            "782/782 [==============================] - 239s 306ms/step - loss: 0.3313 - acc: 0.8840 - val_loss: 0.8150 - val_acc: 0.7903\n",
            "Epoch 8/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.3227 - acc: 0.8879 - val_loss: 0.6252 - val_acc: 0.8286\n",
            "Epoch 9/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.3158 - acc: 0.8918 - val_loss: 0.5758 - val_acc: 0.8420\n",
            "Epoch 10/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.3099 - acc: 0.8933 - val_loss: 0.5252 - val_acc: 0.8564\n",
            "\n",
            "Epoch 00010: acc did not improve from 0.89532\n",
            "Epoch 11/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.3044 - acc: 0.8934 - val_loss: 0.6116 - val_acc: 0.8390\n",
            "Epoch 12/30\n",
            "782/782 [==============================] - 239s 306ms/step - loss: 0.2981 - acc: 0.8968 - val_loss: 0.6420 - val_acc: 0.8318\n",
            "Epoch 13/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2915 - acc: 0.8992 - val_loss: 0.6612 - val_acc: 0.8234\n",
            "Epoch 14/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2828 - acc: 0.9009 - val_loss: 0.5908 - val_acc: 0.8453\n",
            "Epoch 15/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.2793 - acc: 0.9031 - val_loss: 0.6474 - val_acc: 0.8332\n",
            "\n",
            "Epoch 00015: acc improved from 0.89532 to 0.90304, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 16/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.2776 - acc: 0.9039 - val_loss: 0.4799 - val_acc: 0.8666\n",
            "Epoch 17/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.2702 - acc: 0.9074 - val_loss: 0.5622 - val_acc: 0.8509\n",
            "Epoch 18/30\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.2719 - acc: 0.9054 - val_loss: 0.6810 - val_acc: 0.8297\n",
            "Epoch 19/30\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.2723 - acc: 0.9055 - val_loss: 0.5474 - val_acc: 0.8573\n",
            "Epoch 20/30\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.2602 - acc: 0.9096 - val_loss: 0.5303 - val_acc: 0.8560\n",
            "\n",
            "Epoch 00020: acc improved from 0.90304 to 0.90978, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 21/30\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.2581 - acc: 0.9093 - val_loss: 0.5575 - val_acc: 0.8580\n",
            "Epoch 22/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2524 - acc: 0.9128 - val_loss: 0.5067 - val_acc: 0.8669\n",
            "Epoch 23/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2475 - acc: 0.9146 - val_loss: 0.5383 - val_acc: 0.8618\n",
            "Epoch 24/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2446 - acc: 0.9145 - val_loss: 0.6258 - val_acc: 0.8462\n",
            "Epoch 25/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2402 - acc: 0.9168 - val_loss: 0.5809 - val_acc: 0.8508\n",
            "\n",
            "Epoch 00025: acc improved from 0.90978 to 0.91678, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 26/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2402 - acc: 0.9160 - val_loss: 0.5746 - val_acc: 0.8595\n",
            "Epoch 27/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2392 - acc: 0.9167 - val_loss: 0.7056 - val_acc: 0.8259\n",
            "Epoch 28/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2333 - acc: 0.9186 - val_loss: 0.5773 - val_acc: 0.8523\n",
            "Epoch 29/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2285 - acc: 0.9193 - val_loss: 0.4250 - val_acc: 0.8870\n",
            "Epoch 30/30\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.2268 - acc: 0.9208 - val_loss: 0.5870 - val_acc: 0.8558\n",
            "\n",
            "Epoch 00030: acc improved from 0.91678 to 0.92080, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f39352ef588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "JdpSIzdcdy11",
        "colab_type": "code",
        "outputId": "2af74b76-6856-4036-d623-6a70292e8d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 18s 2ms/step\n",
            "Test loss: 0.5870169169366359\n",
            "Test accuracy: 0.8558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3wBePIk5Adku",
        "colab_type": "code",
        "outputId": "bd4ba4d2-3e26-438c-c97c-7400bd5f4917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1026
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__100_.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=5, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    callbacks=[reduce_lr , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=20 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.2232 - acc: 0.9221 - val_loss: 0.7536 - val_acc: 0.8318\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2292 - acc: 0.9192 - val_loss: 0.6578 - val_acc: 0.8404\n",
            "Epoch 3/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.2169 - acc: 0.9241 - val_loss: 0.6643 - val_acc: 0.8382\n",
            "Epoch 4/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.2145 - acc: 0.9239 - val_loss: 0.7835 - val_acc: 0.8382\n",
            "Epoch 5/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.2128 - acc: 0.9247 - val_loss: 0.4297 - val_acc: 0.8819\n",
            "\n",
            "Epoch 00005: acc improved from 0.92080 to 0.92458, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 6/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2126 - acc: 0.9248 - val_loss: 0.4650 - val_acc: 0.8811\n",
            "Epoch 7/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2048 - acc: 0.9267 - val_loss: 0.5597 - val_acc: 0.8593\n",
            "Epoch 8/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2059 - acc: 0.9279 - val_loss: 0.4287 - val_acc: 0.8863\n",
            "Epoch 9/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2031 - acc: 0.9289 - val_loss: 0.5942 - val_acc: 0.8485\n",
            "Epoch 10/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2017 - acc: 0.9286 - val_loss: 0.6702 - val_acc: 0.8461\n",
            "\n",
            "Epoch 00010: acc improved from 0.92458 to 0.92856, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 11/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1975 - acc: 0.9306 - val_loss: 0.5712 - val_acc: 0.8602\n",
            "Epoch 12/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1990 - acc: 0.9297 - val_loss: 0.4948 - val_acc: 0.8773\n",
            "Epoch 13/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1902 - acc: 0.9328 - val_loss: 0.5292 - val_acc: 0.8699\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 14/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1578 - acc: 0.9446 - val_loss: 0.3693 - val_acc: 0.9031\n",
            "Epoch 15/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1387 - acc: 0.9521 - val_loss: 0.3751 - val_acc: 0.9039\n",
            "\n",
            "Epoch 00015: acc improved from 0.92856 to 0.95216, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 16/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1374 - acc: 0.9513 - val_loss: 0.3712 - val_acc: 0.9072\n",
            "Epoch 17/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1317 - acc: 0.9542 - val_loss: 0.3720 - val_acc: 0.9058\n",
            "Epoch 18/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1294 - acc: 0.9550 - val_loss: 0.3807 - val_acc: 0.9045\n",
            "Epoch 19/20\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.1278 - acc: 0.9554 - val_loss: 0.3831 - val_acc: 0.9077\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 20/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1260 - acc: 0.9561 - val_loss: 0.3873 - val_acc: 0.9037\n",
            "\n",
            "Epoch 00020: acc improved from 0.95216 to 0.95610, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3951053198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "_8G1hOdGAdgM",
        "colab_type": "code",
        "outputId": "135a918a-b8ed-4cdd-9012-dabcb81d3829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 18s 2ms/step\n",
            "Test loss: 0.3872690258145332\n",
            "Test accuracy: 0.9037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f1W4g0OKAddM",
        "colab_type": "code",
        "outputId": "5c1646e2-9b4a-4760-cf8a-de1d65bf5d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1045
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__100_.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=5, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    callbacks=[reduce_lr , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=20 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "782/782 [==============================] - 240s 306ms/step - loss: 0.1268 - acc: 0.9557 - val_loss: 0.3831 - val_acc: 0.9070\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1238 - acc: 0.9566 - val_loss: 0.3676 - val_acc: 0.9091\n",
            "Epoch 3/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1211 - acc: 0.9570 - val_loss: 0.3865 - val_acc: 0.9062\n",
            "Epoch 4/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1197 - acc: 0.9585 - val_loss: 0.3870 - val_acc: 0.9057\n",
            "Epoch 5/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1200 - acc: 0.9568 - val_loss: 0.3863 - val_acc: 0.9045\n",
            "\n",
            "Epoch 00005: acc improved from 0.95610 to 0.95680, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 6/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1202 - acc: 0.9574 - val_loss: 0.3924 - val_acc: 0.9060\n",
            "Epoch 7/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1204 - acc: 0.9572 - val_loss: 0.3795 - val_acc: 0.9072\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 8/20\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9591Epoch 9/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1159 - acc: 0.9582 - val_loss: 0.3787 - val_acc: 0.9096\n",
            "Epoch 10/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1150 - acc: 0.9592 - val_loss: 0.4011 - val_acc: 0.9055\n",
            "\n",
            "Epoch 00010: acc improved from 0.95680 to 0.95924, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 11/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1132 - acc: 0.9601 - val_loss: 0.3944 - val_acc: 0.9055\n",
            "Epoch 12/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1150 - acc: 0.9600 - val_loss: 0.3968 - val_acc: 0.9037\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 13/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1120 - acc: 0.9612 - val_loss: 0.3897 - val_acc: 0.9065\n",
            "Epoch 14/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1117 - acc: 0.9601 - val_loss: 0.3946 - val_acc: 0.9074\n",
            "Epoch 15/20\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.1132 - acc: 0.9594 - val_loss: 0.3994 - val_acc: 0.9073\n",
            "\n",
            "Epoch 00015: acc improved from 0.95924 to 0.95940, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 16/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1107 - acc: 0.9615 - val_loss: 0.3856 - val_acc: 0.9090\n",
            "Epoch 17/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1100 - acc: 0.9613 - val_loss: 0.4063 - val_acc: 0.9049\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 18/20\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1086 - acc: 0.9619 - val_loss: 0.3925 - val_acc: 0.9086\n",
            "Epoch 19/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1120 - acc: 0.9609 - val_loss: 0.3913 - val_acc: 0.9089\n",
            "Epoch 20/20\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.1095 - acc: 0.9624 - val_loss: 0.3916 - val_acc: 0.9098\n",
            "\n",
            "Epoch 00020: acc improved from 0.95940 to 0.96244, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3951062198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "1gm3SIbHAdaK",
        "colab_type": "code",
        "outputId": "c803c53f-2dfa-468a-c194-38c8dad302f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__100_.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=1.0, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=2, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.01)\n",
        "\n",
        "\n",
        "#model_checkpoint\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    callbacks=[reduce_lr , csv_logger ] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=5 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.1124 - acc: 0.9594 - val_loss: 0.4045 - val_acc: 0.9059\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1070 - acc: 0.9617 - val_loss: 0.4026 - val_acc: 0.9069\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1074 - acc: 0.9620 - val_loss: 0.4031 - val_acc: 0.9077\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1093 - acc: 0.9615 - val_loss: 0.3968 - val_acc: 0.9082\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.1088 - acc: 0.9604 - val_loss: 0.3989 - val_acc: 0.9080\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3951045f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "mGOPQ8cpAdXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(\"/gdrive/My Drive/CIFAR_10/__epochs5_valaccuracy9080__4PM.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e9ulR1shAdUW",
        "colab_type": "code",
        "outputId": "a7cfac0e-9acb-4ea1-f6e7-6ca61efc6fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__epochs5_valaccuracy9080__4PM.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=1.0, decay=1e-9, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=20, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.001)\n",
        "\n",
        "\n",
        "#model_checkpoint\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    callbacks=[reduce_lr , csv_logger ] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=30 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 285s 183ms/step - loss: 0.1321 - acc: 0.9533 - val_loss: 0.4043 - val_acc: 0.9068\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 282s 180ms/step - loss: 0.1284 - acc: 0.9545 - val_loss: 0.3935 - val_acc: 0.9089\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 281s 180ms/step - loss: 0.1317 - acc: 0.9530 - val_loss: 0.4387 - val_acc: 0.8971\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1276 - acc: 0.9538 - val_loss: 0.4143 - val_acc: 0.9042\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1255 - acc: 0.9563 - val_loss: 0.4020 - val_acc: 0.9054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3950fcf1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "AUshjZ3rAdRU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(\"/gdrive/My Drive/CIFAR_10/epoch5_reduceLR_factor02_patience20_minlr0001_decay1e9.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e4rJ75Gr9kaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 91.06 % Accuracy at 34 Epoch"
      ]
    },
    {
      "metadata": {
        "id": "i0Cf9j55AdOK",
        "colab_type": "code",
        "outputId": "7c190bff-af35-4b7d-aca7-6b06160f2e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1605
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__epochs5_valaccuracy9080__4PM.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "filepath = \"/gdrive/My Drive/CIFAR_10/__best_val_accuracy__.h5\"\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='auto', \n",
        "                                period=10)\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=17, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.001)\n",
        "\n",
        "\n",
        "#model_checkpoint\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    callbacks=[reduce_lr , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=50 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1301 - acc: 0.9535 - val_loss: 0.4220 - val_acc: 0.9030\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 281s 180ms/step - loss: 0.1310 - acc: 0.9538 - val_loss: 0.4199 - val_acc: 0.9043\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 282s 180ms/step - loss: 0.1282 - acc: 0.9542 - val_loss: 0.4010 - val_acc: 0.9055\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1298 - acc: 0.9538 - val_loss: 0.3870 - val_acc: 0.9092\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 284s 181ms/step - loss: 0.1304 - acc: 0.9538 - val_loss: 0.4026 - val_acc: 0.9038\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1273 - acc: 0.9557 - val_loss: 0.4113 - val_acc: 0.9054\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1252 - acc: 0.9553 - val_loss: 0.4229 - val_acc: 0.9023\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1273 - acc: 0.9546 - val_loss: 0.4063 - val_acc: 0.9059\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1215 - acc: 0.9575 - val_loss: 0.4208 - val_acc: 0.9056\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1241 - acc: 0.9564 - val_loss: 0.4135 - val_acc: 0.9056\n",
            "\n",
            "Epoch 00010: val_acc improved from -inf to 0.90560, saving model to /gdrive/My Drive/CIFAR_10/__best_val_accuracy__.h5\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1235 - acc: 0.9564 - val_loss: 0.4153 - val_acc: 0.9055\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1217 - acc: 0.9577 - val_loss: 0.4183 - val_acc: 0.9049\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1222 - acc: 0.9567 - val_loss: 0.4109 - val_acc: 0.9069\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1220 - acc: 0.9566 - val_loss: 0.3925 - val_acc: 0.9087\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1215 - acc: 0.9573 - val_loss: 0.4102 - val_acc: 0.9070\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 288s 184ms/step - loss: 0.1212 - acc: 0.9567 - val_loss: 0.4058 - val_acc: 0.9089\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1183 - acc: 0.9577 - val_loss: 0.3967 - val_acc: 0.9074\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1148 - acc: 0.9599 - val_loss: 0.3980 - val_acc: 0.9081\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1178 - acc: 0.9582 - val_loss: 0.4025 - val_acc: 0.9090\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1159 - acc: 0.9586 - val_loss: 0.4203 - val_acc: 0.9060\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.90560 to 0.90600, saving model to /gdrive/My Drive/CIFAR_10/__best_val_accuracy__.h5\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1126 - acc: 0.9605 - val_loss: 0.4274 - val_acc: 0.9031\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1179 - acc: 0.9582 - val_loss: 0.4156 - val_acc: 0.9043\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 284s 181ms/step - loss: 0.1173 - acc: 0.9586 - val_loss: 0.3945 - val_acc: 0.9090\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 284s 182ms/step - loss: 0.1163 - acc: 0.9595 - val_loss: 0.4311 - val_acc: 0.9046\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 284s 181ms/step - loss: 0.1144 - acc: 0.9597 - val_loss: 0.4188 - val_acc: 0.9063\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1154 - acc: 0.9590 - val_loss: 0.4318 - val_acc: 0.9034\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1148 - acc: 0.9600 - val_loss: 0.4303 - val_acc: 0.9044\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1136 - acc: 0.9598 - val_loss: 0.4212 - val_acc: 0.9043\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1143 - acc: 0.9593 - val_loss: 0.4411 - val_acc: 0.9018\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1150 - acc: 0.9593 - val_loss: 0.3996 - val_acc: 0.9093\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.90600 to 0.90930, saving model to /gdrive/My Drive/CIFAR_10/__best_val_accuracy__.h5\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1133 - acc: 0.9595 - val_loss: 0.4134 - val_acc: 0.9080\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1090 - acc: 0.9611 - val_loss: 0.4137 - val_acc: 0.9087\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 283s 181ms/step - loss: 0.1128 - acc: 0.9609 - val_loss: 0.4183 - val_acc: 0.9099\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1085 - acc: 0.9624 - val_loss: 0.4132 - val_acc: 0.9106\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1073 - acc: 0.9616 - val_loss: 0.4158 - val_acc: 0.9072\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1113 - acc: 0.9606 - val_loss: 0.4276 - val_acc: 0.9064\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 282s 181ms/step - loss: 0.1055 - acc: 0.9614 - val_loss: 0.4181 - val_acc: 0.9059\n",
            "Epoch 38/50\n",
            "1182/1563 [=====================>........] - ETA: 1:04 - loss: 0.1111 - acc: 0.9601Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W1NjYzvgciU4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 91.23%"
      ]
    },
    {
      "metadata": {
        "id": "G-n6BKTkAdLO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "time = str(datetime.now())\n",
        "\n",
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__best_val_accuracy__.h5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "filepath = f\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__{str(datetime.now())}__.h5\"\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='auto', \n",
        "                                period=4)\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1, \n",
        "                                  patience=17, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "                                  min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.001)\n",
        "\n",
        "\n",
        "# earlystop = keras.callbacks.EarlyStopping(monitor='val_acc', \n",
        "# #                                           min_delta=6.0, \n",
        "#                                           verbose=1, \n",
        "#                                           mode='max')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#######################################\n",
        "\n",
        "class EarlyStoppingByLossVal(keras.callbacks.Callback):\n",
        "    def __init__(self, monitor='loss', value=0.01, verbose=0):\n",
        "        super(keras.callbacks.Callback, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.value = value\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            print(\"Early stopping requires %s available!\" % self.monitor)\n",
        "            exit()\n",
        "\n",
        "        if current < self.value:\n",
        "            if self.verbose > 0:\n",
        "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
        "            self.model.stop_training = True\n",
        "            \n",
        "########################################\n",
        "\n",
        "earlystop = EarlyStoppingByLossVal(monitor = \"val_acc\" , value = 92 , verbose = 1)\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"22_455AM__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#                     callbacks=[reduce_lr , csv_logger , model_checkpoint ] ,\n",
        "#                     batch_size=64,\n",
        "#                     epochs=100,\n",
        "#                     verbose=1,\n",
        "#                     validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# score = model.evaluate(x_test, y_test, verbose=1)\n",
        "# print('Test loss:', score[0])\n",
        "# print('Test accuracy:', score[1])\n",
        "\n",
        "# model.save_weights(f\"/gdrive/My Drive/CIFAR_10/__23_456AM__{score[1]}.h5\")\n",
        "# print(\"model saved\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DyES3mSVO3uE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trying to reach from 91 to 92. "
      ]
    },
    {
      "metadata": {
        "id": "PvH6-8VJO3bu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "51p_eEn7AdIZ",
        "colab_type": "code",
        "outputId": "86253ad3-faaf-4261-db60-f90ab04e7783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1557
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.2, \n",
        "                                  patience=1, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "#                                   min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.0001)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "                    callbacks=[reduce_lr , csv_logger , model_checkpoint ] ,\n",
        "                    batch_size=64,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.0050 - acc: 0.9986 - val_loss: 0.5394 - val_acc: 0.9141\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 230s 5ms/step - loss: 0.0051 - acc: 0.9987 - val_loss: 0.5357 - val_acc: 0.9140\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 231s 5ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.5440 - val_acc: 0.9131\n",
            "Epoch 4/10\n",
            "47296/50000 [===========================>..] - ETA: 11s - loss: 0.0051 - acc: 0.9986"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f3a5e8e19c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "OLcMMOScQvlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# reduced the learning rate. To check whether the training accuracy will cross 99% in the above cell. "
      ]
    },
    {
      "metadata": {
        "id": "pTAHfrgNQvBL",
        "colab_type": "code",
        "outputId": "56babc46-362b-4f99-e00f-87e2b6fd4230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.2, \n",
        "                                  patience=1, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "#                                   min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.00001)\n",
        "\n",
        "#reduce_lr\n",
        "model.fit(x_train, y_train,\n",
        "                    callbacks=[ csv_logger , model_checkpoint ] ,\n",
        "                    batch_size=64,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 217s 4ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.5524 - val_acc: 0.9129\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0050 - acc: 0.9987 - val_loss: 0.5720 - val_acc: 0.9106\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.5469 - val_acc: 0.9115\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0051 - acc: 0.9986 - val_loss: 0.5494 - val_acc: 0.9122\n",
            "\n",
            "Epoch 00004: acc improved from 0.98836 to 0.99860, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.5439 - val_acc: 0.9154\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0054 - acc: 0.9986 - val_loss: 0.5543 - val_acc: 0.9121\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 217s 4ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.5445 - val_acc: 0.9133\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 217s 4ms/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.5529 - val_acc: 0.9134\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 217s 4ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.5446 - val_acc: 0.9126\n",
            "\n",
            "Epoch 00009: acc improved from 0.99860 to 0.99878, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0051 - acc: 0.9986 - val_loss: 0.5613 - val_acc: 0.9127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fef0c0edfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "YpLDSkm6Sg8-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# want to increase val accuracy"
      ]
    },
    {
      "metadata": {
        "id": "EpZygE_1SmMd",
        "colab_type": "code",
        "outputId": "59fb561d-8b65-45dd-a559-fb1b1be4083b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        }
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "filepath = f\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__{str(datetime.now())}__.h5\"\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='auto', \n",
        "                                period=4)\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.2, \n",
        "                                  patience=1, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "#                                   min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.00001)\n",
        "\n",
        "#reduce_lr\n",
        "model.fit(x_train, y_train,\n",
        "                    callbacks=[reduce_lr ,  csv_logger , model_checkpoint ] ,\n",
        "                    batch_size=64,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 218s 4ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.5491 - val_acc: 0.9133\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 218s 4ms/step - loss: 0.0051 - acc: 0.9987 - val_loss: 0.5474 - val_acc: 0.9133\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 218s 4ms/step - loss: 0.0048 - acc: 0.9987 - val_loss: 0.5561 - val_acc: 0.9126\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 218s 4ms/step - loss: 0.0050 - acc: 0.9986 - val_loss: 0.5440 - val_acc: 0.9145\n",
            "\n",
            "Epoch 00004: val_acc improved from -inf to 0.91450, saving model to /gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-22 10:22:32.831683__.h5\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.5454 - val_acc: 0.9137\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.5446 - val_acc: 0.9140\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0046 - acc: 0.9989 - val_loss: 0.5441 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 218s 4ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.5464 - val_acc: 0.9137\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.91450\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 217s 4ms/step - loss: 0.0050 - acc: 0.9986 - val_loss: 0.5436 - val_acc: 0.9138\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 218s 4ms/step - loss: 0.0045 - acc: 0.9988 - val_loss: 0.5468 - val_acc: 0.9138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fef0c0e4780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "K5IgD2EwcmVe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# increase val accuracy with data augmentation - model fit generator\n"
      ]
    },
    {
      "metadata": {
        "id": "cfCaXmEqcmBO",
        "colab_type": "code",
        "outputId": "21afdd8c-b3c2-4e9a-9d3f-4db908178119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=1.0, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "filepath = f\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__{str(datetime.now())}__.h5\"\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='auto', \n",
        "                                period=4)\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.2, \n",
        "                                  patience=1, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "#                                   min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.00001)\n",
        "\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_center=True,\n",
        "#     featurewise_std_normalization=True,\n",
        "    brightness_range = [0.1 , 0.5],\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    fill_mode='constant',\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    callbacks=[reduce_lr , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/100,\n",
        "                    epochs=10 , \n",
        "                   verbose = 1 , \n",
        "#                    workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ee4e20e2f41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6UgM9RzadLkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZjpVnGardLiK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9ElU7B8dLgC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HgZYAzH3dLeG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePy_irpcdLaT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zK0zDtIRdLYS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2StAqdNQdLWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bGk0ue2udLSb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kQLeTgM9fENX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "UCt72VjwdLQT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zbdP9nkzdLOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ojCB1NEdLLG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pGuxYc9dLJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4mCiotXkdLHO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RHLW1il6dLE4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ErHj-xS9dLBa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSYYmCkKdK_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K4rEQgRrdK9F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rRc4XrSsdK5a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HLCr54zcdK3Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fSgWcrpodK1R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PtoQ7LFUdKzK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBf5H37OdKwM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-X7a5I_iAdFa",
        "colab_type": "code",
        "outputId": "36dd3604-83bc-4e42-a80f-bfa55df97ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.2, \n",
        "                                  patience=1, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "#                                   min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.00001)\n",
        "\n",
        "#reduce_lr\n",
        "model.fit(x_train, y_train,\n",
        "                    callbacks=[  csv_logger , model_checkpoint ] ,\n",
        "                    batch_size=64,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 243s 5ms/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.5366 - val_acc: 0.9148\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.5360 - val_acc: 0.9149\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 233s 5ms/step - loss: 0.0044 - acc: 0.9991 - val_loss: 0.5349 - val_acc: 0.9149\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 233s 5ms/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.5327 - val_acc: 0.9147\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 233s 5ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.5358 - val_acc: 0.9151\n",
            "\n",
            "Epoch 00005: acc improved from -inf to 0.99882, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 234s 5ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.5367 - val_acc: 0.9145\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.5351 - val_acc: 0.9145\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 233s 5ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.5367 - val_acc: 0.9144\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 233s 5ms/step - loss: 0.0048 - acc: 0.9987 - val_loss: 0.5346 - val_acc: 0.9144\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.5363 - val_acc: 0.9142\n",
            "\n",
            "Epoch 00010: acc did not improve from 0.99882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f94eb36d7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "tsLJDrHYwoDB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Increased initial Learning rate in SGD."
      ]
    },
    {
      "metadata": {
        "id": "PrCuJU68wn3K",
        "colab_type": "code",
        "outputId": "cd17cc29-b3f7-4ee9-dcd9-183b98679643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=1.0, decay=1e-3, momentum=0.3, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.2, \n",
        "                                  patience=1, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "#                                   min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.00001)\n",
        "\n",
        "#reduce_lr\n",
        "model.fit(x_train, y_train,\n",
        "                    callbacks=[  csv_logger , model_checkpoint ] ,\n",
        "                    batch_size=64,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "#0.0003\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 241s 5ms/step - loss: 1.9566 - acc: 0.3168 - val_loss: 2.1539 - val_acc: 0.3153\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 1.2380 - acc: 0.5493 - val_loss: 1.2703 - val_acc: 0.5646\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.9266 - acc: 0.6672 - val_loss: 1.0416 - val_acc: 0.6291\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 233s 5ms/step - loss: 0.7654 - acc: 0.7285 - val_loss: 0.8721 - val_acc: 0.7005\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.6632 - acc: 0.7660 - val_loss: 1.1580 - val_acc: 0.6476\n",
            "\n",
            "Epoch 00005: acc improved from -inf to 0.76602, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.5805 - acc: 0.7951 - val_loss: 0.7414 - val_acc: 0.7534\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.5295 - acc: 0.8173 - val_loss: 0.6545 - val_acc: 0.7880\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.4819 - acc: 0.8317 - val_loss: 0.6275 - val_acc: 0.7970\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.4407 - acc: 0.8447 - val_loss: 0.6719 - val_acc: 0.7922\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 232s 5ms/step - loss: 0.4080 - acc: 0.8564 - val_loss: 0.6787 - val_acc: 0.7960\n",
            "\n",
            "Epoch 00010: acc improved from 0.76602 to 0.85644, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f94e4c41240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yIHfbR4RDcdk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# continuing above execution with extra 10 epochs"
      ]
    },
    {
      "metadata": {
        "id": "RruA8hzRwn0m",
        "colab_type": "code",
        "outputId": "e7c6aaa9-0308-43cd-8e77-3e5048fd064f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__100_.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=1.0, decay=1e-3, momentum=0.3, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.2, \n",
        "                                  patience=1, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "#                                   min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.00001)\n",
        "\n",
        "#reduce_lr\n",
        "model.fit(x_train, y_train,\n",
        "                    callbacks=[  csv_logger , model_checkpoint ] ,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "#0.0003\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 282s 6ms/step - loss: 0.7174 - acc: 0.7542 - val_loss: 0.7687 - val_acc: 0.7469\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.4818 - acc: 0.8347 - val_loss: 0.5174 - val_acc: 0.8321\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 271s 5ms/step - loss: 0.3913 - acc: 0.8661 - val_loss: 0.5272 - val_acc: 0.8344\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 274s 5ms/step - loss: 0.3386 - acc: 0.8830 - val_loss: 0.4464 - val_acc: 0.8595\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 0.2976 - acc: 0.8974 - val_loss: 0.4653 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00005: acc improved from 0.85644 to 0.89740, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 0.2676 - acc: 0.9070 - val_loss: 0.4419 - val_acc: 0.8667\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 0.2497 - acc: 0.9127 - val_loss: 0.5217 - val_acc: 0.8519\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 270s 5ms/step - loss: 0.2266 - acc: 0.9208 - val_loss: 0.5039 - val_acc: 0.8635\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 273s 5ms/step - loss: 0.2117 - acc: 0.9259 - val_loss: 0.4514 - val_acc: 0.8714\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 272s 5ms/step - loss: 0.1965 - acc: 0.9305 - val_loss: 0.4480 - val_acc: 0.8770\n",
            "\n",
            "Epoch 00010: acc improved from 0.89740 to 0.93046, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9556a46748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "QYsV2DIyW4_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# increasing the epochs"
      ]
    },
    {
      "metadata": {
        "id": "_SBvrjD1W4J1",
        "colab_type": "code",
        "outputId": "c1c815af-32d7-4864-987e-9310fd1403ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__100_.h5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.3, nesterov=False)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.2, \n",
        "                                  patience=1, \n",
        "                                  verbose=1, \n",
        "                                  mode='auto', \n",
        "#                                   min_delta=0.0001, \n",
        "                                  cooldown=0, \n",
        "                                  min_lr=0.00001)\n",
        "\n",
        "#reduce_lr\n",
        "model.fit(x_train, y_train,\n",
        "                    callbacks=[  csv_logger , model_checkpoint ] ,\n",
        "                    batch_size=64,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "#0.0003\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 240s 5ms/step - loss: 0.1497 - acc: 0.9482 - val_loss: 0.4145 - val_acc: 0.8890\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 230s 5ms/step - loss: 0.1426 - acc: 0.9503 - val_loss: 0.4122 - val_acc: 0.8904\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.1412 - acc: 0.9510 - val_loss: 0.4145 - val_acc: 0.8890\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.1387 - acc: 0.9518 - val_loss: 0.4170 - val_acc: 0.8886\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.1370 - acc: 0.9519 - val_loss: 0.4156 - val_acc: 0.8890\n",
            "\n",
            "Epoch 00005: acc improved from 0.93046 to 0.95190, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.1362 - acc: 0.9521 - val_loss: 0.4192 - val_acc: 0.8890\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.1357 - acc: 0.9537 - val_loss: 0.4201 - val_acc: 0.8886\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.1356 - acc: 0.9531 - val_loss: 0.4190 - val_acc: 0.8895\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.1370 - acc: 0.9519 - val_loss: 0.4197 - val_acc: 0.8890\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.1340 - acc: 0.9538 - val_loss: 0.4210 - val_acc: 0.8891\n",
            "\n",
            "Epoch 00010: acc improved from 0.95190 to 0.95384, saving model to /gdrive/My Drive/CIFAR_10/__100_.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f94deb14240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "UtKKfgNlFBVi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# collecting information from github code"
      ]
    },
    {
      "metadata": {
        "id": "fckNU7i6wnxm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr_reducer      = ReduceLROnPlateau(monitor='val_acc', factor=np.sqrt(0.1),\n",
        "                                    cooldown=0, patience=5, min_lr=1e-5)\n",
        "model_checkpoint= ModelCheckpoint(weights_file, monitor=\"val_acc\", save_best_only=True,\n",
        "                                  save_weights_only=True, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_acc', factor=np.sqrt(0.1),\n",
        "                                  cooldown=0, patience=3, min_lr=1e-6)\n",
        "  \"\"\"\n",
        "  model_checkpoint = ModelCheckpoint(check_point_file, monitor=\"val_acc\", save_best_only=True,\n",
        "                                save_weights_only=True, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmO9dHVHAdB0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qaTK_PS_68wT",
        "colab_type": "code",
        "outputId": "392be7e6-644b-43d0-81fa-e02b89b2cbff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive' )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ItMd7jj8eO5S",
        "colab_type": "code",
        "outputId": "df7ec5c1-901f-472c-9621-fe8e30bafa39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "! ls \"/gdrive/My Drive/CIFAR_10/__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " __100_.h5\n",
            " __best_val_accuracy__.h5\n",
            " DNST_CIFAR10_AUG.ipynb\n",
            " epoch5_reduceLR_factor02_patience20_minlr0001_decay1e9.h5\n",
            " __epochs5_valaccuracy9080__4PM.h5\n",
            "'__oct23_best_val_accuracy__2018-10-21 23:28:59.643586__.h5'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PMgCE2NnenQ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tK6bC-nt4SVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-FBl-bW4SSg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Hqh9tqW4SP3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uSCYbyaW4SMV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jU3TKUTL4SJi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eqlYxoJG4SFx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8PBahsVA4SC3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N7-xbEN94R_2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-nU5scnn4R9V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9-3evHVNbZ7C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Oct 22 \n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "rYiWOAWMbVmN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - 55 Epochs"
      ]
    },
    {
      "metadata": {
        "id": "M7bXaefq4R6b",
        "colab_type": "code",
        "outputId": "026193ac-8f66-4545-abb1-10cb504fa493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3564
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#keras library\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "\n",
        "# densenet \n",
        "\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "#############################################################################\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "l = 12 #L\n",
        "num_filter = 36 #k\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "#############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing data\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "#optimizer\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  if epoch <= 150:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 150 and epoch <=225:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=5)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=epochs , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "learning rate ---> 0.1 Epoch ---> 0\n",
            "782/782 [==============================] - 330s 422ms/step - loss: 2.2196 - acc: 0.2376 - val_loss: 1.8861 - val_acc: 0.3230\n",
            "Epoch 2/200\n",
            "learning rate ---> 0.1 Epoch ---> 1\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 1.7384 - acc: 0.3564 - val_loss: 1.8138 - val_acc: 0.3477\n",
            "Epoch 3/200\n",
            "learning rate ---> 0.1 Epoch ---> 2\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 1.5817 - acc: 0.4188 - val_loss: 1.8889 - val_acc: 0.3938\n",
            "Epoch 4/200\n",
            "learning rate ---> 0.1 Epoch ---> 3\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 1.4471 - acc: 0.4685 - val_loss: 1.3868 - val_acc: 0.5036\n",
            "Epoch 5/200\n",
            "learning rate ---> 0.1 Epoch ---> 4\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 1.3378 - acc: 0.5141 - val_loss: 1.3788 - val_acc: 0.5284\n",
            "\n",
            "Epoch 00005: val_acc improved from -inf to 0.52840, saving model to /gdrive/My Drive/CIFAR_10/weights.05-0.53.hdf5\n",
            "Epoch 6/200\n",
            "learning rate ---> 0.1 Epoch ---> 5\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 1.2294 - acc: 0.5543 - val_loss: 1.3756 - val_acc: 0.5569\n",
            "Epoch 7/200\n",
            "learning rate ---> 0.1 Epoch ---> 6\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 1.1421 - acc: 0.5916 - val_loss: 1.0520 - val_acc: 0.6383\n",
            "Epoch 8/200\n",
            "learning rate ---> 0.1 Epoch ---> 7\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 1.0690 - acc: 0.6156 - val_loss: 1.0588 - val_acc: 0.6506\n",
            "Epoch 9/200\n",
            "learning rate ---> 0.1 Epoch ---> 8\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 1.0045 - acc: 0.6418 - val_loss: 1.0737 - val_acc: 0.6534\n",
            "Epoch 10/200\n",
            "learning rate ---> 0.1 Epoch ---> 9\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.9555 - acc: 0.6611 - val_loss: 1.6840 - val_acc: 0.5618\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.52840 to 0.56180, saving model to /gdrive/My Drive/CIFAR_10/weights.10-0.56.hdf5\n",
            "Epoch 11/200\n",
            "learning rate ---> 0.1 Epoch ---> 10\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.9094 - acc: 0.6762 - val_loss: 1.1560 - val_acc: 0.6411\n",
            "Epoch 12/200\n",
            "learning rate ---> 0.1 Epoch ---> 11\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.8609 - acc: 0.6962 - val_loss: 1.2046 - val_acc: 0.6543\n",
            "Epoch 13/200\n",
            "learning rate ---> 0.1 Epoch ---> 12\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.8129 - acc: 0.7138 - val_loss: 1.4476 - val_acc: 0.6184\n",
            "Epoch 14/200\n",
            "learning rate ---> 0.1 Epoch ---> 13\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.7767 - acc: 0.7259 - val_loss: 0.8395 - val_acc: 0.7196\n",
            "Epoch 15/200\n",
            "learning rate ---> 0.1 Epoch ---> 14\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.7388 - acc: 0.7408 - val_loss: 1.4138 - val_acc: 0.6130\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.56180 to 0.61300, saving model to /gdrive/My Drive/CIFAR_10/weights.15-0.61.hdf5\n",
            "Epoch 16/200\n",
            "learning rate ---> 0.1 Epoch ---> 15\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.7078 - acc: 0.7519 - val_loss: 1.1292 - val_acc: 0.7006\n",
            "Epoch 17/200\n",
            "learning rate ---> 0.1 Epoch ---> 16\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.6729 - acc: 0.7662 - val_loss: 0.9223 - val_acc: 0.7307\n",
            "Epoch 18/200\n",
            "learning rate ---> 0.1 Epoch ---> 17\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.6489 - acc: 0.7722 - val_loss: 1.2616 - val_acc: 0.6693\n",
            "Epoch 19/200\n",
            "learning rate ---> 0.1 Epoch ---> 18\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.6242 - acc: 0.7812 - val_loss: 0.9176 - val_acc: 0.7352\n",
            "Epoch 20/200\n",
            "learning rate ---> 0.1 Epoch ---> 19\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.6103 - acc: 0.7882 - val_loss: 0.9337 - val_acc: 0.7206\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.61300 to 0.72060, saving model to /gdrive/My Drive/CIFAR_10/weights.20-0.72.hdf5\n",
            "Epoch 21/200\n",
            "learning rate ---> 0.1 Epoch ---> 20\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.5870 - acc: 0.7981 - val_loss: 0.7333 - val_acc: 0.7768\n",
            "Epoch 22/200\n",
            "learning rate ---> 0.1 Epoch ---> 21\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.5686 - acc: 0.8021 - val_loss: 0.7439 - val_acc: 0.7777\n",
            "Epoch 23/200\n",
            "learning rate ---> 0.1 Epoch ---> 22\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.5527 - acc: 0.8107 - val_loss: 0.7608 - val_acc: 0.7738\n",
            "Epoch 24/200\n",
            "learning rate ---> 0.1 Epoch ---> 23\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.5408 - acc: 0.8129 - val_loss: 0.8040 - val_acc: 0.7647\n",
            "Epoch 25/200\n",
            "learning rate ---> 0.1 Epoch ---> 24\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.5269 - acc: 0.8179 - val_loss: 0.7063 - val_acc: 0.7950\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.72060 to 0.79500, saving model to /gdrive/My Drive/CIFAR_10/weights.25-0.80.hdf5\n",
            "Epoch 26/200\n",
            "learning rate ---> 0.1 Epoch ---> 25\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.5117 - acc: 0.8223 - val_loss: 0.8185 - val_acc: 0.7613\n",
            "Epoch 27/200\n",
            "learning rate ---> 0.1 Epoch ---> 26\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4973 - acc: 0.8278 - val_loss: 0.7127 - val_acc: 0.7842\n",
            "Epoch 28/200\n",
            "learning rate ---> 0.1 Epoch ---> 27\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4891 - acc: 0.8297 - val_loss: 0.5742 - val_acc: 0.8173\n",
            "Epoch 29/200\n",
            "learning rate ---> 0.1 Epoch ---> 28\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.4764 - acc: 0.8344 - val_loss: 0.6344 - val_acc: 0.8188\n",
            "Epoch 30/200\n",
            "learning rate ---> 0.1 Epoch ---> 29\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.4667 - acc: 0.8400 - val_loss: 0.5617 - val_acc: 0.8225\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.79500 to 0.82250, saving model to /gdrive/My Drive/CIFAR_10/weights.30-0.82.hdf5\n",
            "Epoch 31/200\n",
            "learning rate ---> 0.1 Epoch ---> 30\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.4596 - acc: 0.8390 - val_loss: 0.7951 - val_acc: 0.7836\n",
            "Epoch 32/200\n",
            "learning rate ---> 0.1 Epoch ---> 31\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.4496 - acc: 0.8443 - val_loss: 0.5340 - val_acc: 0.8401\n",
            "Epoch 33/200\n",
            "learning rate ---> 0.1 Epoch ---> 32\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4395 - acc: 0.8481 - val_loss: 0.5614 - val_acc: 0.8272\n",
            "Epoch 34/200\n",
            "learning rate ---> 0.1 Epoch ---> 33\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4313 - acc: 0.8509 - val_loss: 0.5298 - val_acc: 0.8449\n",
            "Epoch 35/200\n",
            "learning rate ---> 0.1 Epoch ---> 34\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4264 - acc: 0.8531 - val_loss: 0.5493 - val_acc: 0.8374\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.82250 to 0.83740, saving model to /gdrive/My Drive/CIFAR_10/weights.35-0.84.hdf5\n",
            "Epoch 36/200\n",
            "learning rate ---> 0.1 Epoch ---> 35\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4209 - acc: 0.8536 - val_loss: 0.5053 - val_acc: 0.8475\n",
            "Epoch 37/200\n",
            "learning rate ---> 0.1 Epoch ---> 36\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4127 - acc: 0.8577 - val_loss: 0.5009 - val_acc: 0.8495\n",
            "Epoch 38/200\n",
            "learning rate ---> 0.1 Epoch ---> 37\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.4027 - acc: 0.8611 - val_loss: 0.5702 - val_acc: 0.8346\n",
            "Epoch 39/200\n",
            "learning rate ---> 0.1 Epoch ---> 38\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.3916 - acc: 0.8663 - val_loss: 0.5686 - val_acc: 0.8356\n",
            "Epoch 40/200\n",
            "learning rate ---> 0.1 Epoch ---> 39\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.3903 - acc: 0.8643 - val_loss: 0.7390 - val_acc: 0.8015\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.83740\n",
            "Epoch 41/200\n",
            "learning rate ---> 0.1 Epoch ---> 40\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3871 - acc: 0.8669 - val_loss: 0.5167 - val_acc: 0.8479\n",
            "Epoch 42/200\n",
            "learning rate ---> 0.1 Epoch ---> 41\n",
            "782/782 [==============================] - 302s 387ms/step - loss: 0.3801 - acc: 0.8695 - val_loss: 0.5619 - val_acc: 0.8384\n",
            "Epoch 43/200\n",
            "learning rate ---> 0.1 Epoch ---> 42\n",
            "782/782 [==============================] - 303s 387ms/step - loss: 0.3752 - acc: 0.8701 - val_loss: 0.5363 - val_acc: 0.8384\n",
            "Epoch 44/200\n",
            "learning rate ---> 0.1 Epoch ---> 43\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3672 - acc: 0.8730 - val_loss: 0.5169 - val_acc: 0.8485\n",
            "Epoch 45/200\n",
            "learning rate ---> 0.1 Epoch ---> 44\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3681 - acc: 0.8733 - val_loss: 0.5378 - val_acc: 0.8452\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.83740 to 0.84520, saving model to /gdrive/My Drive/CIFAR_10/weights.45-0.85.hdf5\n",
            "Epoch 46/200\n",
            "learning rate ---> 0.1 Epoch ---> 45\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3582 - acc: 0.8759 - val_loss: 0.4902 - val_acc: 0.8519\n",
            "Epoch 47/200\n",
            "learning rate ---> 0.1 Epoch ---> 46\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3531 - acc: 0.8764 - val_loss: 0.6048 - val_acc: 0.8305\n",
            "Epoch 48/200\n",
            "learning rate ---> 0.1 Epoch ---> 47\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3493 - acc: 0.8774 - val_loss: 0.5341 - val_acc: 0.8495\n",
            "Epoch 49/200\n",
            "learning rate ---> 0.1 Epoch ---> 48\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.3468 - acc: 0.8794 - val_loss: 0.5205 - val_acc: 0.8455\n",
            "Epoch 50/200\n",
            "learning rate ---> 0.1 Epoch ---> 49\n",
            "782/782 [==============================] - 308s 394ms/step - loss: 0.3425 - acc: 0.8807 - val_loss: 0.4534 - val_acc: 0.8685\n",
            "\n",
            "Epoch 00050: val_acc improved from 0.84520 to 0.86850, saving model to /gdrive/My Drive/CIFAR_10/weights.50-0.87.hdf5\n",
            "Epoch 51/200\n",
            "learning rate ---> 0.1 Epoch ---> 50\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3391 - acc: 0.8818 - val_loss: 0.4850 - val_acc: 0.8579\n",
            "Epoch 52/200\n",
            "learning rate ---> 0.1 Epoch ---> 51\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3336 - acc: 0.8839 - val_loss: 0.4691 - val_acc: 0.8612\n",
            "Epoch 53/200\n",
            "learning rate ---> 0.1 Epoch ---> 52\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3279 - acc: 0.8863 - val_loss: 0.5298 - val_acc: 0.8474\n",
            "Epoch 54/200\n",
            "learning rate ---> 0.1 Epoch ---> 53\n",
            "782/782 [==============================] - 304s 388ms/step - loss: 0.3237 - acc: 0.8867 - val_loss: 0.4639 - val_acc: 0.8673\n",
            "Epoch 55/200\n",
            "learning rate ---> 0.1 Epoch ---> 54\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3266 - acc: 0.8869 - val_loss: 0.4372 - val_acc: 0.8737\n",
            "\n",
            "Epoch 00055: val_acc improved from 0.86850 to 0.87370, saving model to /gdrive/My Drive/CIFAR_10/weights.55-0.87.hdf5\n",
            "Epoch 56/200\n",
            "learning rate ---> 0.1 Epoch ---> 55\n",
            "782/782 [==============================] - 303s 388ms/step - loss: 0.3144 - acc: 0.8923 - val_loss: 0.4361 - val_acc: 0.8777\n",
            "Epoch 57/200\n",
            "learning rate ---> 0.1 Epoch ---> 56\n",
            "782/782 [==============================] - 304s 388ms/step - loss: 0.3159 - acc: 0.8897 - val_loss: 0.4662 - val_acc: 0.8630\n",
            "Epoch 58/200\n",
            "learning rate ---> 0.1 Epoch ---> 57\n",
            "179/782 [=====>........................] - ETA: 3:39 - loss: 0.2986 - acc: 0.8987"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g5fbkIBBbkVd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 56 - 100 Epoch"
      ]
    },
    {
      "metadata": {
        "id": "y5M__NEB4R2P",
        "colab_type": "code",
        "outputId": "53eda051-59b6-40cd-9a99-f4317bc3391d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2808
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.55-0.87.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 55 \n",
        "  if epoch <= 150:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 150 and epoch <=225:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=5)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=45 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "learning rate ---> 0.1 Epoch ---> 55\n",
            "782/782 [==============================] - 310s 396ms/step - loss: 0.3207 - acc: 0.8883 - val_loss: 0.6701 - val_acc: 0.8259\n",
            "Epoch 2/45\n",
            "learning rate ---> 0.1 Epoch ---> 56\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3173 - acc: 0.8905 - val_loss: 0.4855 - val_acc: 0.8667\n",
            "Epoch 3/45\n",
            "learning rate ---> 0.1 Epoch ---> 57\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3095 - acc: 0.8931 - val_loss: 0.7172 - val_acc: 0.8320\n",
            "Epoch 4/45\n",
            "learning rate ---> 0.1 Epoch ---> 58\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3087 - acc: 0.8924 - val_loss: 0.4474 - val_acc: 0.8726\n",
            "Epoch 5/45\n",
            "learning rate ---> 0.1 Epoch ---> 59\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3043 - acc: 0.8946 - val_loss: 0.5453 - val_acc: 0.8566\n",
            "\n",
            "Epoch 00005: val_acc improved from -inf to 0.85660, saving model to /gdrive/My Drive/CIFAR_10/weights.05-0.86.hdf5\n",
            "Epoch 6/45\n",
            "learning rate ---> 0.1 Epoch ---> 60\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3050 - acc: 0.8947 - val_loss: 0.4403 - val_acc: 0.8766\n",
            "Epoch 7/45\n",
            "learning rate ---> 0.1 Epoch ---> 61\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2991 - acc: 0.8955 - val_loss: 0.4877 - val_acc: 0.8668\n",
            "Epoch 8/45\n",
            "learning rate ---> 0.1 Epoch ---> 62\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2964 - acc: 0.8966 - val_loss: 0.4433 - val_acc: 0.8751\n",
            "Epoch 9/45\n",
            "learning rate ---> 0.1 Epoch ---> 63\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2996 - acc: 0.8963 - val_loss: 0.5215 - val_acc: 0.8559\n",
            "Epoch 10/45\n",
            "learning rate ---> 0.1 Epoch ---> 64\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2908 - acc: 0.8996 - val_loss: 0.4281 - val_acc: 0.8764\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.85660 to 0.87640, saving model to /gdrive/My Drive/CIFAR_10/weights.10-0.88.hdf5\n",
            "Epoch 11/45\n",
            "learning rate ---> 0.1 Epoch ---> 65\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2877 - acc: 0.9003 - val_loss: 0.4226 - val_acc: 0.8826\n",
            "Epoch 12/45\n",
            "learning rate ---> 0.1 Epoch ---> 66\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2898 - acc: 0.8996 - val_loss: 0.4707 - val_acc: 0.8706\n",
            "Epoch 13/45\n",
            "learning rate ---> 0.1 Epoch ---> 67\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2829 - acc: 0.9021 - val_loss: 0.4225 - val_acc: 0.8775\n",
            "Epoch 14/45\n",
            "learning rate ---> 0.1 Epoch ---> 68\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2805 - acc: 0.9024 - val_loss: 0.4403 - val_acc: 0.8727\n",
            "Epoch 15/45\n",
            "learning rate ---> 0.1 Epoch ---> 69\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2752 - acc: 0.9054 - val_loss: 0.5334 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.87640\n",
            "Epoch 16/45\n",
            "learning rate ---> 0.1 Epoch ---> 70\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2716 - acc: 0.9041 - val_loss: 0.4868 - val_acc: 0.8682\n",
            "Epoch 17/45\n",
            "learning rate ---> 0.1 Epoch ---> 71\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2756 - acc: 0.9030 - val_loss: 0.4488 - val_acc: 0.8802\n",
            "Epoch 18/45\n",
            "learning rate ---> 0.1 Epoch ---> 72\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2728 - acc: 0.9052 - val_loss: 0.4315 - val_acc: 0.8806\n",
            "Epoch 19/45\n",
            "learning rate ---> 0.1 Epoch ---> 73\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2656 - acc: 0.9060 - val_loss: 0.5242 - val_acc: 0.8622\n",
            "Epoch 20/45\n",
            "learning rate ---> 0.1 Epoch ---> 74\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2685 - acc: 0.9061 - val_loss: 0.4306 - val_acc: 0.8800\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.87640 to 0.88000, saving model to /gdrive/My Drive/CIFAR_10/weights.20-0.88.hdf5\n",
            "Epoch 21/45\n",
            "learning rate ---> 0.1 Epoch ---> 75\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2622 - acc: 0.9087 - val_loss: 0.4110 - val_acc: 0.8834\n",
            "Epoch 22/45\n",
            "learning rate ---> 0.1 Epoch ---> 76\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2568 - acc: 0.9105 - val_loss: 0.4648 - val_acc: 0.8709\n",
            "Epoch 23/45\n",
            "learning rate ---> 0.1 Epoch ---> 77\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2604 - acc: 0.9094 - val_loss: 0.4069 - val_acc: 0.8827\n",
            "Epoch 24/45\n",
            "learning rate ---> 0.1 Epoch ---> 78\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2582 - acc: 0.9106 - val_loss: 0.5202 - val_acc: 0.8628\n",
            "Epoch 25/45\n",
            "learning rate ---> 0.1 Epoch ---> 79\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2541 - acc: 0.9114 - val_loss: 0.3631 - val_acc: 0.8940\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.88000 to 0.89400, saving model to /gdrive/My Drive/CIFAR_10/weights.25-0.89.hdf5\n",
            "Epoch 26/45\n",
            "learning rate ---> 0.1 Epoch ---> 80\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2537 - acc: 0.9111 - val_loss: 0.4581 - val_acc: 0.8738\n",
            "Epoch 27/45\n",
            "learning rate ---> 0.1 Epoch ---> 81\n",
            "782/782 [==============================] - 297s 379ms/step - loss: 0.2524 - acc: 0.9121 - val_loss: 0.6053 - val_acc: 0.8540\n",
            "Epoch 28/45\n",
            "learning rate ---> 0.1 Epoch ---> 82\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2487 - acc: 0.9133 - val_loss: 0.4967 - val_acc: 0.8721\n",
            "Epoch 29/45\n",
            "learning rate ---> 0.1 Epoch ---> 83\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2477 - acc: 0.9143 - val_loss: 0.4230 - val_acc: 0.8830\n",
            "Epoch 30/45\n",
            "learning rate ---> 0.1 Epoch ---> 84\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2427 - acc: 0.9141 - val_loss: 0.4447 - val_acc: 0.8791\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.89400\n",
            "Epoch 31/45\n",
            "learning rate ---> 0.1 Epoch ---> 85\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2414 - acc: 0.9146 - val_loss: 0.5211 - val_acc: 0.8651\n",
            "Epoch 32/45\n",
            "learning rate ---> 0.1 Epoch ---> 86\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2419 - acc: 0.9155 - val_loss: 0.4193 - val_acc: 0.8849\n",
            "Epoch 33/45\n",
            "learning rate ---> 0.1 Epoch ---> 87\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2374 - acc: 0.9161 - val_loss: 0.4977 - val_acc: 0.8685\n",
            "Epoch 34/45\n",
            "learning rate ---> 0.1 Epoch ---> 88\n",
            "782/782 [==============================] - 302s 386ms/step - loss: 0.2379 - acc: 0.9166 - val_loss: 0.4493 - val_acc: 0.8764\n",
            "Epoch 35/45\n",
            "learning rate ---> 0.1 Epoch ---> 89\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.2370 - acc: 0.9171 - val_loss: 0.5013 - val_acc: 0.8623\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.89400\n",
            "Epoch 36/45\n",
            "learning rate ---> 0.1 Epoch ---> 90\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.2322 - acc: 0.9180 - val_loss: 0.4862 - val_acc: 0.8702\n",
            "Epoch 37/45\n",
            "learning rate ---> 0.1 Epoch ---> 91\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.2339 - acc: 0.9180 - val_loss: 0.3336 - val_acc: 0.9007\n",
            "Epoch 38/45\n",
            "learning rate ---> 0.1 Epoch ---> 92\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.2311 - acc: 0.9197 - val_loss: 0.3711 - val_acc: 0.8964\n",
            "Epoch 39/45\n",
            "learning rate ---> 0.1 Epoch ---> 93\n",
            "782/782 [==============================] - 298s 382ms/step - loss: 0.2294 - acc: 0.9195 - val_loss: 0.3534 - val_acc: 0.9043\n",
            "Epoch 40/45\n",
            "learning rate ---> 0.1 Epoch ---> 94\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.2273 - acc: 0.9198 - val_loss: 0.5664 - val_acc: 0.8676\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.89400\n",
            "Epoch 41/45\n",
            "learning rate ---> 0.1 Epoch ---> 95\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2223 - acc: 0.9224 - val_loss: 0.3694 - val_acc: 0.8997\n",
            "Epoch 42/45\n",
            "learning rate ---> 0.1 Epoch ---> 96\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2252 - acc: 0.9200 - val_loss: 0.3977 - val_acc: 0.8970\n",
            "Epoch 43/45\n",
            "learning rate ---> 0.1 Epoch ---> 97\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2194 - acc: 0.9225 - val_loss: 0.4290 - val_acc: 0.8793\n",
            "Epoch 44/45\n",
            "learning rate ---> 0.1 Epoch ---> 98\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2232 - acc: 0.9216 - val_loss: 0.4166 - val_acc: 0.8897\n",
            "Epoch 45/45\n",
            "learning rate ---> 0.1 Epoch ---> 99\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2205 - acc: 0.9237 - val_loss: 0.3590 - val_acc: 0.8957\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.89400 to 0.89570, saving model to /gdrive/My Drive/CIFAR_10/weights.45-0.90.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84f03d0c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "pyhemMmQO79F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 100 - 150 Epoch"
      ]
    },
    {
      "metadata": {
        "id": "x92PADojXqyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4085
        },
        "outputId": "92651510-2315-43e3-bd3e-b219f9a07648"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.45-0.90.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 100 \n",
        "  if epoch <= 120:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 120 and epoch <=210:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=45 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "learning rate ---> 0.1 Epoch ---> 100\n",
            "782/782 [==============================] - 311s 398ms/step - loss: 0.2192 - acc: 0.9221 - val_loss: 0.3970 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.89070, saving model to /gdrive/My Drive/CIFAR_10/weights.01-0.89.hdf5\n",
            "Epoch 2/45\n",
            "learning rate ---> 0.1 Epoch ---> 101\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2178 - acc: 0.9235 - val_loss: 0.4981 - val_acc: 0.8796\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.89070\n",
            "Epoch 3/45\n",
            "learning rate ---> 0.1 Epoch ---> 102\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2166 - acc: 0.9241 - val_loss: 0.3381 - val_acc: 0.8995\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.89070 to 0.89950, saving model to /gdrive/My Drive/CIFAR_10/weights.03-0.90.hdf5\n",
            "Epoch 4/45\n",
            "learning rate ---> 0.1 Epoch ---> 103\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2136 - acc: 0.9255 - val_loss: 0.3725 - val_acc: 0.9003\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.89950 to 0.90030, saving model to /gdrive/My Drive/CIFAR_10/weights.04-0.90.hdf5\n",
            "Epoch 5/45\n",
            "learning rate ---> 0.1 Epoch ---> 104\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2153 - acc: 0.9232 - val_loss: 0.4007 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.90030\n",
            "Epoch 6/45\n",
            "learning rate ---> 0.1 Epoch ---> 105\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2105 - acc: 0.9265 - val_loss: 0.4289 - val_acc: 0.8861\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.90030\n",
            "Epoch 7/45\n",
            "learning rate ---> 0.1 Epoch ---> 106\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2080 - acc: 0.9275 - val_loss: 0.4266 - val_acc: 0.8850\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.90030\n",
            "Epoch 8/45\n",
            "learning rate ---> 0.1 Epoch ---> 107\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2070 - acc: 0.9271 - val_loss: 0.3861 - val_acc: 0.8960\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.90030\n",
            "Epoch 9/45\n",
            "learning rate ---> 0.1 Epoch ---> 108\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2063 - acc: 0.9269 - val_loss: 0.4347 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.90030\n",
            "Epoch 10/45\n",
            "learning rate ---> 0.1 Epoch ---> 109\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2011 - acc: 0.9303 - val_loss: 0.3889 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.90030\n",
            "Epoch 11/45\n",
            "learning rate ---> 0.1 Epoch ---> 110\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2084 - acc: 0.9276 - val_loss: 0.4106 - val_acc: 0.8974\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.90030\n",
            "Epoch 12/45\n",
            "learning rate ---> 0.1 Epoch ---> 111\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2023 - acc: 0.9291 - val_loss: 0.3885 - val_acc: 0.8978\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.90030\n",
            "Epoch 13/45\n",
            "learning rate ---> 0.1 Epoch ---> 112\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2062 - acc: 0.9273 - val_loss: 0.3962 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.90030\n",
            "Epoch 14/45\n",
            "learning rate ---> 0.1 Epoch ---> 113\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2014 - acc: 0.9301 - val_loss: 0.4303 - val_acc: 0.8925\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.90030\n",
            "Epoch 15/45\n",
            "learning rate ---> 0.1 Epoch ---> 114\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2010 - acc: 0.9301 - val_loss: 0.3969 - val_acc: 0.8951\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.90030\n",
            "Epoch 16/45\n",
            "learning rate ---> 0.1 Epoch ---> 115\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1958 - acc: 0.9309 - val_loss: 0.3985 - val_acc: 0.8961\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.90030\n",
            "Epoch 17/45\n",
            "learning rate ---> 0.1 Epoch ---> 116\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2007 - acc: 0.9298 - val_loss: 0.3586 - val_acc: 0.9019\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.90030 to 0.90190, saving model to /gdrive/My Drive/CIFAR_10/weights.17-0.90.hdf5\n",
            "Epoch 18/45\n",
            "learning rate ---> 0.1 Epoch ---> 117\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1942 - acc: 0.9316 - val_loss: 0.5048 - val_acc: 0.8779\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.90190\n",
            "Epoch 19/45\n",
            "learning rate ---> 0.1 Epoch ---> 118\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1979 - acc: 0.9309 - val_loss: 0.4842 - val_acc: 0.8773\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.90190\n",
            "Epoch 20/45\n",
            "learning rate ---> 0.1 Epoch ---> 119\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1929 - acc: 0.9328 - val_loss: 0.3742 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.90190\n",
            "Epoch 21/45\n",
            "learning rate ---> 0.1 Epoch ---> 120\n",
            "782/782 [==============================] - 297s 379ms/step - loss: 0.1877 - acc: 0.9339 - val_loss: 0.3858 - val_acc: 0.9032\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.90190 to 0.90320, saving model to /gdrive/My Drive/CIFAR_10/weights.21-0.90.hdf5\n",
            "Epoch 22/45\n",
            "learning rate ---> 0.01 Epoch ---> 121\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1597 - acc: 0.9445 - val_loss: 0.3299 - val_acc: 0.9151\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.90320 to 0.91510, saving model to /gdrive/My Drive/CIFAR_10/weights.22-0.92.hdf5\n",
            "Epoch 23/45\n",
            "learning rate ---> 0.01 Epoch ---> 122\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1446 - acc: 0.9490 - val_loss: 0.3265 - val_acc: 0.9176\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.91510 to 0.91760, saving model to /gdrive/My Drive/CIFAR_10/weights.23-0.92.hdf5\n",
            "Epoch 24/45\n",
            "learning rate ---> 0.01 Epoch ---> 123\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1431 - acc: 0.9493 - val_loss: 0.3435 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.91760\n",
            "Epoch 25/45\n",
            "learning rate ---> 0.01 Epoch ---> 124\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1368 - acc: 0.9520 - val_loss: 0.3323 - val_acc: 0.9175\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.91760\n",
            "Epoch 26/45\n",
            "learning rate ---> 0.01 Epoch ---> 125\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1346 - acc: 0.9516 - val_loss: 0.3407 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.91760\n",
            "Epoch 27/45\n",
            "learning rate ---> 0.01 Epoch ---> 126\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1291 - acc: 0.9545 - val_loss: 0.3389 - val_acc: 0.9163\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.91760\n",
            "Epoch 28/45\n",
            "learning rate ---> 0.01 Epoch ---> 127\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1314 - acc: 0.9529 - val_loss: 0.3398 - val_acc: 0.9151\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.91760\n",
            "Epoch 29/45\n",
            "learning rate ---> 0.01 Epoch ---> 128\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1307 - acc: 0.9532 - val_loss: 0.3335 - val_acc: 0.9179\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.91760 to 0.91790, saving model to /gdrive/My Drive/CIFAR_10/weights.29-0.92.hdf5\n",
            "Epoch 30/45\n",
            "learning rate ---> 0.01 Epoch ---> 129\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1272 - acc: 0.9553 - val_loss: 0.3363 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.91790\n",
            "Epoch 31/45\n",
            "learning rate ---> 0.01 Epoch ---> 130\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1260 - acc: 0.9557 - val_loss: 0.3279 - val_acc: 0.9184\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.91790 to 0.91840, saving model to /gdrive/My Drive/CIFAR_10/weights.31-0.92.hdf5\n",
            "Epoch 32/45\n",
            "learning rate ---> 0.01 Epoch ---> 131\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1263 - acc: 0.9554 - val_loss: 0.3402 - val_acc: 0.9185\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.91840 to 0.91850, saving model to /gdrive/My Drive/CIFAR_10/weights.32-0.92.hdf5\n",
            "Epoch 33/45\n",
            "learning rate ---> 0.01 Epoch ---> 132\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1235 - acc: 0.9560 - val_loss: 0.3451 - val_acc: 0.9161\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.91850\n",
            "Epoch 34/45\n",
            "learning rate ---> 0.01 Epoch ---> 133\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1269 - acc: 0.9550 - val_loss: 0.3301 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.91850 to 0.91880, saving model to /gdrive/My Drive/CIFAR_10/weights.34-0.92.hdf5\n",
            "Epoch 35/45\n",
            "learning rate ---> 0.01 Epoch ---> 134\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1269 - acc: 0.9559 - val_loss: 0.3400 - val_acc: 0.9176\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.91880\n",
            "Epoch 36/45\n",
            "learning rate ---> 0.01 Epoch ---> 135\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1219 - acc: 0.9566 - val_loss: 0.3373 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.91880\n",
            "Epoch 37/45\n",
            "learning rate ---> 0.01 Epoch ---> 136\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1214 - acc: 0.9571 - val_loss: 0.3317 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.91880 to 0.91890, saving model to /gdrive/My Drive/CIFAR_10/weights.37-0.92.hdf5\n",
            "Epoch 38/45\n",
            "learning rate ---> 0.01 Epoch ---> 137\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1204 - acc: 0.9569 - val_loss: 0.3411 - val_acc: 0.9183\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.91890\n",
            "Epoch 39/45\n",
            "learning rate ---> 0.01 Epoch ---> 138\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1228 - acc: 0.9563 - val_loss: 0.3301 - val_acc: 0.9195\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.91890 to 0.91950, saving model to /gdrive/My Drive/CIFAR_10/weights.39-0.92.hdf5\n",
            "Epoch 40/45\n",
            "learning rate ---> 0.01 Epoch ---> 139\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1207 - acc: 0.9570 - val_loss: 0.3422 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.91950\n",
            "Epoch 41/45\n",
            "learning rate ---> 0.01 Epoch ---> 140\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1171 - acc: 0.9586 - val_loss: 0.3422 - val_acc: 0.9164\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.91950\n",
            "Epoch 42/45\n",
            "learning rate ---> 0.01 Epoch ---> 141\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1179 - acc: 0.9573 - val_loss: 0.3484 - val_acc: 0.9182\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.91950\n",
            "Epoch 43/45\n",
            "learning rate ---> 0.01 Epoch ---> 142\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1199 - acc: 0.9576 - val_loss: 0.3451 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.91950\n",
            "Epoch 44/45\n",
            "learning rate ---> 0.01 Epoch ---> 143\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1180 - acc: 0.9572 - val_loss: 0.3439 - val_acc: 0.9171\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.91950\n",
            "Epoch 45/45\n",
            "learning rate ---> 0.01 Epoch ---> 144\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1180 - acc: 0.9585 - val_loss: 0.3416 - val_acc: 0.9199\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.91950 to 0.91990, saving model to /gdrive/My Drive/CIFAR_10/weights.45-0.92.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84d3660f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Pw_68Y0QjwOC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 150 - 180 Epochs"
      ]
    },
    {
      "metadata": {
        "id": "ghlJJ6JnXqv8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1601
        },
        "outputId": "15c99a42-7412-490c-8cb1-d838008d99ea"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.45-0.92.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 150 \n",
        "  if epoch <= 120:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 120 and epoch <=210:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=30 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "learning rate ---> 0.01 Epoch ---> 150\n",
            "782/782 [==============================] - 315s 402ms/step - loss: 0.1195 - acc: 0.9584 - val_loss: 0.3483 - val_acc: 0.9168\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91680, saving model to /gdrive/My Drive/CIFAR_10/weights.01-0.92.hdf5\n",
            "Epoch 2/30\n",
            "learning rate ---> 0.01 Epoch ---> 151\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1170 - acc: 0.9584 - val_loss: 0.3473 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.91680 to 0.91730, saving model to /gdrive/My Drive/CIFAR_10/weights.02-0.92.hdf5\n",
            "Epoch 3/30\n",
            "learning rate ---> 0.01 Epoch ---> 152\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1156 - acc: 0.9589 - val_loss: 0.3588 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.91730\n",
            "Epoch 4/30\n",
            "learning rate ---> 0.01 Epoch ---> 153\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1175 - acc: 0.9584 - val_loss: 0.3484 - val_acc: 0.9159\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.91730\n",
            "Epoch 5/30\n",
            "learning rate ---> 0.01 Epoch ---> 154\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1154 - acc: 0.9587 - val_loss: 0.3427 - val_acc: 0.9172\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.91730\n",
            "Epoch 6/30\n",
            "learning rate ---> 0.01 Epoch ---> 155\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1115 - acc: 0.9603 - val_loss: 0.3558 - val_acc: 0.9177\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.91730 to 0.91770, saving model to /gdrive/My Drive/CIFAR_10/weights.06-0.92.hdf5\n",
            "Epoch 7/30\n",
            "learning rate ---> 0.01 Epoch ---> 156\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1153 - acc: 0.9594 - val_loss: 0.3610 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.91770\n",
            "Epoch 8/30\n",
            "learning rate ---> 0.01 Epoch ---> 157\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1119 - acc: 0.9594 - val_loss: 0.3483 - val_acc: 0.9181\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.91770 to 0.91810, saving model to /gdrive/My Drive/CIFAR_10/weights.08-0.92.hdf5\n",
            "Epoch 9/30\n",
            "learning rate ---> 0.01 Epoch ---> 158\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1172 - acc: 0.9585 - val_loss: 0.3544 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.91810\n",
            "Epoch 10/30\n",
            "learning rate ---> 0.01 Epoch ---> 159\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1132 - acc: 0.9590 - val_loss: 0.3542 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.91810\n",
            "Epoch 11/30\n",
            "learning rate ---> 0.01 Epoch ---> 160\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1127 - acc: 0.9591 - val_loss: 0.3500 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.91810\n",
            "Epoch 12/30\n",
            "learning rate ---> 0.01 Epoch ---> 161\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.1128 - acc: 0.9601 - val_loss: 0.3529 - val_acc: 0.9169\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.91810\n",
            "Epoch 13/30\n",
            "learning rate ---> 0.01 Epoch ---> 162\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1117 - acc: 0.9599 - val_loss: 0.3484 - val_acc: 0.9198\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.91810 to 0.91980, saving model to /gdrive/My Drive/CIFAR_10/weights.13-0.92.hdf5\n",
            "Epoch 14/30\n",
            "learning rate ---> 0.01 Epoch ---> 163\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1118 - acc: 0.9607 - val_loss: 0.3561 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.91980\n",
            "Epoch 15/30\n",
            "learning rate ---> 0.01 Epoch ---> 164\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1096 - acc: 0.9610 - val_loss: 0.3546 - val_acc: 0.9184\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.91980\n",
            "Epoch 16/30\n",
            "learning rate ---> 0.01 Epoch ---> 165\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1110 - acc: 0.9598 - val_loss: 0.3523 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.91980 to 0.92010, saving model to /gdrive/My Drive/CIFAR_10/weights.16-0.92.hdf5\n",
            "Epoch 17/30\n",
            "learning rate ---> 0.01 Epoch ---> 166\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1123 - acc: 0.9601 - val_loss: 0.3424 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.92010 to 0.92190, saving model to /gdrive/My Drive/CIFAR_10/weights.17-0.92.hdf5\n",
            "Epoch 18/30\n",
            "learning rate ---> 0.01 Epoch ---> 167\n",
            "447/782 [================>.............] - ETA: 1:59 - loss: 0.1120 - acc: 0.9598"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L2fJyCFTkEo2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}