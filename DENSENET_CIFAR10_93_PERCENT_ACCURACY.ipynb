{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_DENSENET_93_ACURACY.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "rYiWOAWMbVmN",
        "g5fbkIBBbkVd",
        "pyhemMmQO79F",
        "Pw_68Y0QjwOC",
        "ZXJgKPoIf1Yd",
        "lxnyrCdPAQ6j",
        "DnanV4qJSMaM",
        "iJScDWKeSKYn",
        "EFDAWkKfSH11",
        "pMjSbO3dNeij",
        "NPnht2pVYZlM",
        "iaFGtE6HGiXl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hSp3AkdIXOb5",
        "colab_type": "code",
        "outputId": "c6a65a49-3d84-4c37-8241-4ee6d992fe0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kI8pfH2ABgB1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "----\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "T0go040tBVDY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 93.01 % ACCURACY CODE.\n",
        "## Total params: 995,230\n",
        "\n",
        "[My Densenet Blog ](https://karthikziffer.github.io/journal/summary-densenet.html)\n",
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "DZwqzk57Nct2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1 . Batch Size - - 128 - This batch size reached a test accuracy of 91% accuracy, but couldnot converge beyond 92%. The result remained the same for both data augmentation and without data augmentation. - 64 - By reducing the batch size, the model had much room to update the weights during back propagation. This resulted in a test accuracy of 93%. - 34 - For this batch size, the model training time was higher, but there was no much improvement in test accuracy. \n",
        "\n",
        "\n",
        "2 . Train data normalization and data augmentation - - Since CIFAR10 dataset, consists of images shot at different lighting condition and angle. If the images are at differenet resolution level, Normalizing it would scale the entire dataset images accordingly. - From DENSENET paper, the authors have considered two data augmentation tehnique.\n",
        "1. Increase the width and height of the image and crop it randomly. \n",
        "2. Horizontal shifting of images. I considered, width shift range and height shift range along with horizontal shift for 180 epochs.This helped me to reach an test accuracy of 92%. Then I removed data augmentation, for another 70 epochs, this increased both my training accuracy and test accuracy to 93%.\n",
        "\n",
        "\n",
        "**Conclusion**: By training the model with data augmentation for 200 epochs, and then removing augmentation will increase the test accuracy considerably. 1. Learning Rate scheduler - By refering to the paper, for initial 150 epochs the lr was 0.1 , next till 225 the lr was 0.01 and for ramaining epochs it was 0.001. ''\" Learning rate annealing recommends starting with a relatively high learning rate and then gradually lowering the learning rate during training. The intuition behind this approach is that we'd lika a learning rate small enough that we can explore the deeper but narrower parts of the loss function\"'' - By Andrej Karparthy."
      ]
    },
    {
      "metadata": {
        "id": "9-3evHVNbZ7C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Oct 22 \n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "rYiWOAWMbVmN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - 55 Epoch\n",
        "\n",
        "## Validation Accuracy - 85%"
      ]
    },
    {
      "metadata": {
        "id": "M7bXaefq4R6b",
        "colab_type": "code",
        "outputId": "60337177-fbde-4d36-8e0c-4605bdce91a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3624
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#keras library\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "\n",
        "# densenet \n",
        "\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "#############################################################################\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 55\n",
        "l = 12 #L\n",
        "num_filter = 36 #k\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "#############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing data\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "#optimizer\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  if epoch <= 150:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 150 and epoch <=225:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=5)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=epochs , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 86s 1us/step\n",
            "Epoch 1/55\n",
            "learning rate ---> 0.1 Epoch ---> 0\n",
            "782/782 [==============================] - 311s 398ms/step - loss: 2.3500 - acc: 0.2070 - val_loss: 1.8722 - val_acc: 0.3054\n",
            "Epoch 2/55\n",
            "learning rate ---> 0.1 Epoch ---> 1\n",
            "782/782 [==============================] - 296s 378ms/step - loss: 1.7681 - acc: 0.3392 - val_loss: 1.7469 - val_acc: 0.3362\n",
            "Epoch 3/55\n",
            "learning rate ---> 0.1 Epoch ---> 2\n",
            "782/782 [==============================] - 296s 379ms/step - loss: 1.5673 - acc: 0.4217 - val_loss: 1.5570 - val_acc: 0.4376\n",
            "Epoch 4/55\n",
            "learning rate ---> 0.1 Epoch ---> 3\n",
            "782/782 [==============================] - 296s 379ms/step - loss: 1.3899 - acc: 0.4923 - val_loss: 1.6115 - val_acc: 0.4590\n",
            "Epoch 5/55\n",
            "learning rate ---> 0.1 Epoch ---> 4\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 1.2633 - acc: 0.5453 - val_loss: 1.4195 - val_acc: 0.5025\n",
            "\n",
            "Epoch 00005: val_acc improved from -inf to 0.50250, saving model to /gdrive/My Drive/CIFAR_10/weights.05-0.50.hdf5\n",
            "Epoch 6/55\n",
            "learning rate ---> 0.1 Epoch ---> 5\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 1.1573 - acc: 0.5856 - val_loss: 1.4498 - val_acc: 0.5559\n",
            "Epoch 7/55\n",
            "learning rate ---> 0.1 Epoch ---> 6\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 1.0748 - acc: 0.6159 - val_loss: 1.6109 - val_acc: 0.5349\n",
            "Epoch 8/55\n",
            "learning rate ---> 0.1 Epoch ---> 7\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 1.0043 - acc: 0.6443 - val_loss: 1.2003 - val_acc: 0.6164\n",
            "Epoch 9/55\n",
            "learning rate ---> 0.1 Epoch ---> 8\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.9423 - acc: 0.6643 - val_loss: 2.2290 - val_acc: 0.5049\n",
            "Epoch 10/55\n",
            "learning rate ---> 0.1 Epoch ---> 9\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.8749 - acc: 0.6923 - val_loss: 1.2118 - val_acc: 0.6516\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.50250 to 0.65160, saving model to /gdrive/My Drive/CIFAR_10/weights.10-0.65.hdf5\n",
            "Epoch 11/55\n",
            "learning rate ---> 0.1 Epoch ---> 10\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.8313 - acc: 0.7070 - val_loss: 1.5075 - val_acc: 0.5949\n",
            "Epoch 12/55\n",
            "learning rate ---> 0.1 Epoch ---> 11\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.7962 - acc: 0.7209 - val_loss: 1.0281 - val_acc: 0.6788\n",
            "Epoch 13/55\n",
            "learning rate ---> 0.1 Epoch ---> 12\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.7624 - acc: 0.7337 - val_loss: 1.0368 - val_acc: 0.6971\n",
            "Epoch 14/55\n",
            "learning rate ---> 0.1 Epoch ---> 13\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.7271 - acc: 0.7455 - val_loss: 1.9034 - val_acc: 0.5781\n",
            "Epoch 15/55\n",
            "learning rate ---> 0.1 Epoch ---> 14\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.7037 - acc: 0.7562 - val_loss: 1.4973 - val_acc: 0.6426\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.65160\n",
            "Epoch 16/55\n",
            "learning rate ---> 0.1 Epoch ---> 15\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.6752 - acc: 0.7653 - val_loss: 1.0125 - val_acc: 0.6999\n",
            "Epoch 17/55\n",
            "learning rate ---> 0.1 Epoch ---> 16\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.6486 - acc: 0.7732 - val_loss: 1.0447 - val_acc: 0.7147\n",
            "Epoch 18/55\n",
            "learning rate ---> 0.1 Epoch ---> 17\n",
            "782/782 [==============================] - 300s 384ms/step - loss: 0.6257 - acc: 0.7844 - val_loss: 0.8518 - val_acc: 0.7438\n",
            "Epoch 19/55\n",
            "learning rate ---> 0.1 Epoch ---> 18\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.5986 - acc: 0.7924 - val_loss: 1.3423 - val_acc: 0.6677\n",
            "Epoch 20/55\n",
            "learning rate ---> 0.1 Epoch ---> 19\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.5943 - acc: 0.7955 - val_loss: 0.8928 - val_acc: 0.7431\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.65160 to 0.74310, saving model to /gdrive/My Drive/CIFAR_10/weights.20-0.74.hdf5\n",
            "Epoch 21/55\n",
            "learning rate ---> 0.1 Epoch ---> 20\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.5713 - acc: 0.8030 - val_loss: 0.6470 - val_acc: 0.8070\n",
            "Epoch 22/55\n",
            "learning rate ---> 0.1 Epoch ---> 21\n",
            "782/782 [==============================] - 301s 385ms/step - loss: 0.5507 - acc: 0.8111 - val_loss: 0.9431 - val_acc: 0.7375\n",
            "Epoch 23/55\n",
            "learning rate ---> 0.1 Epoch ---> 22\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.5373 - acc: 0.8135 - val_loss: 0.9678 - val_acc: 0.7398\n",
            "Epoch 24/55\n",
            "learning rate ---> 0.1 Epoch ---> 23\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.5203 - acc: 0.8201 - val_loss: 0.6041 - val_acc: 0.8179\n",
            "Epoch 25/55\n",
            "learning rate ---> 0.1 Epoch ---> 24\n",
            "782/782 [==============================] - 299s 383ms/step - loss: 0.5098 - acc: 0.8250 - val_loss: 0.6702 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.74310 to 0.80650, saving model to /gdrive/My Drive/CIFAR_10/weights.25-0.81.hdf5\n",
            "Epoch 26/55\n",
            "learning rate ---> 0.1 Epoch ---> 25\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.5047 - acc: 0.8275 - val_loss: 0.6920 - val_acc: 0.7993\n",
            "Epoch 27/55\n",
            "learning rate ---> 0.1 Epoch ---> 26\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.4904 - acc: 0.8314 - val_loss: 0.6772 - val_acc: 0.8082\n",
            "Epoch 28/55\n",
            "learning rate ---> 0.1 Epoch ---> 27\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.4839 - acc: 0.8336 - val_loss: 0.7576 - val_acc: 0.7832\n",
            "Epoch 29/55\n",
            "learning rate ---> 0.1 Epoch ---> 28\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.4670 - acc: 0.8375 - val_loss: 0.6310 - val_acc: 0.8181\n",
            "Epoch 30/55\n",
            "learning rate ---> 0.1 Epoch ---> 29\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.4592 - acc: 0.8407 - val_loss: 0.6312 - val_acc: 0.8240\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.80650 to 0.82400, saving model to /gdrive/My Drive/CIFAR_10/weights.30-0.82.hdf5\n",
            "Epoch 31/55\n",
            "learning rate ---> 0.1 Epoch ---> 30\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.4540 - acc: 0.8448 - val_loss: 0.6549 - val_acc: 0.8123\n",
            "Epoch 32/55\n",
            "learning rate ---> 0.1 Epoch ---> 31\n",
            "782/782 [==============================] - 298s 382ms/step - loss: 0.4437 - acc: 0.8480 - val_loss: 0.5873 - val_acc: 0.8282\n",
            "Epoch 33/55\n",
            "learning rate ---> 0.1 Epoch ---> 32\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.4330 - acc: 0.8516 - val_loss: 0.9311 - val_acc: 0.7564\n",
            "Epoch 34/55\n",
            "learning rate ---> 0.1 Epoch ---> 33\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.4271 - acc: 0.8525 - val_loss: 0.6344 - val_acc: 0.8285\n",
            "Epoch 35/55\n",
            "learning rate ---> 0.1 Epoch ---> 34\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.4178 - acc: 0.8584 - val_loss: 1.2206 - val_acc: 0.7274\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.82400\n",
            "Epoch 36/55\n",
            "learning rate ---> 0.1 Epoch ---> 35\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.4122 - acc: 0.8586 - val_loss: 0.5620 - val_acc: 0.8407\n",
            "Epoch 37/55\n",
            "learning rate ---> 0.1 Epoch ---> 36\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.4028 - acc: 0.8598 - val_loss: 0.9174 - val_acc: 0.7575\n",
            "Epoch 38/55\n",
            "learning rate ---> 0.1 Epoch ---> 37\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.4010 - acc: 0.8630 - val_loss: 0.6867 - val_acc: 0.8098\n",
            "Epoch 39/55\n",
            "learning rate ---> 0.1 Epoch ---> 38\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3946 - acc: 0.8630 - val_loss: 0.4980 - val_acc: 0.8593\n",
            "Epoch 40/55\n",
            "learning rate ---> 0.1 Epoch ---> 39\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3879 - acc: 0.8668 - val_loss: 0.9555 - val_acc: 0.7613\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.82400\n",
            "Epoch 41/55\n",
            "learning rate ---> 0.1 Epoch ---> 40\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.3836 - acc: 0.8678 - val_loss: 0.5928 - val_acc: 0.8351\n",
            "Epoch 42/55\n",
            "learning rate ---> 0.1 Epoch ---> 41\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3764 - acc: 0.8709 - val_loss: 0.6381 - val_acc: 0.8286\n",
            "Epoch 43/55\n",
            "learning rate ---> 0.1 Epoch ---> 42\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3731 - acc: 0.8707 - val_loss: 0.6897 - val_acc: 0.8196\n",
            "Epoch 44/55\n",
            "learning rate ---> 0.1 Epoch ---> 43\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3677 - acc: 0.8729 - val_loss: 0.6858 - val_acc: 0.8196\n",
            "Epoch 45/55\n",
            "learning rate ---> 0.1 Epoch ---> 44\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3569 - acc: 0.8768 - val_loss: 0.9469 - val_acc: 0.7707\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.82400\n",
            "Epoch 46/55\n",
            "learning rate ---> 0.1 Epoch ---> 45\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3537 - acc: 0.8790 - val_loss: 0.7072 - val_acc: 0.8111\n",
            "Epoch 47/55\n",
            "learning rate ---> 0.1 Epoch ---> 46\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3512 - acc: 0.8801 - val_loss: 0.4956 - val_acc: 0.8570\n",
            "Epoch 48/55\n",
            "learning rate ---> 0.1 Epoch ---> 47\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3462 - acc: 0.8799 - val_loss: 0.4859 - val_acc: 0.8607\n",
            "Epoch 49/55\n",
            "learning rate ---> 0.1 Epoch ---> 48\n",
            "782/782 [==============================] - 298s 382ms/step - loss: 0.3449 - acc: 0.8821 - val_loss: 0.4855 - val_acc: 0.8564\n",
            "Epoch 50/55\n",
            "learning rate ---> 0.1 Epoch ---> 49\n",
            "782/782 [==============================] - 298s 382ms/step - loss: 0.3418 - acc: 0.8822 - val_loss: 0.5950 - val_acc: 0.8487\n",
            "\n",
            "Epoch 00050: val_acc improved from 0.82400 to 0.84870, saving model to /gdrive/My Drive/CIFAR_10/weights.50-0.85.hdf5\n",
            "Epoch 51/55\n",
            "learning rate ---> 0.1 Epoch ---> 50\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3320 - acc: 0.8860 - val_loss: 0.4777 - val_acc: 0.8661\n",
            "Epoch 52/55\n",
            "learning rate ---> 0.1 Epoch ---> 51\n",
            "782/782 [==============================] - 299s 383ms/step - loss: 0.3314 - acc: 0.8878 - val_loss: 0.6166 - val_acc: 0.8436\n",
            "Epoch 53/55\n",
            "learning rate ---> 0.1 Epoch ---> 52\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3279 - acc: 0.8860 - val_loss: 0.4272 - val_acc: 0.8736\n",
            "Epoch 54/55\n",
            "learning rate ---> 0.1 Epoch ---> 53\n",
            "782/782 [==============================] - 299s 383ms/step - loss: 0.3207 - acc: 0.8894 - val_loss: 0.6635 - val_acc: 0.8223\n",
            "Epoch 55/55\n",
            "learning rate ---> 0.1 Epoch ---> 54\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.3224 - acc: 0.8889 - val_loss: 0.5092 - val_acc: 0.8521\n",
            "\n",
            "Epoch 00055: val_acc improved from 0.84870 to 0.85210, saving model to /gdrive/My Drive/CIFAR_10/weights.55-0.85.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e1b3d5630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "g5fbkIBBbkVd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 56 - 100 Epoch\n",
        "\n",
        "## Validation Accuracy - 89%"
      ]
    },
    {
      "metadata": {
        "id": "y5M__NEB4R2P",
        "colab_type": "code",
        "outputId": "f7b1a2a4-da4f-4351-8cc3-04e34f6085c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2925
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.55-0.87.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 55 \n",
        "  if epoch <= 150:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 150 and epoch <=225:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=5)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=45 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "learning rate ---> 0.1 Epoch ---> 55\n",
            "782/782 [==============================] - 309s 395ms/step - loss: 0.3191 - acc: 0.8897 - val_loss: 0.4737 - val_acc: 0.8688\n",
            "Epoch 2/45\n",
            "learning rate ---> 0.1 Epoch ---> 56\n",
            "782/782 [==============================] - 290s 370ms/step - loss: 0.3109 - acc: 0.8910 - val_loss: 0.5782 - val_acc: 0.8369\n",
            "Epoch 3/45\n",
            "learning rate ---> 0.1 Epoch ---> 57\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 0.3138 - acc: 0.8884 - val_loss: 0.4684 - val_acc: 0.8673\n",
            "Epoch 4/45\n",
            "learning rate ---> 0.1 Epoch ---> 58\n",
            "782/782 [==============================] - 290s 370ms/step - loss: 0.3097 - acc: 0.8931 - val_loss: 0.4202 - val_acc: 0.8778\n",
            "Epoch 5/45\n",
            "learning rate ---> 0.1 Epoch ---> 59\n",
            "782/782 [==============================] - 289s 370ms/step - loss: 0.3023 - acc: 0.8954 - val_loss: 0.5740 - val_acc: 0.8486\n",
            "\n",
            "Epoch 00005: val_acc improved from -inf to 0.84860, saving model to /gdrive/My Drive/CIFAR_10/weights.05-0.85.hdf5\n",
            "Epoch 6/45\n",
            "learning rate ---> 0.1 Epoch ---> 60\n",
            "782/782 [==============================] - 289s 369ms/step - loss: 0.3034 - acc: 0.8946 - val_loss: 0.5159 - val_acc: 0.8560\n",
            "Epoch 7/45\n",
            "learning rate ---> 0.1 Epoch ---> 61\n",
            "782/782 [==============================] - 290s 371ms/step - loss: 0.3008 - acc: 0.8963 - val_loss: 0.4592 - val_acc: 0.8709\n",
            "Epoch 8/45\n",
            "learning rate ---> 0.1 Epoch ---> 62\n",
            "782/782 [==============================] - 288s 368ms/step - loss: 0.2936 - acc: 0.8984 - val_loss: 0.4495 - val_acc: 0.8755\n",
            "Epoch 9/45\n",
            "learning rate ---> 0.1 Epoch ---> 63\n",
            "782/782 [==============================] - 294s 376ms/step - loss: 0.2940 - acc: 0.8970 - val_loss: 0.4620 - val_acc: 0.8717\n",
            "Epoch 10/45\n",
            "learning rate ---> 0.1 Epoch ---> 64\n",
            "782/782 [==============================] - 287s 367ms/step - loss: 0.2896 - acc: 0.8993 - val_loss: 0.3831 - val_acc: 0.8845\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.84860 to 0.88450, saving model to /gdrive/My Drive/CIFAR_10/weights.10-0.88.hdf5\n",
            "Epoch 11/45\n",
            "learning rate ---> 0.1 Epoch ---> 65\n",
            "782/782 [==============================] - 289s 369ms/step - loss: 0.2906 - acc: 0.8996 - val_loss: 0.5029 - val_acc: 0.8520\n",
            "Epoch 12/45\n",
            "learning rate ---> 0.1 Epoch ---> 66\n",
            "782/782 [==============================] - 291s 372ms/step - loss: 0.2857 - acc: 0.9016 - val_loss: 0.6697 - val_acc: 0.8309\n",
            "Epoch 13/45\n",
            "learning rate ---> 0.1 Epoch ---> 67\n",
            "782/782 [==============================] - 288s 368ms/step - loss: 0.2798 - acc: 0.9026 - val_loss: 0.5089 - val_acc: 0.8661\n",
            "Epoch 14/45\n",
            "learning rate ---> 0.1 Epoch ---> 68\n",
            "782/782 [==============================] - 290s 370ms/step - loss: 0.2805 - acc: 0.9019 - val_loss: 0.4663 - val_acc: 0.8692\n",
            "Epoch 15/45\n",
            "learning rate ---> 0.1 Epoch ---> 69\n",
            "782/782 [==============================] - 287s 367ms/step - loss: 0.2781 - acc: 0.9024 - val_loss: 0.3913 - val_acc: 0.8902\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.88450 to 0.89020, saving model to /gdrive/My Drive/CIFAR_10/weights.15-0.89.hdf5\n",
            "Epoch 16/45\n",
            "learning rate ---> 0.1 Epoch ---> 70\n",
            "782/782 [==============================] - 292s 373ms/step - loss: 0.2765 - acc: 0.9032 - val_loss: 0.5251 - val_acc: 0.8657\n",
            "Epoch 17/45\n",
            "learning rate ---> 0.1 Epoch ---> 71\n",
            "782/782 [==============================] - 287s 367ms/step - loss: 0.2705 - acc: 0.9048 - val_loss: 0.4276 - val_acc: 0.8798\n",
            "Epoch 18/45\n",
            "learning rate ---> 0.1 Epoch ---> 72\n",
            "782/782 [==============================] - 288s 369ms/step - loss: 0.2710 - acc: 0.9062 - val_loss: 0.4221 - val_acc: 0.8806\n",
            "Epoch 19/45\n",
            "learning rate ---> 0.1 Epoch ---> 73\n",
            "782/782 [==============================] - 291s 373ms/step - loss: 0.2706 - acc: 0.9061 - val_loss: 0.4570 - val_acc: 0.8781\n",
            "Epoch 20/45\n",
            "learning rate ---> 0.1 Epoch ---> 74\n",
            "782/782 [==============================] - 288s 368ms/step - loss: 0.2683 - acc: 0.9057 - val_loss: 0.4431 - val_acc: 0.8805\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.89020\n",
            "Epoch 21/45\n",
            "learning rate ---> 0.1 Epoch ---> 75\n",
            "782/782 [==============================] - 290s 371ms/step - loss: 0.2626 - acc: 0.9073 - val_loss: 0.4797 - val_acc: 0.8765\n",
            "Epoch 22/45\n",
            "learning rate ---> 0.1 Epoch ---> 76\n",
            "782/782 [==============================] - 289s 369ms/step - loss: 0.2634 - acc: 0.9091 - val_loss: 0.4542 - val_acc: 0.8714\n",
            "Epoch 23/45\n",
            "learning rate ---> 0.1 Epoch ---> 77\n",
            "782/782 [==============================] - 291s 372ms/step - loss: 0.2615 - acc: 0.9092 - val_loss: 0.4750 - val_acc: 0.8724\n",
            "Epoch 24/45\n",
            "learning rate ---> 0.1 Epoch ---> 78\n",
            "782/782 [==============================] - 287s 367ms/step - loss: 0.2572 - acc: 0.9105 - val_loss: 0.3881 - val_acc: 0.8920\n",
            "Epoch 25/45\n",
            "learning rate ---> 0.1 Epoch ---> 79\n",
            "782/782 [==============================] - 289s 369ms/step - loss: 0.2519 - acc: 0.9110 - val_loss: 0.4148 - val_acc: 0.8850\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.89020\n",
            "Epoch 26/45\n",
            "learning rate ---> 0.1 Epoch ---> 80\n",
            "782/782 [==============================] - 293s 374ms/step - loss: 0.2544 - acc: 0.9101 - val_loss: 0.4165 - val_acc: 0.8775\n",
            "Epoch 27/45\n",
            "learning rate ---> 0.1 Epoch ---> 81\n",
            "782/782 [==============================] - 288s 369ms/step - loss: 0.2473 - acc: 0.9135 - val_loss: 0.4340 - val_acc: 0.8833\n",
            "Epoch 28/45\n",
            "learning rate ---> 0.1 Epoch ---> 82\n",
            "782/782 [==============================] - 291s 372ms/step - loss: 0.2515 - acc: 0.9121 - val_loss: 0.3969 - val_acc: 0.8899\n",
            "Epoch 29/45\n",
            "learning rate ---> 0.1 Epoch ---> 83\n",
            "782/782 [==============================] - 287s 367ms/step - loss: 0.2474 - acc: 0.9133 - val_loss: 0.3827 - val_acc: 0.8945\n",
            "Epoch 30/45\n",
            "learning rate ---> 0.1 Epoch ---> 84\n",
            "782/782 [==============================] - 292s 373ms/step - loss: 0.2431 - acc: 0.9162 - val_loss: 0.3967 - val_acc: 0.8839\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.89020\n",
            "Epoch 31/45\n",
            "learning rate ---> 0.1 Epoch ---> 85\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 0.2385 - acc: 0.9168 - val_loss: 0.4712 - val_acc: 0.8732\n",
            "Epoch 32/45\n",
            "learning rate ---> 0.1 Epoch ---> 86\n",
            "782/782 [==============================] - 288s 369ms/step - loss: 0.2411 - acc: 0.9156 - val_loss: 0.4946 - val_acc: 0.8685\n",
            "Epoch 33/45\n",
            "learning rate ---> 0.1 Epoch ---> 87\n",
            "782/782 [==============================] - 288s 368ms/step - loss: 0.2360 - acc: 0.9171 - val_loss: 0.3911 - val_acc: 0.8891\n",
            "Epoch 34/45\n",
            "learning rate ---> 0.1 Epoch ---> 88\n",
            "782/782 [==============================] - 290s 371ms/step - loss: 0.2397 - acc: 0.9158 - val_loss: 0.4001 - val_acc: 0.8871\n",
            "Epoch 35/45\n",
            "learning rate ---> 0.1 Epoch ---> 89\n",
            "782/782 [==============================] - 287s 367ms/step - loss: 0.2310 - acc: 0.9180 - val_loss: 0.5434 - val_acc: 0.8559\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.89020\n",
            "Epoch 36/45\n",
            "learning rate ---> 0.1 Epoch ---> 90\n",
            "782/782 [==============================] - 288s 368ms/step - loss: 0.2319 - acc: 0.9184 - val_loss: 0.3871 - val_acc: 0.8893\n",
            "Epoch 37/45\n",
            "learning rate ---> 0.1 Epoch ---> 91\n",
            "782/782 [==============================] - 290s 371ms/step - loss: 0.2289 - acc: 0.9200 - val_loss: 0.5350 - val_acc: 0.8604\n",
            "Epoch 38/45\n",
            "learning rate ---> 0.1 Epoch ---> 92\n",
            "782/782 [==============================] - 287s 367ms/step - loss: 0.2278 - acc: 0.9222 - val_loss: 0.4327 - val_acc: 0.8874\n",
            "Epoch 39/45\n",
            "learning rate ---> 0.1 Epoch ---> 93\n",
            "782/782 [==============================] - 289s 369ms/step - loss: 0.2273 - acc: 0.9209 - val_loss: 0.4509 - val_acc: 0.8838\n",
            "Epoch 40/45\n",
            "learning rate ---> 0.1 Epoch ---> 94\n",
            "782/782 [==============================] - 287s 367ms/step - loss: 0.2261 - acc: 0.9216 - val_loss: 0.6466 - val_acc: 0.8421\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.89020\n",
            "Epoch 41/45\n",
            "learning rate ---> 0.1 Epoch ---> 95\n",
            "782/782 [==============================] - 291s 372ms/step - loss: 0.2300 - acc: 0.9193 - val_loss: 0.3745 - val_acc: 0.8959\n",
            "Epoch 42/45\n",
            "learning rate ---> 0.1 Epoch ---> 96\n",
            "782/782 [==============================] - 285s 364ms/step - loss: 0.2213 - acc: 0.9221 - val_loss: 0.4459 - val_acc: 0.8889\n",
            "Epoch 43/45\n",
            "learning rate ---> 0.1 Epoch ---> 97\n",
            "782/782 [==============================] - 288s 369ms/step - loss: 0.2207 - acc: 0.9234 - val_loss: 0.4916 - val_acc: 0.8729\n",
            "Epoch 44/45\n",
            "learning rate ---> 0.1 Epoch ---> 98\n",
            "782/782 [==============================] - 290s 371ms/step - loss: 0.2251 - acc: 0.9206 - val_loss: 0.4862 - val_acc: 0.8744\n",
            "Epoch 45/45\n",
            "learning rate ---> 0.1 Epoch ---> 99\n",
            "782/782 [==============================] - 287s 367ms/step - loss: 0.2219 - acc: 0.9218 - val_loss: 0.4081 - val_acc: 0.8866\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.89020\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2a0c390908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "pyhemMmQO79F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 100 - 150 Epoch\n",
        "\n",
        "## Validation Accuracy - 91.99%"
      ]
    },
    {
      "metadata": {
        "id": "x92PADojXqyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4085
        },
        "outputId": "92651510-2315-43e3-bd3e-b219f9a07648"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.15-0.89.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 100 \n",
        "  if epoch <= 120:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 120 and epoch <=210:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=45 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "learning rate ---> 0.1 Epoch ---> 100\n",
            "782/782 [==============================] - 311s 398ms/step - loss: 0.2192 - acc: 0.9221 - val_loss: 0.3970 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.89070, saving model to /gdrive/My Drive/CIFAR_10/weights.01-0.89.hdf5\n",
            "Epoch 2/45\n",
            "learning rate ---> 0.1 Epoch ---> 101\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2178 - acc: 0.9235 - val_loss: 0.4981 - val_acc: 0.8796\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.89070\n",
            "Epoch 3/45\n",
            "learning rate ---> 0.1 Epoch ---> 102\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2166 - acc: 0.9241 - val_loss: 0.3381 - val_acc: 0.8995\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.89070 to 0.89950, saving model to /gdrive/My Drive/CIFAR_10/weights.03-0.90.hdf5\n",
            "Epoch 4/45\n",
            "learning rate ---> 0.1 Epoch ---> 103\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2136 - acc: 0.9255 - val_loss: 0.3725 - val_acc: 0.9003\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.89950 to 0.90030, saving model to /gdrive/My Drive/CIFAR_10/weights.04-0.90.hdf5\n",
            "Epoch 5/45\n",
            "learning rate ---> 0.1 Epoch ---> 104\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2153 - acc: 0.9232 - val_loss: 0.4007 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.90030\n",
            "Epoch 6/45\n",
            "learning rate ---> 0.1 Epoch ---> 105\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2105 - acc: 0.9265 - val_loss: 0.4289 - val_acc: 0.8861\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.90030\n",
            "Epoch 7/45\n",
            "learning rate ---> 0.1 Epoch ---> 106\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2080 - acc: 0.9275 - val_loss: 0.4266 - val_acc: 0.8850\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.90030\n",
            "Epoch 8/45\n",
            "learning rate ---> 0.1 Epoch ---> 107\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2070 - acc: 0.9271 - val_loss: 0.3861 - val_acc: 0.8960\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.90030\n",
            "Epoch 9/45\n",
            "learning rate ---> 0.1 Epoch ---> 108\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2063 - acc: 0.9269 - val_loss: 0.4347 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.90030\n",
            "Epoch 10/45\n",
            "learning rate ---> 0.1 Epoch ---> 109\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2011 - acc: 0.9303 - val_loss: 0.3889 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.90030\n",
            "Epoch 11/45\n",
            "learning rate ---> 0.1 Epoch ---> 110\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2084 - acc: 0.9276 - val_loss: 0.4106 - val_acc: 0.8974\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.90030\n",
            "Epoch 12/45\n",
            "learning rate ---> 0.1 Epoch ---> 111\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2023 - acc: 0.9291 - val_loss: 0.3885 - val_acc: 0.8978\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.90030\n",
            "Epoch 13/45\n",
            "learning rate ---> 0.1 Epoch ---> 112\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2062 - acc: 0.9273 - val_loss: 0.3962 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.90030\n",
            "Epoch 14/45\n",
            "learning rate ---> 0.1 Epoch ---> 113\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.2014 - acc: 0.9301 - val_loss: 0.4303 - val_acc: 0.8925\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.90030\n",
            "Epoch 15/45\n",
            "learning rate ---> 0.1 Epoch ---> 114\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.2010 - acc: 0.9301 - val_loss: 0.3969 - val_acc: 0.8951\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.90030\n",
            "Epoch 16/45\n",
            "learning rate ---> 0.1 Epoch ---> 115\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1958 - acc: 0.9309 - val_loss: 0.3985 - val_acc: 0.8961\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.90030\n",
            "Epoch 17/45\n",
            "learning rate ---> 0.1 Epoch ---> 116\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.2007 - acc: 0.9298 - val_loss: 0.3586 - val_acc: 0.9019\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.90030 to 0.90190, saving model to /gdrive/My Drive/CIFAR_10/weights.17-0.90.hdf5\n",
            "Epoch 18/45\n",
            "learning rate ---> 0.1 Epoch ---> 117\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1942 - acc: 0.9316 - val_loss: 0.5048 - val_acc: 0.8779\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.90190\n",
            "Epoch 19/45\n",
            "learning rate ---> 0.1 Epoch ---> 118\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1979 - acc: 0.9309 - val_loss: 0.4842 - val_acc: 0.8773\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.90190\n",
            "Epoch 20/45\n",
            "learning rate ---> 0.1 Epoch ---> 119\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1929 - acc: 0.9328 - val_loss: 0.3742 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.90190\n",
            "Epoch 21/45\n",
            "learning rate ---> 0.1 Epoch ---> 120\n",
            "782/782 [==============================] - 297s 379ms/step - loss: 0.1877 - acc: 0.9339 - val_loss: 0.3858 - val_acc: 0.9032\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.90190 to 0.90320, saving model to /gdrive/My Drive/CIFAR_10/weights.21-0.90.hdf5\n",
            "Epoch 22/45\n",
            "learning rate ---> 0.01 Epoch ---> 121\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1597 - acc: 0.9445 - val_loss: 0.3299 - val_acc: 0.9151\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.90320 to 0.91510, saving model to /gdrive/My Drive/CIFAR_10/weights.22-0.92.hdf5\n",
            "Epoch 23/45\n",
            "learning rate ---> 0.01 Epoch ---> 122\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1446 - acc: 0.9490 - val_loss: 0.3265 - val_acc: 0.9176\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.91510 to 0.91760, saving model to /gdrive/My Drive/CIFAR_10/weights.23-0.92.hdf5\n",
            "Epoch 24/45\n",
            "learning rate ---> 0.01 Epoch ---> 123\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1431 - acc: 0.9493 - val_loss: 0.3435 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.91760\n",
            "Epoch 25/45\n",
            "learning rate ---> 0.01 Epoch ---> 124\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1368 - acc: 0.9520 - val_loss: 0.3323 - val_acc: 0.9175\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.91760\n",
            "Epoch 26/45\n",
            "learning rate ---> 0.01 Epoch ---> 125\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1346 - acc: 0.9516 - val_loss: 0.3407 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.91760\n",
            "Epoch 27/45\n",
            "learning rate ---> 0.01 Epoch ---> 126\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1291 - acc: 0.9545 - val_loss: 0.3389 - val_acc: 0.9163\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.91760\n",
            "Epoch 28/45\n",
            "learning rate ---> 0.01 Epoch ---> 127\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1314 - acc: 0.9529 - val_loss: 0.3398 - val_acc: 0.9151\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.91760\n",
            "Epoch 29/45\n",
            "learning rate ---> 0.01 Epoch ---> 128\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1307 - acc: 0.9532 - val_loss: 0.3335 - val_acc: 0.9179\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.91760 to 0.91790, saving model to /gdrive/My Drive/CIFAR_10/weights.29-0.92.hdf5\n",
            "Epoch 30/45\n",
            "learning rate ---> 0.01 Epoch ---> 129\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1272 - acc: 0.9553 - val_loss: 0.3363 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.91790\n",
            "Epoch 31/45\n",
            "learning rate ---> 0.01 Epoch ---> 130\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1260 - acc: 0.9557 - val_loss: 0.3279 - val_acc: 0.9184\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.91790 to 0.91840, saving model to /gdrive/My Drive/CIFAR_10/weights.31-0.92.hdf5\n",
            "Epoch 32/45\n",
            "learning rate ---> 0.01 Epoch ---> 131\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1263 - acc: 0.9554 - val_loss: 0.3402 - val_acc: 0.9185\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.91840 to 0.91850, saving model to /gdrive/My Drive/CIFAR_10/weights.32-0.92.hdf5\n",
            "Epoch 33/45\n",
            "learning rate ---> 0.01 Epoch ---> 132\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1235 - acc: 0.9560 - val_loss: 0.3451 - val_acc: 0.9161\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.91850\n",
            "Epoch 34/45\n",
            "learning rate ---> 0.01 Epoch ---> 133\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1269 - acc: 0.9550 - val_loss: 0.3301 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.91850 to 0.91880, saving model to /gdrive/My Drive/CIFAR_10/weights.34-0.92.hdf5\n",
            "Epoch 35/45\n",
            "learning rate ---> 0.01 Epoch ---> 134\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1269 - acc: 0.9559 - val_loss: 0.3400 - val_acc: 0.9176\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.91880\n",
            "Epoch 36/45\n",
            "learning rate ---> 0.01 Epoch ---> 135\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1219 - acc: 0.9566 - val_loss: 0.3373 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.91880\n",
            "Epoch 37/45\n",
            "learning rate ---> 0.01 Epoch ---> 136\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1214 - acc: 0.9571 - val_loss: 0.3317 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.91880 to 0.91890, saving model to /gdrive/My Drive/CIFAR_10/weights.37-0.92.hdf5\n",
            "Epoch 38/45\n",
            "learning rate ---> 0.01 Epoch ---> 137\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1204 - acc: 0.9569 - val_loss: 0.3411 - val_acc: 0.9183\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.91890\n",
            "Epoch 39/45\n",
            "learning rate ---> 0.01 Epoch ---> 138\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1228 - acc: 0.9563 - val_loss: 0.3301 - val_acc: 0.9195\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.91890 to 0.91950, saving model to /gdrive/My Drive/CIFAR_10/weights.39-0.92.hdf5\n",
            "Epoch 40/45\n",
            "learning rate ---> 0.01 Epoch ---> 139\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1207 - acc: 0.9570 - val_loss: 0.3422 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.91950\n",
            "Epoch 41/45\n",
            "learning rate ---> 0.01 Epoch ---> 140\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1171 - acc: 0.9586 - val_loss: 0.3422 - val_acc: 0.9164\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.91950\n",
            "Epoch 42/45\n",
            "learning rate ---> 0.01 Epoch ---> 141\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1179 - acc: 0.9573 - val_loss: 0.3484 - val_acc: 0.9182\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.91950\n",
            "Epoch 43/45\n",
            "learning rate ---> 0.01 Epoch ---> 142\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1199 - acc: 0.9576 - val_loss: 0.3451 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.91950\n",
            "Epoch 44/45\n",
            "learning rate ---> 0.01 Epoch ---> 143\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1180 - acc: 0.9572 - val_loss: 0.3439 - val_acc: 0.9171\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.91950\n",
            "Epoch 45/45\n",
            "learning rate ---> 0.01 Epoch ---> 144\n",
            "782/782 [==============================] - 298s 380ms/step - loss: 0.1180 - acc: 0.9585 - val_loss: 0.3416 - val_acc: 0.9199\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.91950 to 0.91990, saving model to /gdrive/My Drive/CIFAR_10/weights.45-0.92.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84d3660f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Pw_68Y0QjwOC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 150 - 180 Epoch\n",
        "\n",
        "## Validation Accuracy - 92.19%"
      ]
    },
    {
      "metadata": {
        "id": "ghlJJ6JnXqv8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1890
        },
        "outputId": "15c99a42-7412-490c-8cb1-d838008d99ea"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.45-0.92.hdf5\")\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callbacks\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \n",
        "  epoch += 150 \n",
        "  if epoch <= 120:\n",
        "    K.set_value(model.optimizer.lr, 0.1)\n",
        "    print(\"learning rate --->\" , 0.1 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  elif epoch > 120 and epoch <=210:\n",
        "    K.set_value(model.optimizer.lr, 0.01)\n",
        "    print(\"learning rate --->\" , 0.01 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "  else:\n",
        "    K.set_value(model.optimizer.lr, 0.001)\n",
        "    print(\"learning rate --->\" , 0.001 , \"Epoch --->\" , epoch)\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "    \n",
        "      \n",
        "  \n",
        "lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"fifth__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    callbacks=[lr_reducer , csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs=30 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "learning rate ---> 0.01 Epoch ---> 150\n",
            "782/782 [==============================] - 315s 402ms/step - loss: 0.1195 - acc: 0.9584 - val_loss: 0.3483 - val_acc: 0.9168\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91680, saving model to /gdrive/My Drive/CIFAR_10/weights.01-0.92.hdf5\n",
            "Epoch 2/30\n",
            "learning rate ---> 0.01 Epoch ---> 151\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1170 - acc: 0.9584 - val_loss: 0.3473 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.91680 to 0.91730, saving model to /gdrive/My Drive/CIFAR_10/weights.02-0.92.hdf5\n",
            "Epoch 3/30\n",
            "learning rate ---> 0.01 Epoch ---> 152\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1156 - acc: 0.9589 - val_loss: 0.3588 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.91730\n",
            "Epoch 4/30\n",
            "learning rate ---> 0.01 Epoch ---> 153\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1175 - acc: 0.9584 - val_loss: 0.3484 - val_acc: 0.9159\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.91730\n",
            "Epoch 5/30\n",
            "learning rate ---> 0.01 Epoch ---> 154\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1154 - acc: 0.9587 - val_loss: 0.3427 - val_acc: 0.9172\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.91730\n",
            "Epoch 6/30\n",
            "learning rate ---> 0.01 Epoch ---> 155\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1115 - acc: 0.9603 - val_loss: 0.3558 - val_acc: 0.9177\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.91730 to 0.91770, saving model to /gdrive/My Drive/CIFAR_10/weights.06-0.92.hdf5\n",
            "Epoch 7/30\n",
            "learning rate ---> 0.01 Epoch ---> 156\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1153 - acc: 0.9594 - val_loss: 0.3610 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.91770\n",
            "Epoch 8/30\n",
            "learning rate ---> 0.01 Epoch ---> 157\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1119 - acc: 0.9594 - val_loss: 0.3483 - val_acc: 0.9181\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.91770 to 0.91810, saving model to /gdrive/My Drive/CIFAR_10/weights.08-0.92.hdf5\n",
            "Epoch 9/30\n",
            "learning rate ---> 0.01 Epoch ---> 158\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1172 - acc: 0.9585 - val_loss: 0.3544 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.91810\n",
            "Epoch 10/30\n",
            "learning rate ---> 0.01 Epoch ---> 159\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1132 - acc: 0.9590 - val_loss: 0.3542 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.91810\n",
            "Epoch 11/30\n",
            "learning rate ---> 0.01 Epoch ---> 160\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1127 - acc: 0.9591 - val_loss: 0.3500 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.91810\n",
            "Epoch 12/30\n",
            "learning rate ---> 0.01 Epoch ---> 161\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 0.1128 - acc: 0.9601 - val_loss: 0.3529 - val_acc: 0.9169\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.91810\n",
            "Epoch 13/30\n",
            "learning rate ---> 0.01 Epoch ---> 162\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1117 - acc: 0.9599 - val_loss: 0.3484 - val_acc: 0.9198\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.91810 to 0.91980, saving model to /gdrive/My Drive/CIFAR_10/weights.13-0.92.hdf5\n",
            "Epoch 14/30\n",
            "learning rate ---> 0.01 Epoch ---> 163\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1118 - acc: 0.9607 - val_loss: 0.3561 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.91980\n",
            "Epoch 15/30\n",
            "learning rate ---> 0.01 Epoch ---> 164\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1096 - acc: 0.9610 - val_loss: 0.3546 - val_acc: 0.9184\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.91980\n",
            "Epoch 16/30\n",
            "learning rate ---> 0.01 Epoch ---> 165\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1110 - acc: 0.9598 - val_loss: 0.3523 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.91980 to 0.92010, saving model to /gdrive/My Drive/CIFAR_10/weights.16-0.92.hdf5\n",
            "Epoch 17/30\n",
            "learning rate ---> 0.01 Epoch ---> 166\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1123 - acc: 0.9601 - val_loss: 0.3424 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.92010 to 0.92190, saving model to /gdrive/My Drive/CIFAR_10/weights.17-0.92.hdf5\n",
            "Epoch 18/30\n",
            "learning rate ---> 0.01 Epoch ---> 167\n",
            "782/782 [==============================] - 298s 381ms/step - loss: 0.1109 - acc: 0.9601 - val_loss: 0.3477 - val_acc: 0.9197\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.92190\n",
            "Epoch 19/30\n",
            "learning rate ---> 0.01 Epoch ---> 168\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1124 - acc: 0.9596 - val_loss: 0.3443 - val_acc: 0.9192\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.92190\n",
            "Epoch 20/30\n",
            "learning rate ---> 0.01 Epoch ---> 169\n",
            "782/782 [==============================] - 297s 380ms/step - loss: 0.1120 - acc: 0.9601 - val_loss: 0.3443 - val_acc: 0.9205\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.92190\n",
            "Epoch 21/30\n",
            "learning rate ---> 0.01 Epoch ---> 170\n",
            "717/782 [==========================>...] - ETA: 23s - loss: 0.1101 - acc: 0.9604"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZXJgKPoIf1Yd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Refresh"
      ]
    },
    {
      "metadata": {
        "id": "L2fJyCFTkEo2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11010
        },
        "outputId": "b9b96f15-9978-4d0c-b415-842a6d7497ea"
      },
      "cell_type": "code",
      "source": [
        "#keras library\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "\n",
        "# densenet \n",
        "\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "#############################################################################\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "l = 12 #L\n",
        "num_filter = 36 #k\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "#############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing data\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 27s 0us/step\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 36)   972         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 36)   144         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 36)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 18)   5832        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 18)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 54)   0           conv2d_1[0][0]                   \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 54)   216         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 54)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 18)   8748        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 18)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 72)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 72)   288         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 72)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 18)   11664       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 18)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 90)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 90)   360         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 90)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 18)   14580       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 18)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 108)  0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 108)  432         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 108)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 18)   17496       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 18)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 126)  0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 126)  504         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 126)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 18)   20412       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 18)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 144)  0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 144)  576         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 144)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 18)   23328       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 18)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 162)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 162)  648         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 162)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 18)   26244       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 18)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 180)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 180)  720         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 180)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 18)   29160       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 18)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 198)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 198)  792         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 198)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 18)   32076       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 18)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 216)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 216)  864         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 216)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 18)   34992       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 18)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 234)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 234)  936         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 234)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 18)   37908       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 18)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 252)  0           concatenate_11[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 252)  1008        concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 252)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 18)   4536        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 18)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 18)   0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 18)   72          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 18)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 18)   2916        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 18)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 36)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 36)   144         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 36)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 18)   5832        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 18)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 54)   0           concatenate_13[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 54)   216         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 54)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 18)   8748        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 16, 16, 18)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 72)   0           concatenate_14[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 72)   288         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 72)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 18)   11664       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 18)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 90)   0           concatenate_15[0][0]             \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 90)   360         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 90)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 18)   14580       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 18)   0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 108)  0           concatenate_16[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 108)  432         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 108)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 18)   17496       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 16, 16, 18)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 126)  0           concatenate_17[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 126)  504         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 126)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 18)   20412       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 18)   0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 144)  0           concatenate_18[0][0]             \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 144)  576         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 144)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 18)   23328       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 16, 16, 18)   0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 162)  0           concatenate_19[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 162)  648         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 162)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 18)   26244       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 18)   0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 180)  0           concatenate_20[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 180)  720         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 180)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 18)   29160       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16, 16, 18)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 198)  0           concatenate_21[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 198)  792         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 198)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 18)   32076       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 16, 16, 18)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 216)  0           concatenate_22[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 216)  864         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 216)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 18)   34992       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 16, 16, 18)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 234)  0           concatenate_23[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 234)  936         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 234)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 18)   4212        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 18)   0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 18)     0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 18)     72          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 18)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 18)     2916        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 8, 8, 18)     0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 36)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 36)     144         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 36)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 18)     5832        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 8, 8, 18)     0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 54)     0           concatenate_25[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 54)     216         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 54)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 18)     8748        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 8, 8, 18)     0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 72)     0           concatenate_26[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 72)     288         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 72)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 18)     11664       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 8, 8, 18)     0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 90)     0           concatenate_27[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 90)     360         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 90)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 18)     14580       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 8, 8, 18)     0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 108)    0           concatenate_28[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 108)    432         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 108)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 18)     17496       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 8, 8, 18)     0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 126)    0           concatenate_29[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 126)    504         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 126)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 18)     20412       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 8, 8, 18)     0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 144)    0           concatenate_30[0][0]             \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 144)    576         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 144)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 18)     23328       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 8, 8, 18)     0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 162)    0           concatenate_31[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 162)    648         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 162)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 18)     26244       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 8, 8, 18)     0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 180)    0           concatenate_32[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 180)    720         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 180)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 18)     29160       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 8, 8, 18)     0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 198)    0           concatenate_33[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 198)    792         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 198)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 18)     32076       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 8, 8, 18)     0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 216)    0           concatenate_34[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 216)    864         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 216)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 18)     34992       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 8, 18)     0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 234)    0           concatenate_35[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 234)    936         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 234)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 18)     4212        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 8, 8, 18)     0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 4, 4, 18)     0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 18)     72          average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 18)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 18)     2916        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 4, 4, 18)     0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 36)     0           average_pooling2d_3[0][0]        \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 36)     144         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 36)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 18)     5832        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 4, 4, 18)     0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 54)     0           concatenate_37[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 54)     216         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 54)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 18)     8748        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 4, 4, 18)     0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 72)     0           concatenate_38[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 72)     288         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 72)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 18)     11664       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 4, 4, 18)     0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 90)     0           concatenate_39[0][0]             \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 90)     360         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 90)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 18)     14580       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 4, 4, 18)     0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 108)    0           concatenate_40[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 108)    432         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 108)    0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 18)     17496       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 4, 4, 18)     0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 126)    0           concatenate_41[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 126)    504         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 126)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 18)     20412       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 4, 4, 18)     0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 144)    0           concatenate_42[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 144)    576         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 144)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 18)     23328       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 4, 4, 18)     0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 162)    0           concatenate_43[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 162)    648         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 162)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 18)     26244       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 4, 4, 18)     0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 180)    0           concatenate_44[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 180)    720         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 180)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 18)     29160       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 4, 4, 18)     0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 198)    0           concatenate_45[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 198)    792         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 198)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 18)     32076       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 4, 4, 18)     0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 216)    0           concatenate_46[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 216)    864         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 216)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 18)     34992       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 4, 4, 18)     0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 4, 4, 234)    0           concatenate_47[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 4, 234)    936         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 4, 234)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 2, 2, 234)    0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 936)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           9370        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 995,230\n",
            "Trainable params: 981,658\n",
            "Non-trainable params: 13,572\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lxnyrCdPAQ6j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# First Top Assignment Accuracy\n",
        "## Model Evaluate - 92.19%"
      ]
    },
    {
      "metadata": {
        "id": "ef2Qryab-aH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "47de03e2-a792-48b6-ea79-300ea9a2a1d3"
      },
      "cell_type": "code",
      "source": [
        "sgd = keras.optimizers.SGD(lr=0.1 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.17-0.92.hdf5\")\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 24s 2ms/step\n",
            "Test loss: 0.34242540892288087\n",
            "Test accuracy: 0.9219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKZOKxOEwhiS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experimenting to increase accuracy"
      ]
    },
    {
      "metadata": {
        "id": "uRh3mGoa-SPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Multiple version of models between 180-220. We will select the best.\n",
        "\n",
        "\n",
        "## /gdrive/My Drive/CIFAR_10/weights.01-0.92.hdf5-92.25%\n",
        "## /gdrive/My Drive/CIFAR_10/weights.04-0.92.hdf5-92.24%\n",
        "##/gdrive/My Drive/CIFAR_10/__24__weights.01-0.92.hdf5-92.27%"
      ]
    },
    {
      "metadata": {
        "id": "8fR0dsBk-qK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10991
        },
        "outputId": "ecf6a090-edde-414f-80f7-040bfcc2c559"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import Input\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "\n",
        "# densenet \n",
        "\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "  \n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "#############################################################################\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "l = 12 #L\n",
        "num_filter = 36 #k\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "#############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "#normalizing data\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "# Back_Prop_First_Conv2D\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 37s 0us/step\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 36)   972         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 36)   144         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 36)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 18)   5832        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 18)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 54)   0           conv2d_1[0][0]                   \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 54)   216         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 54)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 18)   8748        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 18)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 72)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 72)   288         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 72)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 18)   11664       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 18)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 90)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 90)   360         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 90)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 18)   14580       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 18)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 108)  0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 108)  432         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 108)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 18)   17496       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 18)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 126)  0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 126)  504         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 126)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 18)   20412       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 18)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 144)  0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 144)  576         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 144)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 18)   23328       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 18)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 162)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 162)  648         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 162)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 18)   26244       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 18)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 180)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 180)  720         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 180)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 18)   29160       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 18)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 198)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 198)  792         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 198)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 18)   32076       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 18)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 216)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 216)  864         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 216)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 18)   34992       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 18)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 234)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 234)  936         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 234)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 18)   37908       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 18)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 252)  0           concatenate_11[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 252)  1008        concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 252)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 18)   4536        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 18)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 18)   0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 18)   72          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 18)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 18)   2916        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 18)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 36)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 36)   144         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 36)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 18)   5832        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 18)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 54)   0           concatenate_13[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 54)   216         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 54)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 18)   8748        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 16, 16, 18)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 72)   0           concatenate_14[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 72)   288         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 72)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 18)   11664       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 18)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 90)   0           concatenate_15[0][0]             \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 90)   360         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 90)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 18)   14580       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 18)   0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 108)  0           concatenate_16[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 108)  432         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 108)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 18)   17496       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 16, 16, 18)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 126)  0           concatenate_17[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 126)  504         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 126)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 18)   20412       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 18)   0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 144)  0           concatenate_18[0][0]             \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 144)  576         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 144)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 18)   23328       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 16, 16, 18)   0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 162)  0           concatenate_19[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 162)  648         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 162)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 18)   26244       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 18)   0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 180)  0           concatenate_20[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 180)  720         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 180)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 18)   29160       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16, 16, 18)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 198)  0           concatenate_21[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 198)  792         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 198)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 18)   32076       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 16, 16, 18)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 216)  0           concatenate_22[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 216)  864         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 216)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 18)   34992       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 16, 16, 18)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 234)  0           concatenate_23[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 234)  936         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 234)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 18)   4212        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 18)   0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 18)     0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 18)     72          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 18)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 18)     2916        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 8, 8, 18)     0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 36)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 36)     144         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 36)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 18)     5832        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 8, 8, 18)     0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 54)     0           concatenate_25[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 54)     216         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 54)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 18)     8748        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 8, 8, 18)     0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 72)     0           concatenate_26[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 72)     288         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 72)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 18)     11664       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 8, 8, 18)     0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 90)     0           concatenate_27[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 90)     360         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 90)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 18)     14580       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 8, 8, 18)     0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 108)    0           concatenate_28[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 108)    432         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 108)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 18)     17496       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 8, 8, 18)     0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 126)    0           concatenate_29[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 126)    504         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 126)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 18)     20412       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 8, 8, 18)     0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 144)    0           concatenate_30[0][0]             \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 144)    576         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 144)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 18)     23328       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 8, 8, 18)     0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 162)    0           concatenate_31[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 162)    648         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 162)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 18)     26244       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 8, 8, 18)     0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 180)    0           concatenate_32[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 180)    720         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 180)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 18)     29160       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 8, 8, 18)     0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 198)    0           concatenate_33[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 198)    792         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 198)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 18)     32076       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 8, 8, 18)     0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 216)    0           concatenate_34[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 216)    864         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 216)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 18)     34992       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 8, 18)     0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 234)    0           concatenate_35[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 234)    936         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 234)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 18)     4212        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 8, 8, 18)     0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 4, 4, 18)     0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 18)     72          average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 18)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 18)     2916        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 4, 4, 18)     0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 36)     0           average_pooling2d_3[0][0]        \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 36)     144         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 36)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 18)     5832        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 4, 4, 18)     0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 54)     0           concatenate_37[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 54)     216         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 54)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 18)     8748        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 4, 4, 18)     0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 72)     0           concatenate_38[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 72)     288         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 72)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 18)     11664       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 4, 4, 18)     0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 90)     0           concatenate_39[0][0]             \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 90)     360         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 90)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 18)     14580       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 4, 4, 18)     0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 108)    0           concatenate_40[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 108)    432         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 108)    0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 18)     17496       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 4, 4, 18)     0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 126)    0           concatenate_41[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 126)    504         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 126)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 18)     20412       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 4, 4, 18)     0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 144)    0           concatenate_42[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 144)    576         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 144)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 18)     23328       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 4, 4, 18)     0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 162)    0           concatenate_43[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 162)    648         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 162)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 18)     26244       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 4, 4, 18)     0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 180)    0           concatenate_44[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 180)    720         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 180)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 18)     29160       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 4, 4, 18)     0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 198)    0           concatenate_45[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 198)    792         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 198)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 18)     32076       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 4, 4, 18)     0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 216)    0           concatenate_46[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 216)    864         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 216)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 18)     34992       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 4, 4, 18)     0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 4, 4, 234)    0           concatenate_47[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 4, 234)    936         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 4, 234)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 2, 2, 234)    0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 936)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           9370        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 995,230\n",
            "Trainable params: 981,658\n",
            "Non-trainable params: 13,572\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fLmo8NSEBiIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 180 - 220 Epochs --------> Accuracy - 92.92%"
      ]
    },
    {
      "metadata": {
        "id": "ZMfnkT-2wmFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3057
        },
        "outputId": "671a12fa-b4aa-4ad2-ceb6-34f98ea07b44"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/weights.17-0.92.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=9 , \n",
        "#     width_shift_range=0.125,\n",
        "#     height_shift_range=0.125,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=128),\n",
        "                    callbacks=[ csv_logger , model_checkpoint] ,\n",
        "#                     steps_per_epoch= len(x_train)/batch_size,\n",
        "                    epochs= 40 , \n",
        "                   verbose = 1 , \n",
        "                   workers = 4 , \n",
        "#                    use_multiprocessing = True , \n",
        "                   shuffle = True , \n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "391/391 [==============================] - 276s 705ms/step - loss: 0.0772 - acc: 0.9726 - val_loss: 0.3066 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92630, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n",
            "Epoch 2/40\n",
            "391/391 [==============================] - 262s 670ms/step - loss: 0.0745 - acc: 0.9739 - val_loss: 0.3076 - val_acc: 0.9272\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.92630 to 0.92720, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.02-0.93.hdf5\n",
            "Epoch 3/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0708 - acc: 0.9752 - val_loss: 0.3173 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.92720\n",
            "Epoch 4/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0687 - acc: 0.9763 - val_loss: 0.3191 - val_acc: 0.9255\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.92720\n",
            "Epoch 5/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0702 - acc: 0.9754 - val_loss: 0.3198 - val_acc: 0.9261\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.92720\n",
            "Epoch 6/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0682 - acc: 0.9764 - val_loss: 0.3231 - val_acc: 0.9266\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.92720\n",
            "Epoch 7/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0692 - acc: 0.9749 - val_loss: 0.3205 - val_acc: 0.9275\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.92720 to 0.92750, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.07-0.93.hdf5\n",
            "Epoch 8/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0658 - acc: 0.9772 - val_loss: 0.3264 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.92750\n",
            "Epoch 9/40\n",
            "391/391 [==============================] - 263s 674ms/step - loss: 0.0664 - acc: 0.9765 - val_loss: 0.3298 - val_acc: 0.9269\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.92750\n",
            "Epoch 10/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0688 - acc: 0.9752 - val_loss: 0.3267 - val_acc: 0.9267\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.92750\n",
            "Epoch 11/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0666 - acc: 0.9765 - val_loss: 0.3269 - val_acc: 0.9266\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.92750\n",
            "Epoch 12/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0638 - acc: 0.9780 - val_loss: 0.3281 - val_acc: 0.9267\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.92750\n",
            "Epoch 13/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0645 - acc: 0.9770 - val_loss: 0.3298 - val_acc: 0.9273\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.92750\n",
            "Epoch 14/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0624 - acc: 0.9768 - val_loss: 0.3334 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.92750\n",
            "Epoch 15/40\n",
            "391/391 [==============================] - 261s 668ms/step - loss: 0.0638 - acc: 0.9774 - val_loss: 0.3333 - val_acc: 0.9272\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.92750\n",
            "Epoch 16/40\n",
            "391/391 [==============================] - 261s 668ms/step - loss: 0.0651 - acc: 0.9781 - val_loss: 0.3332 - val_acc: 0.9269\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.92750\n",
            "Epoch 17/40\n",
            "391/391 [==============================] - 262s 670ms/step - loss: 0.0630 - acc: 0.9774 - val_loss: 0.3290 - val_acc: 0.9292\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.92750 to 0.92920, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\n",
            "Epoch 18/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0630 - acc: 0.9771 - val_loss: 0.3296 - val_acc: 0.9279\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.92920\n",
            "Epoch 19/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0643 - acc: 0.9769 - val_loss: 0.3349 - val_acc: 0.9278\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.92920\n",
            "Epoch 20/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0611 - acc: 0.9776 - val_loss: 0.3391 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.92920\n",
            "Epoch 21/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0619 - acc: 0.9779 - val_loss: 0.3346 - val_acc: 0.9278\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.92920\n",
            "Epoch 22/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0622 - acc: 0.9782 - val_loss: 0.3366 - val_acc: 0.9269\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.92920\n",
            "Epoch 23/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0628 - acc: 0.9781 - val_loss: 0.3336 - val_acc: 0.9275\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.92920\n",
            "Epoch 24/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0623 - acc: 0.9775 - val_loss: 0.3375 - val_acc: 0.9260\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.92920\n",
            "Epoch 25/40\n",
            "391/391 [==============================] - 264s 674ms/step - loss: 0.0625 - acc: 0.9773 - val_loss: 0.3336 - val_acc: 0.9285\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.92920\n",
            "Epoch 26/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0594 - acc: 0.9789 - val_loss: 0.3362 - val_acc: 0.9284\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.92920\n",
            "Epoch 27/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0621 - acc: 0.9781 - val_loss: 0.3363 - val_acc: 0.9275\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.92920\n",
            "Epoch 28/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0605 - acc: 0.9776 - val_loss: 0.3360 - val_acc: 0.9274\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.92920\n",
            "Epoch 29/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0589 - acc: 0.9787 - val_loss: 0.3369 - val_acc: 0.9273\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.92920\n",
            "Epoch 30/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0584 - acc: 0.9791 - val_loss: 0.3378 - val_acc: 0.9274\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.92920\n",
            "Epoch 31/40\n",
            "391/391 [==============================] - 263s 674ms/step - loss: 0.0579 - acc: 0.9793 - val_loss: 0.3420 - val_acc: 0.9269\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.92920\n",
            "Epoch 32/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0608 - acc: 0.9782 - val_loss: 0.3387 - val_acc: 0.9267\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.92920\n",
            "Epoch 33/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0596 - acc: 0.9779 - val_loss: 0.3406 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.92920\n",
            "Epoch 34/40\n",
            "391/391 [==============================] - 266s 680ms/step - loss: 0.0600 - acc: 0.9782 - val_loss: 0.3397 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.92920\n",
            "Epoch 35/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0593 - acc: 0.9786 - val_loss: 0.3405 - val_acc: 0.9270\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.92920\n",
            "Epoch 36/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0576 - acc: 0.9789 - val_loss: 0.3415 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.92920\n",
            "Epoch 37/40\n",
            "391/391 [==============================] - 263s 672ms/step - loss: 0.0577 - acc: 0.9796 - val_loss: 0.3388 - val_acc: 0.9280\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.92920\n",
            "Epoch 38/40\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.0585 - acc: 0.9788 - val_loss: 0.3384 - val_acc: 0.9278\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.92920\n",
            "Epoch 39/40\n",
            "391/391 [==============================] - 264s 674ms/step - loss: 0.0578 - acc: 0.9788 - val_loss: 0.3378 - val_acc: 0.9280\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.92920\n",
            "Epoch 40/40\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.0571 - acc: 0.9796 - val_loss: 0.3439 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.92920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9def47dc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "z3Fzd1AOB8O7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Second Top Assignment Accuracy\n",
        "## Model Evaluate 92.92%"
      ]
    },
    {
      "metadata": {
        "id": "8VMy6OdAB-d7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "b2ca236a-7bce-4dd4-a1d8-ad14daa052e6"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,  momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\")\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 25s 2ms/step\n",
            "Test loss: 0.32899246576428415\n",
            "Test accuracy: 0.9292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "akn2T40yDxpP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 10 more Epochs remaining out of 250 to get 94%"
      ]
    },
    {
      "metadata": {
        "id": "I6zXkOT6IqOF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Remove Augmentation for 1 Epoch."
      ]
    },
    {
      "metadata": {
        "id": "GEr9KvJnSOIZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "--------\n",
        "-------\n",
        "---------\n",
        "---------\n",
        "------------"
      ]
    },
    {
      "metadata": {
        "id": "DnanV4qJSMaM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 1 -  9266 - No Improvement"
      ]
    },
    {
      "metadata": {
        "id": "TeoaXwDww4Vc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "b81ab7d2-d12e-4f35-95f9-3d27fd9de869"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ csv_logger , model_checkpoint],\n",
        "          batch_size=64, \n",
        "          epochs=1, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 306s 6ms/step - loss: 0.0707 - acc: 0.9737 - val_loss: 0.3346 - val_acc: 0.9266\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92660, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc440c80940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "o-3HD4VmL0Qr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJScDWKeSKYn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 2 -  9247 - No Improvement"
      ]
    },
    {
      "metadata": {
        "id": "-NUJwSssL0Lm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "627596c0-2c6f-462c-c226-e1fef89c2b22"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ csv_logger , model_checkpoint],\n",
        "          batch_size=32, \n",
        "          epochs=1, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 351s 7ms/step - loss: 0.0909 - acc: 0.9676 - val_loss: 0.3337 - val_acc: 0.9247\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92470, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.92.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc4aafcf550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "yUkyVNN3DxZi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "EFDAWkKfSH11",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 3 - 9275 - No Improvement"
      ]
    },
    {
      "metadata": {
        "id": "F6RXkblUSUhl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "u4WJh4xhOSdj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "546a5a35-37ac-4cac-db8f-b2bca98454f8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ csv_logger , model_checkpoint],\n",
        "          batch_size=128, \n",
        "          epochs=1, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 280s 6ms/step - loss: 0.0623 - acc: 0.9781 - val_loss: 0.3314 - val_acc: 0.9275\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92750, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc440c1f978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "-Wg2Ah-iSXIO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "---------\n",
        "-----------\n",
        "---------\n",
        "----------\n",
        "------------\n",
        "--------------\n",
        "-------------\n",
        "--------------\n",
        "---------------\n",
        "--------------\n",
        "-------------\n",
        "-----------------\n",
        "-------------\n",
        "--------------------\n",
        "------------------\n",
        "-------------------\n",
        "-----------------\n",
        "-------------\n"
      ]
    },
    {
      "metadata": {
        "id": "6hQkHGngWmCB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 4 - 9301 - Improvement"
      ]
    },
    {
      "metadata": {
        "id": "pMjSbO3dNeij",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Third Top Assignment Accuracy \n",
        "## Model Evaluate - 93%"
      ]
    },
    {
      "metadata": {
        "id": "59u3S62DOURg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "a5254225-6396-4245-d885-974e81e2b3e7"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.17-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ csv_logger , model_checkpoint],\n",
        "          batch_size=200, \n",
        "          epochs=1, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 265s 5ms/step - loss: 0.0623 - acc: 0.9784 - val_loss: 0.3289 - val_acc: 0.9301\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.93010, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc421400ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "ikKVc3cWV5U1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------\n",
        "--------------------------------------\n",
        "------------------------------------------\n",
        "--------------------------------------\n",
        "---------------------------------------------\n",
        "---------------------------------------------\n",
        "----------------------------------------------\n",
        "-----------------------------------------------\n",
        "-----------------------------------------------\n"
      ]
    },
    {
      "metadata": {
        "id": "NPnht2pVYZlM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experimenting to Reach 94%\n",
        "# Load 9301% model "
      ]
    },
    {
      "metadata": {
        "id": "ME7HF7zWSQXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "4ceed179-47be-40f1-f0e4-ab85c0c35ea7"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-2 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              factor=0.1, \n",
        "                                              patience=1, \n",
        "                                              verbose=1, \n",
        "                                              mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "                                              min_lr=0.001)\n",
        "\n",
        "      \n",
        "  \n",
        "# lr_reducer = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"180-250-epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ reduce_lr , csv_logger , model_checkpoint],\n",
        "          batch_size=50, \n",
        "          epochs=5, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "50000/50000 [==============================] - 318s 6ms/step - loss: 0.0695 - acc: 0.9754 - val_loss: 0.3203 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92620, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n",
            "Epoch 2/5\n",
            "50000/50000 [==============================] - 293s 6ms/step - loss: 0.0691 - acc: 0.9752 - val_loss: 0.3223 - val_acc: 0.9270\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.92620 to 0.92700, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.02-0.93.hdf5\n",
            "Epoch 3/5\n",
            "50000/50000 [==============================] - 293s 6ms/step - loss: 0.0683 - acc: 0.9758 - val_loss: 0.3218 - val_acc: 0.9271\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.92700 to 0.92710, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.03-0.93.hdf5\n",
            "Epoch 4/5\n",
            "50000/50000 [==============================] - 293s 6ms/step - loss: 0.0703 - acc: 0.9751 - val_loss: 0.3214 - val_acc: 0.9267\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.92710\n",
            "Epoch 5/5\n",
            "50000/50000 [==============================] - 292s 6ms/step - loss: 0.0691 - acc: 0.9755 - val_loss: 0.3204 - val_acc: 0.9274\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.92710 to 0.92740, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.05-0.93.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc3f3615828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "iaFGtE6HGiXl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trying out Stochastic Gradient descent with restarts"
      ]
    },
    {
      "metadata": {
        "id": "KjjYEoodUFsc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Github LInk](https://gist.github.com/jeremyjordan/5a222e04bb78c242f5763ad40626c452)"
      ]
    },
    {
      "metadata": {
        "id": "p0rdEQFqGl-D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import Callback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "class SGDRScheduler(Callback):\n",
        "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
        "    # Usage\n",
        "        ```python\n",
        "            schedule = SGDRScheduler(min_lr=1e-5,\n",
        "                                     max_lr=1e-2,\n",
        "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
        "                                     lr_decay=0.9,\n",
        "                                     cycle_length=5,\n",
        "                                     mult_factor=1.5)\n",
        "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
        "        ```\n",
        "    # Arguments\n",
        "        min_lr: The lower bound of the learning rate range for the experiment.\n",
        "        max_lr: The upper bound of the learning rate range for the experiment.\n",
        "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
        "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
        "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
        "        cycle_length: Initial number of epochs in a cycle.\n",
        "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
        "    # References\n",
        "        Blog post: jeremyjordan.me/nn-learning-rate\n",
        "        Original paper: http://arxiv.org/abs/1608.03983\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 min_lr,\n",
        "                 max_lr,\n",
        "                 steps_per_epoch,\n",
        "                 lr_decay=1,\n",
        "                 cycle_length=10,\n",
        "                 mult_factor=2):\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.lr_decay = lr_decay\n",
        "\n",
        "        self.batch_since_restart = 0\n",
        "        self.next_restart = cycle_length\n",
        "\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "        self.cycle_length = cycle_length\n",
        "        self.mult_factor = mult_factor\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
        "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
        "        return lr\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        self.batch_since_restart += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
        "        if epoch + 1 == self.next_restart:\n",
        "            self.batch_since_restart = 0\n",
        "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
        "            self.next_restart += self.cycle_length\n",
        "            self.max_lr *= self.lr_decay\n",
        "            self.best_weights = self.model.get_weights()\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
        "        self.model.set_weights(self.best_weights)\n",
        "        \n",
        "        \n",
        "\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QM08OFLyX-dF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1194
        },
        "outputId": "0b3ef633-664c-45bc-92c0-9c8010809d49"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 64\n",
        "epoch_size = 5\n",
        "\n",
        "model.load_weights(\"/gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\")\n",
        "sgd = keras.optimizers.SGD(lr=0.01 ,decay = 1e-6 ,   momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "#                                               factor=0.1, \n",
        "#                                               patience=1, \n",
        "#                                               verbose=1, \n",
        "#                                               mode='auto', \n",
        "#                                               min_delta=0.0001, \n",
        "#                                               cooldown=0, \n",
        "#                                               min_lr=0.001)\n",
        "\n",
        "schedule = SGDRScheduler(min_lr=1e-3,\n",
        "                         max_lr=1e-2,\n",
        "                         steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
        "                         lr_decay=0.6,\n",
        "                         cycle_length=10,\n",
        "                         mult_factor=1.5)\n",
        "\n",
        "\n",
        "      \n",
        "  \n",
        "\n",
        "filepath = '/gdrive/My Drive/CIFAR_10/__24__weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                monitor='val_acc', \n",
        "                                verbose=1, \n",
        "                                save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                mode='max', \n",
        "                                period=1)\n",
        "\n",
        "\n",
        "csv_logger = keras.callbacks.CSVLogger(filename =  \"___210-240____epoch__.csv\", separator=',', append=False)\n",
        "\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y= y_train, \n",
        "          callbacks=[ schedule , csv_logger , model_checkpoint],\n",
        "          batch_size=batch_size, \n",
        "          epochs=epoch_size, \n",
        "          verbose=1, \n",
        "          validation_split=0.0, \n",
        "          validation_data=(x_test, y_test), \n",
        "          shuffle=True)\n",
        "         \n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "  128/50000 [..............................] - ETA: 1:14:01 - loss: 0.0947 - acc: 0.9531"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.897983). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 313s 6ms/step - loss: 0.0665 - acc: 0.9770 - val_loss: 0.3222 - val_acc: 0.9280\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92800, saving model to /gdrive/My Drive/CIFAR_10/__24__weights.01-0.93.hdf5\n",
            "Epoch 2/5\n",
            "50000/50000 [==============================] - 300s 6ms/step - loss: 0.0631 - acc: 0.9768 - val_loss: 0.3229 - val_acc: 0.9278\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.92800\n",
            "Epoch 3/5\n",
            "50000/50000 [==============================] - 299s 6ms/step - loss: 0.0658 - acc: 0.9761 - val_loss: 0.3229 - val_acc: 0.9279\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.92800\n",
            "Epoch 4/5\n",
            "50000/50000 [==============================] - 299s 6ms/step - loss: 0.0657 - acc: 0.9771 - val_loss: 0.3234 - val_acc: 0.9273\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.92800\n",
            "Epoch 5/5\n",
            "50000/50000 [==============================] - 299s 6ms/step - loss: 0.0641 - acc: 0.9771 - val_loss: 0.3235 - val_acc: 0.9275\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.92800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-f384f3502a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m           shuffle=True)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-1f3baf0d07ba>\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;34m'''Set weights to the values from the end of the most recent cycle for best performance.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'SGDRScheduler' object has no attribute 'best_weights'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ZYfKrne3N3FC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Try increasing batch size without reducing the learning rate. As mentioned in Google AI paper\n",
        "\n",
        "[Paper](https://ai.google/research/pubs/pub46644)"
      ]
    },
    {
      "metadata": {
        "id": "571SepJ8VNe4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}